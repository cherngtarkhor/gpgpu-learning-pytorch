<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="37" failures="120" skipped="1298" tests="11079" time="2044.207" timestamp="2025-01-14T18:56:53.851060+08:00" hostname="flexgpua770-X299-AORUS-Gaming-3-Pro"><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[False-8]" time="1.427" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint32-/]" time="1.416" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int64-+]" time="1.466" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-&amp;1]" time="1.465" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-^1]" time="1.415" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-bfloat16-&amp;1]" time="1.431" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-|2]" time="1.462" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint16-==-real-real]" time="1.461" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-^1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint64-/]" time="0.009" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[False-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float16-/]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-^1]" time="0.009" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[False-32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[False-64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-^1]" time="0.010" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[True-8]" time="0.003" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[True-16]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[True-32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-^1]" time="0.009" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[True-64]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_annotations" name="test_int_annotation[False-1]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-^1]" time="0.009" /><testcase classname="test.unit.language.test_annotations" name="test_unknown_annotation" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint32-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-|2]" time="0.011" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str0-64-None-None]" time="0.176" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-&amp;1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint8-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-&amp;1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint64-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-&amp;2]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-&amp;1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-&amp;1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str1-64-None-lower]" time="0.096" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str2-64-None-upper]" time="0.004" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str3-64-zero-None]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str4-64-zero-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str5-64-zero-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str6-64-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-bfloat16-^0]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-bfloat16-/]" time="0.047" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str7-64-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str8-64-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str9-128-None-None]" time="0.002" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-|3]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str10-128-None-lower]" time="0.002" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str11-128-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str12-128-zero-None]" time="0.002" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str13-128-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str14-128-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-^0]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str15-128-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-|3]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str16-128-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-^0]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str17-128-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str18-256-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str19-256-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int8-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-|3]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str20-256-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str21-256-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str22-256-zero-lower]" time="0.002" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str23-256-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str24-256-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-&amp;3]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int16-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int16-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-|3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int32-/]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-bfloat16-+]" time="0.170" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-bfloat16-&amp;0]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int64-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int64-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-|3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint8-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint8-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint16-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-|3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-&amp;0]" time="0.135" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint32-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint64-/]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint32-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float16-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint64-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str25-256-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float32-/]" time="0.024" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str26-256-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str27-512-None-None]" time="0.002" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str28-512-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str29-512-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str30-512-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str31-512-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str32-512-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str33-512-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str34-512-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str35-512-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str36-1024-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str37-1024-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str38-1024-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str39-1024-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str40-1024-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str41-1024-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str42-1024-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str43-1024-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str44-1024-nan-upper]" time="0.000"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str45-64-None-None]" time="0.004" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str46-64-None-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str47-64-None-upper]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int8-+]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str48-64-zero-None]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str49-64-zero-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str50-64-zero-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str51-64-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int16-+]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str52-64-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str53-64-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str54-128-None-None]" time="0.002" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str55-128-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str56-128-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str57-128-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int32-+]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str58-128-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str59-128-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str60-128-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str61-128-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str62-128-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int64-+]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str63-256-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str64-256-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str65-256-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str66-256-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str67-256-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str68-256-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str69-256-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint8-+]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str70-256-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str71-256-nan-upper]" time="0.000"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str72-512-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str73-512-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint16-+]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str74-512-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str75-512-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str76-512-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str77-512-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str78-512-nan-None]" time="0.000"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str79-512-nan-lower]" time="0.000"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint32-+]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str80-512-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str81-1024-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str82-1024-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str83-1024-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str84-1024-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str85-1024-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint64-+]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str86-1024-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str87-1024-nan-None]" time="0.000"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str88-1024-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str89-1024-nan-upper]" time="0.000"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float16-+]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str90-64-None-None]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str91-64-None-lower]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str92-64-None-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str93-64-zero-None]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float32-+]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str94-64-zero-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str95-64-zero-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str96-64-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str97-64-nan-lower]" time="0.000"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-^1]" time="0.009" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str98-64-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str99-128-None-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-bfloat16-&amp;1]" time="0.012" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str100-128-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str101-128-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str102-128-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str103-128-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-^1]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str104-128-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str105-128-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-^1]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str106-128-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str107-128-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-bfloat16-|1]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-^1]" time="0.009" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str108-256-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str109-256-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str110-256-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str111-256-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str112-256-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-^1]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str113-256-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str114-256-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str115-256-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-^1]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str116-256-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str117-512-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str118-512-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-^1]" time="0.009" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str119-512-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str120-512-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str121-512-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-|2]" time="0.018" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str122-512-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str123-512-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-bfloat16-/]" time="0.021" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str124-512-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str125-512-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-^1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-|2]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str126-1024-None-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int8-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str127-1024-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str128-1024-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str129-1024-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str130-1024-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str131-1024-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int8-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-^1]" time="0.009" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str132-1024-nan-None]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str133-1024-nan-lower]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str134-1024-nan-upper]" time="0.001"><skipped type="pytest.xfail" message="Padding with NaN is not supported for integer types" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int16-/]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str135-64-None-None]" time="0.004" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str136-64-None-lower]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-|2]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int32-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str137-64-None-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str138-64-zero-None]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int32-/]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str139-64-zero-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str140-64-zero-upper]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-|2]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str141-64-nan-None]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int64-==-real-real]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str142-64-nan-lower]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int64-/]" time="0.018" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str143-64-nan-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str144-128-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str145-128-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str146-128-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-|2]" time="0.018" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str147-128-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str148-128-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str149-128-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str150-128-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str151-128-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint8-/]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str152-128-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str153-256-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str154-256-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str155-256-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str156-256-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-|2]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str157-256-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint16-/]" time="0.009" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str158-256-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str159-256-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str160-256-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str161-256-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str162-512-None-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint32-/]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str163-512-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str164-512-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str165-512-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-|2]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str166-512-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str167-512-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str168-512-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint64-/]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str169-512-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str170-512-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str171-1024-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str172-1024-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str173-1024-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str174-1024-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float16-/]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str175-1024-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str176-1024-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str177-1024-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str178-1024-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str179-1024-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-|2]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str180-64-None-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float16-==-real-real]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float32-/]" time="0.015" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str181-64-None-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str182-64-None-upper]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str183-64-zero-None]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str184-64-zero-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str185-64-zero-upper]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float32-==-real-real]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str186-64-nan-None]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str187-64-nan-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str188-64-nan-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str189-128-None-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str190-128-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str191-128-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str192-128-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str193-128-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str194-128-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str195-128-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str196-128-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str197-128-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str198-256-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str199-256-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str200-256-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str201-256-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str202-256-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str203-256-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str204-256-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str205-256-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str206-256-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str207-512-None-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str208-512-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str209-512-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str210-512-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-bfloat16-+]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str211-512-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str212-512-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str213-512-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str214-512-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str215-512-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str216-1024-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str217-1024-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str218-1024-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str219-1024-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str220-1024-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str221-1024-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int8-+]" time="0.085" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str222-1024-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str223-1024-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str224-1024-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str225-64-None-None]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str226-64-None-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str227-64-None-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str228-64-zero-None]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str229-64-zero-lower]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str230-64-zero-upper]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str231-64-nan-None]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str232-64-nan-lower]" time="0.003" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str233-64-nan-upper]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str234-128-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str235-128-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str236-128-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str237-128-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str238-128-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str239-128-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str240-128-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str241-128-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str242-128-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str243-256-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str244-256-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str245-256-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str246-256-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str247-256-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str248-256-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str249-256-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-bfloat16-^0]" time="0.012" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str250-256-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str251-256-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str252-512-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str253-512-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str254-512-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str255-512-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str256-512-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int16-+]" time="0.016" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str257-512-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str258-512-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str259-512-nan-lower]" time="0.002" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str260-512-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str261-1024-None-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str262-1024-None-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str263-1024-None-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str264-1024-zero-None]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int32-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str265-1024-zero-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str266-1024-zero-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str267-1024-nan-None]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str268-1024-nan-lower]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_copy[dtypes_str269-1024-nan-upper]" time="0.001" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_ptr_matmul_no_scf[shape0-4]" time="0.175" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int64-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint8-+]" time="0.063" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint16-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-bfloat16-/]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-|3]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int8-/]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint64-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float32-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int64-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int64-/]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-|3]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_block_pointer" name="test_block_ptr_matmul_no_scf[shape1-8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint8-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint8-==-real-real]" time="0.139" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_ptr_matmul_no_scf[shape2-4]" time="0.008" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_ptr_matmul_no_scf[shape3-8]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint16-/]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-|3]" time="0.019" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_ptr_matmul_no_scf[shape4-4]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint32-/]" time="0.131" /><testcase classname="test.unit.language.test_block_pointer" name="test_block_ptr_matmul_no_scf[shape5-8]" time="0.005" /><testcase classname="test.unit.language.test_compile_errors" name="test_err_undefined_variable" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-|3]" time="0.138" /><testcase classname="test.unit.language.test_compile_errors" name="test_err_in_binary_operator" time="0.003" /><testcase classname="test.unit.language.test_compile_errors" name="test_err_static_assert" time="0.003" /><testcase classname="test.unit.language.test_compile_errors" name="test_err_in_unary_op" time="0.004" /><testcase classname="test.unit.language.test_compile_errors" name="test_err_in_binary_op" time="0.003" /><testcase classname="test.unit.language.test_compile_errors" name="test_err_in_nested_call" time="0.004" /><testcase classname="test.unit.language.test_compile_errors" name="test_err_in_builtin" time="0.004" /><testcase classname="test.unit.language.test_compile_errors" name="test_two_returns_no_err" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_not_const_annotate_no_err" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_returns_branched_on_constexpr" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_compile_errors" name="test_returns_branched_on_non_constexpr" time="0.005" /><testcase classname="test.unit.language.test_compile_errors" name="test_power_of_two_shapes" time="0.003" /><testcase classname="test.unit.language.test_compile_errors" name="test_power_of_two_shapes_2" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-bfloat16-&amp;0]" time="0.012" /><testcase classname="test.unit.language.test_compile_errors" name="test_captured_var_access" time="0.003" /><testcase classname="test.unit.language.test_compile_errors" name="test_global_var_access" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-&amp;0]" time="0.017" /><testcase classname="test.unit.language.test_compile_errors" name="test_constexpr_annotated_global_var_access" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_constexpr_global_var_access" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_global_type_alias_access" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_global_access_in_fn_default_arg" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_defaults_assign_no_err" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_where_warning" time="4.206" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-&amp;0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint64-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-bfloat16-+]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int8-+]" time="0.063" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int16-+]" time="0.061" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int64-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint8-+]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-bfloat16-|1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-bfloat16-/]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int8-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int16-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int32-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int64-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint16-+]" time="0.066" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-|2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint8-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-|2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint16-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int64-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-|2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-&amp;1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint64-/]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint16-==-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint64-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-|2]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float32-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-|3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-bfloat16-/]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-bfloat16-+]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int8-/]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int16-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-bfloat16-&amp;0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int32-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int8-+]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int64-/]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint8-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int16-+]" time="0.068" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint64-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-&amp;0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-&amp;0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float32-/]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint32-==-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint64-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int32-+]" time="0.066" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int64-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint8-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint64-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float16-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float32-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-bfloat16-|1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-bfloat16-/]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int8-/]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int16-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int32-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int64-/]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint8-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint64-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float16-/]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-|2]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint16-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float32-/]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-&amp;1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint64-==-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-bfloat16-+]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float16-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float32-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int8-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int16-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int32-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int64-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint8-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint64-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-bfloat16-/]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-|3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-bfloat16-&amp;0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int8-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-&amp;0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int16-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-|3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int32-/]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int64-/]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint8-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int16-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint32-/]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint64-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-&amp;0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float16-==-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-bfloat16-+]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float32-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int8-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int32-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int64-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint8-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint64-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float16-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float32-+]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-bfloat16-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-bfloat16-/]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-|2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int8-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int16-/]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-|2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int32-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-|2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int64-/]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint8-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint16-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-|2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint32-/]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint64-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float16-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float32-/]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float32-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-bfloat16-+]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int8-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-|3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-bfloat16-/]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-bfloat16-&amp;0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-&amp;0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int8-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int32-/]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-|3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int64-/]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint8-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-|3]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint16-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int16-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-|3]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint64-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float16-/]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int32-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float32-/]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint8-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint16-==-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint32-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint64-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float16-==-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-==-real-real]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int32-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-bfloat16-|1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-&amp;1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-bfloat16-/]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int8-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-&amp;1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int8-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-&amp;1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int16-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-|3]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int16-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-bfloat16-&amp;0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint8-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-&amp;0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int32-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int32-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-bfloat16-|1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint16-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-|2]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-&amp;1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint32-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint8-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-bfloat16-&amp;0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-&amp;0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-|3]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-&amp;0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-&amp;0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint8-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_compile_errors" name="test_fp8_support[dtype0]" time="0.002" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_compile_errors" name="test_fp8_support[dtype1]" time="0.004" /><testcase classname="test.unit.language.test_compile_errors" name="test_fp8_support[dtype2]" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_fp8_support[dtype3]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_compile_errors" name="test_fp8_support[dtype4]" time="0.002" /><testcase classname="test.unit.language.test_compile_errors" name="test_min_dot_size[dtype0]" time="0.005" /><testcase classname="test.unit.language.test_compile_errors" name="test_min_dot_size[dtype1]" time="0.005" /><testcase classname="test.unit.language.test_compile_errors" name="test_min_dot_size[dtype2]" time="0.004" /><testcase classname="test.unit.language.test_compile_errors" name="test_max_num_imprecise_acc_limit" time="0.005" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float16-float32]" time="0.017" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[bfloat16-float32]" time="0.007" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e5-float16]" time="0.007" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e5-bfloat16]" time="0.004" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e5-float32]" time="0.003" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e4b15-float16]" time="0.006" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e4b15-float32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint16-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e4nv-float16]" time="0.006" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e4nv-bfloat16]" time="0.003" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e4nv-float32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e4b8-float32]" time="0.004" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e4b8-float16]" time="0.004" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e5b16-float32]" time="0.003" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_upcast[float8e5b16-float16]" time="0.004" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-float16-rtne-1199562752]" time="3.877" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-bfloat16-|1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float16-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint16-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-&amp;1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint32-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-&amp;1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float32-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint32-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-bfloat16-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float16-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-bfloat16-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-bfloat16-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float16-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float32-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int8-&amp;0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int16-&amp;0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int8-+]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int32-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int64-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint8-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int16-+]" time="0.041" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float32-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint16-&amp;0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint32-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int32-+]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint64-&amp;0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float16-&amp;0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-bfloat16-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float32-&amp;0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int64-+]" time="0.043" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint8-+]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint16-+]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint32-+]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint64-+]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float64-==-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float16-+]" time="0.030" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-bfloat16-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float32-+]" time="0.030" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-^2]" time="0.045" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int8-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int16-&amp;1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-^2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int32-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int64-&amp;1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint8-&amp;1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint16-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-^2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint32-&amp;1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint64-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float16-&amp;1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float32-&amp;1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-^2]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int8-!=-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int16-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int8-/]" time="0.031" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int32-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int64-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-bfloat16-+]" time="0.051" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int16-/]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint8-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int32-/]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint16-!=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int8--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint32-!=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int64-/]" time="0.031" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int16--]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint64-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int32--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float16-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint8-/]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int64--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float32-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint16-/]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-bfloat16-&amp;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint32-/]" time="0.031" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint16--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-|0]" time="0.042" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint32--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint64-/]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint64--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-^3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-|0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float16--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float16-/]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-^3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-|0]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float32--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-|0]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float32-/]" time="0.031" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-^3]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float64-/]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-^3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-^3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-^3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int8-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int16-!=-real-real]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int32-!=-real-real]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int64-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint8-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-bfloat16--]" time="0.031" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint16-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-bfloat16-/]" time="0.048" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint32-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint64-!=-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int16--]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int8-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int32--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float16-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int32-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float32-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int64--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int64-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint8-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-bfloat16-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint16-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint16--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint64-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint32--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-^2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float16-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float32-%]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint64--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float16--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-^2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float32--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-^2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-^2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int8-!=-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int16-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int32-!=-real-real]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-bfloat16-%]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int64-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int8-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint8-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-bfloat16--]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int32-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint16-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int64-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint32-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint8-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int8--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint16-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint64-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint32-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int16--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint64-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-bfloat16-|0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float16-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float16-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int32--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float32-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float32-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-^3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int64--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint8--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint16--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-^3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint32--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint64--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-^3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float16--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float32--]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-^3]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-|0]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-bfloat16-%]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int8-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int8-%]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int16-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int16-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int32-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int32-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int64-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int64-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint8-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint16-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint8-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint32-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint64-%]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint16-!=-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float32-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-bfloat16--]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint32-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-|1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint64-!=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-bfloat16-^1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float16-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int16--]" time="0.140" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-^2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float32-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-|1]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-^2]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-^2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int32--]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int64--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint8--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-^2]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint16--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-bfloat16-%]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint32--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int8-%]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint64--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int32-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float16--]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int64-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float32--]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint8-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int8-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint16-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint64-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int16-!=-real-real]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float16-%]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float32-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int32-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int64-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint8-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-bfloat16-|0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint16-!=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-float16-rtz-1199562752]" time="1.147" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint32-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint64-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-^3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float16-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-^3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float32-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-bfloat16--]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-^3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int8--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int16--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-bfloat16-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int32--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int8-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int16-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int64--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int32-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int64-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint8--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint8-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint16--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint16-%]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint32-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint32--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint64-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint64--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float16-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float32-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float16--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float32--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int8-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int16-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int32-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int64-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint8-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-bfloat16-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint16-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-^2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint32-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-^2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint64-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-^2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float16-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-^2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float32-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-^2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-^2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-^2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-bfloat16-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-^2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int8-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int16-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-bfloat16--]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-^2]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int64-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int8--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint8-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int16--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint32-%]" time="0.130" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int32--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int64--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint8--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint16--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint32--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint64--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float16--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint64-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float32--]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float16-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int8-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int16-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-bfloat16-|0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int32-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int64-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint8-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-^3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-|0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint16-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-^3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint32-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint64-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float16-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-^3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float32-!=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-|0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-^3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-^3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-bfloat16--]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-bfloat16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int8-%]" time="0.010" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-bfloat16-rtne-2139029504]" time="4.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int8--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int16-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int16--]" time="0.028" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int64-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint8-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int32--]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint32-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int64--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint64-%]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint8--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float32-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint16--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint32--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint64--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int8-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float16--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int16-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float32--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-bfloat16-^1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int32-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int64-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint8-!=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint16-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-|1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-^2]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint32-!=-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-|1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint64-!=-real-real]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-^2]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float16-!=-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-^2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float32-!=-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-bfloat16-%]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-^2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int8-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int16-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int32-%]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int64-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint8-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint32-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint64-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-bfloat16--]" time="0.042" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float16-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float32-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int16--]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int32--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int64--]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-bfloat16-|0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint16--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint32--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int8-!=-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint64--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int16-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float16--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int32-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float32--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-|0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-^3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int64-!=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint8-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint16-!=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-bfloat16-%]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-|0]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint32-!=-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-^3]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-bfloat16-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int8-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint64-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int16-%]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int32-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float16-!=-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int64-%]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint8-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint16-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float32-!=-real-real]" time="0.142" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint32-%]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint64-%]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float16-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float32-%]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-bfloat16--]" time="0.043" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int8-|0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int16-|0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int16--]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int32-|0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int32--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int64-|0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint8-|0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int64--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint16-|0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint8--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint32-|0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-bfloat16-^1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint16--]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint64-|0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-^2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-bfloat16-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float16-|0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int8-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint32--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-^2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float32-|0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int16-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint64--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-^2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int64-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float16--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint8-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float32--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint16-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-bfloat16-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint32-%]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-|1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint64-%]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int8-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float16-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float32-%]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int16-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int32-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int64-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint8-!=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint16-!=-real-real]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint32-!=-real-real]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint64-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float16-!=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int8-|1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int16-|1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-!=-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int32-|1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int64-|1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-bfloat16--]" time="0.044" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint8-|1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint16-|1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int8--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint32-|1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-bfloat16-%]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint64-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int16--]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int8-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float16-|1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int32--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float32-|1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int64--]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint8--]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-&amp;2]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint16--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-bfloat16-|0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-^3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint32--]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-&amp;2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-^3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint64--]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-&amp;2]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float16--]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float32--]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-|0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-&amp;2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int8-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-&amp;2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-|0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int16-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-bfloat16-|]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-^0]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-^0]" time="0.134" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-bfloat16--]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int16-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int8--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-bfloat16-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-^0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-^2]" time="0.145" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int32-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-^0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-&amp;3]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-^0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-&amp;3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-^0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-&amp;3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-^0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-&amp;3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-&amp;3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-|1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-&amp;3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-^2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-|1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-&amp;3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-^2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-|1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-&amp;3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-|1]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-|1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-^2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int16--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int32-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-^2]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-^2]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-^1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-^1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-bfloat16-&amp;1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int32--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-&amp;2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-bfloat16-|0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint8-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-&amp;2]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-|0]" time="0.028" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-^1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-^3]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-^1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-|0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-^3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-&amp;2]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-&amp;2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-|0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-^3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-|0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-^3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint8-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint16-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-bfloat16-^0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-^0]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-^0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-^0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-^0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-&amp;3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-^0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint8--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-bfloat16-^1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-&amp;3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint32-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint16-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-|1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-^2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-&amp;3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-^0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-&amp;3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-|1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-^2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-&amp;3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-|1]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-&amp;3]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-^2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-^2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-^2]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint16--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint32-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-^1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-^1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-bfloat16-&amp;1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-bfloat16-|0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-&amp;2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-^1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-|0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-|0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-&amp;2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-^1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-|0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-^3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint32--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-|0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-&amp;2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-^3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-|0]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float16-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-^3]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-&amp;2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-|0]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-|0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-^3]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-^3]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-^3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-bfloat16-^0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-^0]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float32-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-^0]" time="0.017" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-bfloat16-rtz-2139029504]" time="0.980" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float16-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-&amp;3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-^0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-&amp;3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-^0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-|1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-bfloat16-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-|1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-&amp;3]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-^2]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-|1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float16--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-^2]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-^2]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float32-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-bfloat16-&amp;1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float32--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-^1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-&amp;2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-^1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-bfloat16-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-bfloat16-|0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-&amp;2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-&amp;2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-&amp;2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-^1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float64-!=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-&amp;2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-&amp;2]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-|0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-&amp;2]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-^3]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int8-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int16-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int64-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint8-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint16-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint32-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int8-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint64-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float16-%]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int16-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float32-%]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-bfloat16-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float64-%]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-^0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int32-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int64-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-|1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-bfloat16-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint8-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint16-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint32-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-^2]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint64-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-^0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-float8e5-rtne-1197473792]" time="3.628" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float16-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-^2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-bfloat16--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-^2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float32-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-^0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-&amp;3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-&amp;3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-&amp;3]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-bfloat16-%]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_addptr[int8-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[int8-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[int16-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[int16-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[int32-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_addptr[int32-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[int64-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[int64-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint8-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint8-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint16-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint16-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint32-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint32-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-bfloat16-|0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint64-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_addptr[uint64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_addptr[float16-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int8--]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_addptr[float16-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_addptr[float32-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_addptr[float32-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_addptr[float64-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int16--]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int8-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-^1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-|0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int32--]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int16-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-|0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-^1]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-|0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int32-&gt;-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-^3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-^1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int64--]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-^3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-bfloat16-&amp;1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int64-&gt;-real-real]" time="0.057" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-^1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-&amp;2]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint8--]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-^1]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-&amp;2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-^1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint8-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint16--]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-^1]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint16-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint32--]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint32-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint64-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_addptr[float64-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint64-&gt;-real-real]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float16-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-&amp;2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float32-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint64--]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float16-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-&amp;2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float32-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float16--]" time="0.028" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float32--]" time="0.028" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float64--]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int8-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int32-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-bfloat16-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-int64-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint8-|1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint32-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_addptr[bfloat16-0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-uint64-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_addptr[bfloat16-1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float16-|1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float32-|1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int8-int8]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int8-int16]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int8-int32]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int8-int64]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-bfloat16-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int16-int8]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int16-int16]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int8-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int16-int32]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-^0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-bfloat16--]" time="0.049" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int16-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int16-int64]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-^0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int32-int8]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int32-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int32-int16]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int64-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-&amp;3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int8-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int32-int32]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint8-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint16-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int32-int64]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-&amp;3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int32-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int64-int8]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint32-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int64-*]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float32-bfloat16-|0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int64-int16]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint64-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-&amp;3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint8-*]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int64-int32]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float16-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-int64-int64]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float32-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint32-*]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint8-uint8]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint64-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint8-uint16]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float16-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint8-uint32]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float32-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint8-uint64]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint16-uint8]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint16-uint16]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint16-uint32]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint16-uint64]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint32-uint8]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint32-uint16]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int8-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint32-uint32]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int16-^1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int32-^1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint32-uint64]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-bfloat16-&amp;1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-int64-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint64-uint8]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int8-&gt;-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint64-uint16]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint8-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int16-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint16-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint64-uint32]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int32-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-&amp;2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_floordiv[1-uint64-uint64]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint32-^1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int64-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-bfloat16-*]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_unsigned_name_mangling" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-uint64-^1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-&amp;0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint8-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int8-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-&amp;0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float16-^1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint16-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float32-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-&amp;0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint32-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-&amp;2]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-&amp;0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint64-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int64-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-&amp;0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float16-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint8-*]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-&amp;0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float32-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint16-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-&amp;0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-&amp;0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint64-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-&amp;0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float32-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint8-bfloat16-^0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-^0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-&amp;3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int8-&gt;-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-&amp;3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int16-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int32-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-&amp;1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int64-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-^0]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-&amp;1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint8-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-bfloat16-*]" time="0.043" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-&amp;1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint16-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-^0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-&amp;3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint32-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-&amp;1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int8-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint64-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-&amp;1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float16-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-&amp;1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int32-*]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-&amp;1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float32-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int64-*]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-&amp;1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint8-*]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-&amp;1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint32-*]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint64-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float32-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int8-^1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int16-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-bfloat16-&amp;1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int32-^1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-int64-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint8-^1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int8-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint16-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int16-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint32-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-bfloat16-&amp;0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int32-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-&amp;2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-&amp;0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-uint64-^1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int64-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float16-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-&amp;0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint8-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float32-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-&amp;0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint16-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-&amp;2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-&amp;0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint32-&gt;-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-&amp;0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-&amp;2]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-bfloat16-*]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint64-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-&amp;0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-&amp;0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float16-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int8-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-&amp;0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float32-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int64-*]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint8-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint16-*]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-uint64-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float16-*]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint16-bfloat16-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int8-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int16-&gt;-real-real]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-&amp;1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-&amp;3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-^0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-&amp;1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int32-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-&amp;1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int64-&gt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-&amp;3]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-&amp;1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint8-&gt;-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-&amp;1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint16-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-&amp;1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint32-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-&amp;1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint64-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-&amp;1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float16-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-bfloat16-*]" time="0.029" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-&amp;1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-&amp;1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float32-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-&amp;1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int8-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int32-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-int64-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint8-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint16-*]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int8-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint32-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int16-^1]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-uint64-*]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-bfloat16-&amp;1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int32-^1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-&amp;2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float16-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-int64-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-&amp;2]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint8-^1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint16-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint32-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-bfloat16-&amp;0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-&amp;2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-&amp;0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int8-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-uint64-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int16-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-&amp;0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float16-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float32-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-&amp;0]" time="0.141" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int32-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-&amp;2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int64-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint8-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-^2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint16-&gt;-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint32-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint64-&gt;-real-real]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float16-&gt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-&amp;0]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float32-&gt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-&amp;0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint8-bfloat16-*]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-&amp;0]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int8-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-&amp;0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-&amp;0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int32-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint32-bfloat16-^0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-&amp;0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-int64-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-&amp;0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-float8e5-rtz-1197473792]" time="1.915" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint8-*]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint16-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint32-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-&amp;3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-^0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-uint64-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-&amp;3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float16-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-^0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-&amp;3]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-^0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-&amp;3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-^0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int8-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-&amp;3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int16-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int32-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int64-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint8-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint16-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-&amp;1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int8-|1]" time="0.003"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint32-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint64-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-&amp;1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float16-&gt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int16-&lt;=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float32-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int64-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint8-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint16-bfloat16-*]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int8-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint32-&lt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int16-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int8-^1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int32-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float16-&lt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int16-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-bfloat16-&amp;1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-int64-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int32-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint8-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-int64-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-&amp;2]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint16-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint8-^1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-&amp;2]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int16-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint16-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-uint64-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint32-^1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-&amp;2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-&amp;2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float16-*]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-uint64-^1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-&amp;2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float32-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float16-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float32-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int8-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int16-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int32-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int64-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint8-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint16-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint32-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int8-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint64-&gt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float16-&gt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int32-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint32-bfloat16-*]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-uint64-bfloat16-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-&amp;3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int8-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-^0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-^0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-&amp;3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int16-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-^0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-&amp;3]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint8-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-&amp;3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int32-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint16-^0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint32-^0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-int64-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-uint64-^0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float16-^0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint8-*]" time="0.143" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float32-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int16-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-int64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int8-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint16-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint32-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-uint64-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float16-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_pos-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float32-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int8-^1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int16-^1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int32-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int32-^1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float16-int64-^1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape20-0-1-float16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape21-0-1-float32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape22-0-1-uint64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint8-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape23-0-1-int64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape24-0-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int16-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_pos-acquire]" time="0.127" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-uint64-bfloat16-*]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int8-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape25-1-1-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape26-1-1-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape27-1-1-uint64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int16-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape28-1-1-int64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape29-1-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int32-*]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_pos-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint16-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-int64-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint8-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int32-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint16-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint32-*]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-uint64-*]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float16-*]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float32-*]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape30-0-1-float16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape31-0-1-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape32-0-1-uint64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape33-0-1-int64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape34-0-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_pos-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint8-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint32-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-float8e4nv-rtne-1138753536]" time="3.277" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float16-bfloat16-*]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape35-1-1-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape36-1-1-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape37-1-1-uint64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape38-1-1-int64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int8-*]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape39-1-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int16-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_pos-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int32-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint16-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-int64-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-uint64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint8-*]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint16-*]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint8-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint32-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-uint64-*]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float16-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float32-*]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape40-0-1-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape41-0-1-float32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape42-0-1-uint64]" time="0.129" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_pos-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint32-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float16-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape43-0-1-int64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape44-0-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint16-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_pos-release]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float32-bfloat16-*]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_pos-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int8-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape45-1-1-float16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape46-1-1-float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape47-1-1-uint64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape48-1-1-int64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape49-1-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float32-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-^3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint32-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_pos-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_pos-acq_rel]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_pos-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int16-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape50-0-1-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape51-0-1-float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape52-0-1-uint64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape53-0-1-int64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape54-0-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float16-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-float64-|1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-bfloat16-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_pos-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_pos-acq_rel]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_pos-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape55-1-1-float16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape56-1-1-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int32-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape57-1-1-uint64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape58-1-1-int64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape59-1-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float32-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-float64-bfloat16-|0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float16-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int8-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int16-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int32-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int64-^0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_pos-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint8-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint16-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint32-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint64-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float16-^0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw_block[1]" time="0.057" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float32-^0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float64-^0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-int64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_cas[1-None]" time="0.106" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-|2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-|2]" time="0.030" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_pos-relaxed]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float32-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_pos-relaxed]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_cas[1-acquire]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-|2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_pos-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_cas[1-release]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-|2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_atomic_cas[1-acq_rel]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-|2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_atomic_cas[1-relaxed]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_cas[1-None]" time="0.051" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-|2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-|2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int8-^1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-|2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_cas[1-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int16-^1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_cas[1-release]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_cas[1-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int32-^1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_cas[1-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint8-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_load_scope_sem_coop_grid_cta_not_one" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-int64-^1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_load_scope_sem_coop_grid_cta_one" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-int8-False-1024]" time="0.154" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint8-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-==-nan-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint16-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint32-^1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-==-real-nan]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-uint64-^1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float16-^1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-==-nan-nan]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_pos-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float32-^1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-!=-nan-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-float64-^1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_pos-relaxed]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float64-&gt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_pos-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-!=-real-nan]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-!=-nan-nan]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;-nan-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-int16-False-1024]" time="0.196" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;-real-nan]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;-nan-nan]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;-nan-real]" time="0.053" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint16-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;-real-nan]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;-nan-nan]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int8-|3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;=-nan-real]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int16-|3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_pos-relaxed]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int32-|3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_pos-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;=-real-nan]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-int64-|3]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;=-nan-nan]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-int32-False-1024]" time="0.201" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int8-&lt;-real-real]" time="0.147" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;=-nan-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint8-|3]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-bfloat16-bfloat16-^]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;=-real-nan]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint8-&lt;&lt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint16-|3]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;=-nan-nan]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint16-&lt;&lt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint32-|3]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_broadcast[int8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_broadcast[int16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint32-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_broadcast[int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-uint64-|3]" time="0.152" /><testcase classname="test.unit.language.test_core" name="test_broadcast[int64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint64-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_broadcast[uint8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_broadcast[uint16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_broadcast[uint32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint8-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_broadcast[uint64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_broadcast[float16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_broadcast[float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_broadcast[float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint16-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int16-&lt;-real-real]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint32-&lt;&lt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint32-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-min_neg-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int32-&lt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint64-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-int64-False-1024]" time="0.211" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int64-&lt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-min_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint8-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-min_neg-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint8-&lt;-real-real]" time="0.070" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint16-&lt;&lt;]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float16-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint32-&lt;&lt;]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint64-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint16-&lt;-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint8-&lt;&lt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint32-&lt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint16-&lt;&lt;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint64-&lt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_broadcast[bfloat16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint32-&lt;&lt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_slice" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float16-&lt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_invalid_slice" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint64-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_expand_dims" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_expand_dims_error_cases" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float32-&lt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint8-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-uint8-False-1024]" time="0.152" /><testcase classname="test.unit.language.test_core" name="test_invalid_pid_axis" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_where[1-int8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint16-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-min_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_where[1-int16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-min_neg-None]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_where[1-int32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-min_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_where[1-int64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-uint64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint32-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-min_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_where[1-uint8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-min_neg-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_where[1-uint16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_where[1-uint32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint64-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_where[1-uint64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_where[1-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_where[1-float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint8-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_where[1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint16-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int8-bfloat16-|1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint32-&lt;&lt;]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-|2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-uint16-False-1024]" time="0.202" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint64-&lt;&lt;]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-|2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint8-&lt;&lt;]" time="0.144" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-|2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-min_neg-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-|2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int8-&lt;-real-real]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-min_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-|2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-min_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-min_neg-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_where[1-bfloat16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-|2]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float16-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_where[1-*int32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int16-&lt;-real-real]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_where_broadcast[1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint16-&lt;&lt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int8- -x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-|2]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int16- -x]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int32- -x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint32-&lt;&lt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int64- -x]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int32-&lt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-uint32-False-1024]" time="0.199" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-uint8- -x]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-uint16- -x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-uint32- -x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int64-&lt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint64-&lt;&lt;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-uint64- -x]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-float16- -x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-float32- -x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint8-&lt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint8-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-float64- -x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint16-&lt;&lt;]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint16-&lt;-real-real]" time="0.068" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint32-&lt;&lt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint64-&lt;&lt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint8-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint32-&lt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-min_neg-acquire]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-min_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint16-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-min_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint64-&lt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-min_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-min_neg-acquire]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint32-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-min_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float16-&lt;-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-min_neg-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int8-uint64-&gt;&gt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-uint64-False-1024]" time="0.208" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float32-&lt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint8-&gt;&gt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float32-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint16-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int8-|3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint32-&gt;&gt;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int16-|3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int16-uint64-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int32-|3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-bfloat16- -x]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint8-&gt;&gt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int8- ~x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-int64-|3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int16- ~x]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int32- ~x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint16-&gt;&gt;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_unary_op[1-int64- ~x]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint8-|3]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-exp-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-exp-3.0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint32-&gt;&gt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-log-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-log-3.0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint16-|3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-cos-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int32-uint64-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-cos-3.0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-sin-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-min_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint32-|3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-min_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-sin-3.0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-exp2-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-min_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint8-&gt;&gt;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-exp2-3.0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-min_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-log2-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-min_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-uint64-|3]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-float16-False-1024]" time="0.198" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-log2-3.0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-min_neg-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint16-&gt;&gt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-sqrt-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-sqrt-3.0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float16-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-floor-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint32-&gt;&gt;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float32-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-floor-3.0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_op[float32-ceil-x]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_math_op[float32-ceil-3.0]" time="0.004" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-float8e4b8-rtne-1131413504]" time="0.001"><skipped type="pytest.xfail" message="float8e4b8 downcast with RTNE rounding tests only supported on AMDGPU MI300" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-int64-uint64-&gt;&gt;]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-exp-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float32-float8e5b16-rtne-1197473792]" time="0.001"><skipped type="pytest.xfail" message="float8e5b16 downcast with RTNE rounding tests only supported on AMDGPU MI300" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint8-&gt;&gt;]" time="0.026" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[bfloat16-float8e5-rtne-18272]" time="3.098" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int8-&lt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint16-&gt;&gt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint32-&gt;&gt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int16-&lt;-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint8-uint64-&gt;&gt;]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int32-&lt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint8-&gt;&gt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int64-&lt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint16-&gt;&gt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint8-&lt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-float32-False-1024]" time="0.203" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint32-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint16-&lt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-min_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-min_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint16-uint64-&gt;&gt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-min_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint32-&lt;-real-real]" time="0.073" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-min_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-min_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-min_neg-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint8-&gt;&gt;]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint16-&gt;&gt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint32-&gt;&gt;]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint64-&lt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int16-bfloat16-|1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint32-uint64-&gt;&gt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-exp-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-|2]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float16-&lt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint8-&gt;&gt;]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float32-&lt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-|2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint16-&gt;&gt;]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-float64-bfloat16-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint32-&gt;&gt;]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-|2]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_shift_op[1-uint64-uint64-&gt;&gt;]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int8-==-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-|2]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int16-==-real-real]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-min_neg-release]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-min_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-min_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int32-==-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-min_neg-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int64-==-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-|2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-|2]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint8-==-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-|2]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape114-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape115-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape116-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape117-0-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape118-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-log-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape119-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape120-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape121-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-int8-False-1024]" time="0.165" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape122-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape123-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int8-&lt;-real-real]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int8-*]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape124-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape125-2-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int16-&lt;-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape126-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape127-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int16-*]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape128-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int32-&lt;-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape129-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-min_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape130-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int64-&lt;-real-real]" time="0.042" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape131-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-min_neg-release]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int32-*]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-min_neg-release]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape132-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-min_neg-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape133-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape134-2-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint8-&lt;-real-real]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape135-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-int64-*]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape136-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape137-2-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint16-&lt;-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape138-2-False]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int8-|3]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-int16-False-1024]" time="0.155" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint32-&lt;-real-real]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape139-3-False]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint8-*]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int16-|3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape140--1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape141-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint64-&lt;-real-real]" time="0.072" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int32-|3]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape142-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint16-*]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape143-0-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape144-1-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-int64-|3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-log-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape145-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape146-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint8-|3]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint32-*]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape147-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape148-1-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int64-512]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint16-|3]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape149-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape150-1-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint8-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-uint64-*]" time="0.042" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape151-None-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint32-|3]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape152-None-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape153-None-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-int32-False-1024]" time="0.203" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-min_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-uint64-|3]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape154-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-min_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape155-1-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-min_neg-release]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float16-*]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint32-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-min_neg-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape156-2-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float16-|3]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape157-0-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float32-|3]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape158-1-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape159-2-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float32-*]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-float64-|3]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint64-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape160-0-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-uint64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape161-1-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape162-2-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape163-0-True]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-float64-*]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape164-1-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape165-2-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape166-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape167-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape168-2-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape169-None-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape170-None-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape171-None-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-cos-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-xor_sum-bool-shape172-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-xor_sum-bool-shape173-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-xor_sum-bool-shape174-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-int64-False-1024]" time="0.216" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-xor_sum-bool-shape175-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-xor_sum-bool-shape176-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-xor_sum-bool-shape177-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-min_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-int8-shape0-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-min_neg-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-int8-shape1-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-int8-shape2-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-min_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-int8-shape3-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-min_neg-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-int8-shape4-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-int16-shape5-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-int16-shape6-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-int16-shape7-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-int16-shape8-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int32-bfloat16-|1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-int16-shape9-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int8-|2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-int32-shape10-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-int32-shape11-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int16-|2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-int32-shape12-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-bfloat16-bfloat16-*]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-int32-shape13-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-int32-shape14-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int32-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-int64-shape15-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-int64-shape16-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-int64-|2]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int8-/]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-int64-shape17-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-int64-shape18-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-uint8-False-1024]" time="0.158" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint8-|2]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int16-/]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-int64-shape19-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-uint8-shape20-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-cos-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-uint8-shape21-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint16-|2]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int32-/]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-uint8-shape22-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-uint8-shape23-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bitwise_op[1-int64-uint32-|2]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int64-/]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-min_neg-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-uint8-shape24-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-min_neg-acq_rel]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-uint16-shape25-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-min_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint8-/]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-uint16-shape26-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-uint16-shape27-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-min_neg-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint16-/]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-uint16-shape28-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-uint16-shape29-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-uint32-shape30-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-uint32-shape31-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-uint32-shape32-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint8-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-uint32-shape33-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-uint32-shape34-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-uint16-False-1024]" time="0.162" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-uint64-shape35-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-uint64-shape36-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-uint64-shape37-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-uint64-shape38-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-uint64-shape39-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float16-shape40-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint64-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float16-shape41-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-uint64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float16-shape42-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float16-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float16-shape43-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float16-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float16-shape44-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float32-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape45-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape46-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape47-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-sin-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape48-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-min_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape49-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float64-shape50-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-min_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-uint32-False-1024]" time="0.199" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-min_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-min_neg-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float64-shape51-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-uint64-False-1024]" time="0.218" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-min_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-min_neg-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-bfloat16-32]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-min_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-min_neg-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-min_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-sin-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-min_neg-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-bfloat16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-min_neg-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int8-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int64-64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int64-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-int64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint8-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float64-shape52-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-float16-False-1024]" time="0.161" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-min_neg-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-min_neg-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint64-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-min_neg-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-min_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-min_neg-relaxed]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-min_neg-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-uint64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-exp2-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float32-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-float32-False-1024]" time="0.214" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float64-shape53-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-bfloat16-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-bfloat16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-bfloat16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-min_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-min_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int16-512]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-min_neg-relaxed]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-min_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-min_neg-relaxed]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int32-64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-min_neg-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int64-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int64-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-int64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-exp2-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-bfloat16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-bfloat16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float64-shape54-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-bfloat16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-uint64-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int32-512]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float32-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int64-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int64-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-max_pos-None]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-max_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-int64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-max_pos-None]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-max_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint8-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-max_pos-None]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-max_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-max_pos-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint16-512]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-int8-False-1024]" time="0.159" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-bfloat16-shape55-1-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-bfloat16-shape56-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint64-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-bfloat16-shape57-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-log2-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-uint64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-bfloat16-shape58-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-bfloat16-shape59-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape60-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape61-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape62-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape63-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape64-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape65-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape66-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape67-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape68-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape69-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape70-0-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-int16-False-1024]" time="0.162" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-max_pos-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape71-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-max_pos-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape72-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-max_pos-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-max_pos-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape73-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-max_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape74-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-max_pos-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape75-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape76-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape77-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape78-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape79-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape80-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape81-1-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape82-0-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape83-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-log2-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape84-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape85-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape86-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape87-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-int32-False-1024]" time="0.162" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape88-0-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape89-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape90-None-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape91-None-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-bfloat16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape92-None-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape93-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-bfloat16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape94-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-bfloat16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-max_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape95-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-max_pos-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-max_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape96-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int8-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-max_pos-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape97-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-max_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-max_pos-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape98-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape99-0-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape100-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int32-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape101-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape102-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int64-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape103-1-False]" time="0.007" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[bfloat16-float8e4nv-rtne-17376]" time="3.110" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape104-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-int64-False-1024]" time="0.243" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-int64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape105-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape106-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape107-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape108-0-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape109-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint16-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape110-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-sqrt-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape111-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape112-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape113-2-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape114-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape115-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-uint64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape116-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape117-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape118-1-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape119-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape120-0-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape121-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-max_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape122-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-max_pos-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape123-0-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape124-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape125-2-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape126-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape127-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-uint8-False-1024]" time="0.161" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape128-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape129-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape130-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-bfloat16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape131-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-bfloat16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape132-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-bfloat16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape133-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-sum-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape134-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape135-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape136-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-sqrt-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape137-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape138-2-False]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape139-3-False]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int32-64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape140--1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int64-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape141-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int64-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape142-1-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-max_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-uint16-False-1024]" time="0.168" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-int64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape143-0-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-max_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape144-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-max_pos-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape145-0-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-bfloat16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape146-1-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-bfloat16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape147-0-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmax-tie-break-left-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape148-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape149-0-True]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape150-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int8-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape151-None-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape152-None-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint64-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int16-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape153-None-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-uint64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape154-0-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int32-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float16-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape155-1-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float16-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int32-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape156-2-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int64-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape157-0-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int64-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int64-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape158-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-int64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape159-2-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-uint32-False-1024]" time="0.160" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape160-0-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint8-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape161-1-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape162-2-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-floor-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape163-0-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape164-1-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-max_pos-acquire]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmin-float32-shape165-2-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape166-0-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-max_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape167-1-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-max_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-argmax-float32-shape168-2-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-max_pos-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-min-float32-shape169-None-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-uint64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-max-float32-shape170-None-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-sum-float32-shape171-None-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-xor_sum-bool-shape172-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-xor_sum-bool-shape173-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-xor_sum-bool-shape174-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-xor_sum-bool-shape175-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-uint64-False-1024]" time="0.227" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-xor_sum-bool-shape176-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape677-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape678-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape679-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape680-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape681-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape682-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape683-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape684-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape685-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape686-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape687-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape688-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape689-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape690-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape691-1-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-floor-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape692-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-max_pos-release]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape693-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-max_pos-release]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape694-1-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-max_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape695-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape696-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-max_pos-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape697-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape698-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape699-1-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape700-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape701-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape702-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-float16-False-1024]" time="0.168" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape703-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape704-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape705-1-True-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape706-1-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape707-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape708-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-bfloat16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape709-1-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape710-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-bfloat16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape711-1-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-with-indices-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape712-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape713-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int8-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape714-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape715-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape716-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int16-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape717-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int32-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape718-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape719-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int32-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape720-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int64-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape721-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-max_pos-release]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int64-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape722-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape723-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-float32-False-1024]" time="0.175" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-max_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-int64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape724-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-ceil-x]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape725-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-max_pos-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape726-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape727-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape728-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape729-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape730-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint32-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape731-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint32-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape732-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape733-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape734-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape735-1-False-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-uint64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape736-1-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape737-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape738-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape739-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float32-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape740-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape741-1-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape742-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape743-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape744-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape745-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape746-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape747-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape748-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape749-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape750-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape751-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-max_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-max_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape752-1-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-max_pos-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-max_pos-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape753-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-max_pos-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape754-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape755-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_math_op[float64-ceil-3.0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-bfloat16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape756-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape757-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-bfloat16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape758-0-True-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape759-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape760-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape761-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape762-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape763-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape764-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape765-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape766-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape767-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape768-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape769-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-int8-False-1024]" time="0.171" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape770-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int64-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape771-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape772-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int64-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-int64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape773-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape774-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint8-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape775-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape776-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-bfloat16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-bfloat16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape777-0-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-bfloat16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-sum-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape778-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-max_pos-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-int8-shape0-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape779-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint32-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-max_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape780-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-int8-shape1-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint32-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape781-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-int8-shape2-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-max_pos-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape782-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-int8-shape3-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-max_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape783-0-True-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-max_pos-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-int8-shape4-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape784-0-True-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-uint64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-int16-shape5-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape785-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-int16-shape6-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape786-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-int16-shape7-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape787-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_erf_op[float32]" time="0.298" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape788-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-int16-shape8-1-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float32-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape789-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-int16-False-1024]" time="0.173" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-int16-shape9-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float32-128]" time="0.058" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape790-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-int32-shape10-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape791-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-int32-shape11-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape792-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-int32-shape12-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape793-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape794-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-int32-shape13-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape795-0-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-int32-shape14-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape796-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-int64-shape15-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape797-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-int64-shape16-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape798-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-int64-shape17-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape799-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-int64-shape18-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape800-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-int64-shape19-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape801-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape802-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-uint8-shape20-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape803-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-uint8-shape21-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape804-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-uint8-shape22-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape805-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-uint8-shape23-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape806-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-uint8-shape24-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape807-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-uint16-shape25-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape808-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape809-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-max_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-uint16-shape26-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape810-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-int32-False-1024]" time="0.183" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-max_pos-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-uint16-shape27-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape811-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape812-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-uint16-shape28-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-max_pos-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape813-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-uint16-shape29-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape814-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-uint32-shape30-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape815-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-uint32-shape31-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape816-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-uint32-shape32-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape817-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape818-0-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-uint32-shape33-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-uint32-shape34-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape819-0-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-uint64-shape35-1-False]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape820-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-uint64-shape36-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape821-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape822-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_math_erf_op[float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-uint64-shape37-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape823-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-uint64-shape38-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape824-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape825-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-uint64-shape39-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float16-shape40-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape826-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape827-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float16-shape41-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape828-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float16-shape42-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape829-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float16-shape43-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape830-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape831-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float16-shape44-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-int64-False-1024]" time="0.171" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape45-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape832-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape833-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape46-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape834-0-False-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape47-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape835-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape48-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape836-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape837-0-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape49-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-max_pos-acq_rel]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float64-shape50-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape838-0-False-16]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-max_pos-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-max_pos-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-max_pos-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-bfloat16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape839-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-bfloat16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-bfloat16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape840-1-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-max-with-indices-bfloat16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int8-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape841-1-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape842-1-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape843-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_math_fma_op[float32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_math_fma_op[float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-uint8-False-1024]" time="0.161" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int64-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-int64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint8-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float64-shape51-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint32-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-max_pos-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-max_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape844-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-max_pos-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-max_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-max_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-max_pos-relaxed]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-max_pos-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-uint64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-uint16-False-1024]" time="0.170" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_math_divide_op[1-tl.math.fdiv(x, y)]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_math_divide_op[1-tl.math.div_rn(x, y)]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_precise_math[1-tl.math.sqrt_rn(x)-tl.math.sqrt(x.to(tl.float64)).to(tl.float32)]" time="0.029"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">expr_prec = 'tl.math.sqrt_rn(x)'
expr_ref = 'tl.math.sqrt(x.to(tl.float64)).to(tl.float32)', num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("expr_prec, expr_ref",
                             [('tl.math.sqrt_rn(x)', 'tl.math.sqrt(x.to(tl.float64)).to(tl.float32)'),
                              ('tl.math.div_rn(x,y)', '(x.to(tl.float64) / y.to(tl.float64)).to(tl.float32)')])
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_precise_math(expr_prec, expr_ref, num_ctas, device):
    
        @triton.jit
        def kernel(X, Y, OUT, OUT_REF, BLOCK: tl.constexpr):
            x = tl.load(X + tl.arange(0, BLOCK))
            y = tl.load(Y + tl.arange(0, BLOCK))
            prec = PREC_CALC
            ref = REF_CALC
            tl.store(OUT + tl.arange(0, BLOCK), prec)
            tl.store(OUT_REF + tl.arange(0, BLOCK), ref)
    
        shape = (128, )
        out = torch.zeros(shape, dtype=torch.float32, device=device)
        out_ref = torch.zeros(shape, dtype=torch.float32, device=device)
    
        x = torch.randn(shape, dtype=torch.float32, device=device)
        y = torch.randn(shape, dtype=torch.float32, device=device)
    
        if (expr_prec.count('sqrt') &gt; 0):
            x = torch.abs(x)
    
        if (expr_prec.count('div') &gt; 0):
            y += 1e-6
    
        kernel = patch_kernel(kernel, {'PREC_CALC': expr_prec, 'REF_CALC': expr_ref})
    
&gt;       kernel[(1, )](x, y, out, out_ref, BLOCK=shape[0], num_ctas=num_ctas)

language/test_core.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02f09bc2c0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape845-1-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape846-1-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape847-1-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float64-shape52-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape848-1-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape849-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-uint32-False-1024]" time="0.167" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-max_pos-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-max_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-max_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-max_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-max_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-max_pos-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-bfloat16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_precise_math[1-tl.math.div_rn(x,y)-(x.to(tl.float64) / y.to(tl.float64)).to(tl.float32)]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-bfloat16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_abs[int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-bfloat16-512]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_abs[int16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_abs[int32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int8-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_abs[int64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape850-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_abs[uint8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_abs[uint16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_abs[uint32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_abs[uint64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_abs[float16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_abs[float32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-uint64-False-1024]" time="0.175" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float64-shape53-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_abs[float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int32-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int32-512]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int64-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int64-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-int64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint8-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint8-512]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-max_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-max_pos-relaxed]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape851-1-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-max_pos-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-max_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape852-1-True-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint16-512]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-max_pos-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-max_pos-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape853-1-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape854-1-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint32-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape855-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-float16-False-1024]" time="0.170" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-uint64-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float64-shape54-1-False]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float16-512]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_abs[bfloat16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_shapes_as_params" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_transpose[int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_transpose[int16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_transpose[int32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_transpose[int64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_transpose[uint8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape856-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_transpose[uint16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_transpose[uint32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_transpose[uint64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw_predicate[1]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_transpose[float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-float32-False-1024]" time="0.176" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_transpose[float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_transpose[float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape0-0-1-float16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape1-0-1-float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape2-0-1-uint64]" time="0.005" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float16-float8e5-rtne-31488]" time="3.261" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape3-0-1-int64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape4-0-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-bfloat16-shape55-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-bfloat16-shape56-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-bfloat16-shape57-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-bfloat16-shape58-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-bfloat16-shape59-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape857-1-True-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape60-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape61-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape858-1-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape62-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape859-1-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape63-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape64-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape860-1-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape65-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape66-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape861-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape67-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape68-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape69-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape70-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-bfloat16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape71-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_transpose[bfloat16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-bfloat16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-bfloat16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape72-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[None, :]-int32]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmin-tie-break-left-bfloat16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape73-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int8-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape74-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape5-1-1-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[None, :]-uint32]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape6-1-1-float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape75-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape7-1-1-uint64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape76-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape8-1-1-int64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape77-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape9-1-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[None, :]-uint16]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape78-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape79-1-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape80-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[:, None]-int32]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape81-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int32-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape82-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape862-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[:, None]-uint32]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int64-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmin-float32-shape83-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape84-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-int64-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[:, None]-uint16]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape85-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint8-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape86-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint8-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape87-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[None, :, :]-int32]" time="0.029" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape88-0-False]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-int8-False-1024]" time="0.158" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-argmax-float32-shape89-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[None, :, :]-uint32]" time="0.030" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape90-None-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape91-None-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape92-None-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint32-512]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[None, :, :]-uint16]" time="0.029" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape93-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint64-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape94-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint64-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape95-2-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-uint64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[:, :, None]-int32]" time="0.029" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape96-0-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape97-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float16-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape98-2-False]" time="0.137" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape10-0-1-float16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float16-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[:, :, None]-uint32]" time="0.029" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape11-0-1-float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape12-0-1-uint64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape13-0-1-int64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-argmax-tie-break-left-float32-512]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape14-0-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_index1d[1-x[:, :, None]-uint16]" time="0.028" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape889-1-False-16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape890-1-False-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-int16-False-1024]" time="0.206" /><testcase classname="test.unit.language.test_core" name="test_tuples" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape891-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_noinline[simple]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_noinline[call_graph]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_noinline[dynamic]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_noinline[multi_values]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape863-1-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape99-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape100-1-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape864-1-True-16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_neg-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-min-float32-shape101-2-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape102-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape865-1-True-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_neg-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_neg-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape103-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape866-1-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape104-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape867-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape105-0-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape106-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape107-2-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape108-0-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape892-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape109-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape15-1-1-float16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-max-float32-shape110-2-False]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape16-1-1-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape17-1-1-uint64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape111-0-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape18-1-1-int64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape112-1-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-int32-False-1024]" time="0.214" /><testcase classname="test.unit.language.test_core" name="test_tensor_atomic_rmw[shape19-1-1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce[64-16-1-sum-float32-shape113-2-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-bfloat16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout6-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout6-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-bfloat16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout6-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-min-with-indices-bfloat16-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int8-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout6-64-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout7-32-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int8-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout7-32-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout7-64-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape868-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout8-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout8-32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout8-64-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape893-1-False-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout9-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int64-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_neg-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int64-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout9-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape894-1-False-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-int64-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout9-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_neg-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape895-1-False-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint8-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout9-64-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint8-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout10-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint8-512]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape896-1-False-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout10-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint16-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout10-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape897-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout10-64-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint32-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce[4-32-1-xor_sum-bool-shape177-1-False]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout0-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-int64-False-1024]" time="0.225" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint32-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape0-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout0-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape1-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape2-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape869-1-True-16]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout0-64-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout1-32-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape3-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-uint64-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape4-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float16-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape870-1-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape5-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float16-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout1-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float16-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape871-1-True-16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape6-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout1-64-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float16-512]" time="0.132" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape7-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout2-32-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape8-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape872-1-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout2-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape9-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape873-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape10-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout2-32-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape11-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout2-64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape12-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout3-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape13-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape14-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout3-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape898-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape15-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout3-64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape16-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout4-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape17-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_neg-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape18-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout4-32-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape19-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout4-64-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_neg-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape20-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout5-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape21-1-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_neg-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_neg-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape22-1-True-4]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout5-32-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float32-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape23-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout5-64-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float32-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape24-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout6-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape25-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-uint8-False-1024]" time="0.153" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float64-32]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout6-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape26-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout6-32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape27-1-True-4]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape874-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout6-64-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape28-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout7-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape29-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout7-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape30-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout7-32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape899-1-False-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape31-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout7-64-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape32-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape900-1-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout8-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape33-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape901-1-False-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape34-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout8-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape35-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape902-1-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout8-64-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape36-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape903-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout9-32-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape37-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout9-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape38-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout9-32-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape39-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout9-64-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape40-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape41-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout10-32-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape42-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout10-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-uint16-False-1024]" time="0.217" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape43-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout10-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape44-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape875-1-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-1-src_layout10-64-32]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape45-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape876-1-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape46-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout0-32-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_neg-acquire]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape47-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape877-1-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape48-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout0-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape49-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape878-1-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape50-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout0-32-64]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float64-64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_neg-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape51-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape879-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout0-64-32]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape52-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape53-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout1-32-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape54-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout1-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape55-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape904-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape56-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout1-32-64]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape57-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape58-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout1-64-32]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape59-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape60-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout2-32-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape61-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape62-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout2-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape63-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout2-32-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape64-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout2-64-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape65-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout3-32-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape66-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape67-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout3-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape68-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-uint32-False-1024]" time="0.204" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape69-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout3-32-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape880-1-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape70-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout3-64-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape71-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape72-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout4-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape73-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape905-1-False-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape74-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout4-32-64]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape75-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape906-1-False-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape76-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float64-128]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout4-64-32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_neg-acquire]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape77-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout5-32-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape907-1-False-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape78-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape79-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout5-32-32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape80-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape908-1-False-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_neg-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape81-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout5-32-64]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape909-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape82-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape83-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout5-64-32]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape84-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape85-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout6-32-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape86-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout6-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape87-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape88-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout6-32-64]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape881-1-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape89-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape90-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape882-1-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout6-64-32]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape91-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape883-1-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape92-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout7-32-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape93-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape884-1-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-uint64-False-1024]" time="0.228" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout7-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape94-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape885-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape95-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout7-32-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape96-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout7-64-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape97-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout8-32-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape98-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape99-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout8-32-32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape100-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape910-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape101-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout8-32-64]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape102-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-float64-512]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape103-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout8-64-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape104-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape105-0-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout9-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout9-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape106-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_neg-acquire]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape107-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout9-32-64]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_neg-acquire]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape108-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_neg-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout9-64-32]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape109-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_neg-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_neg-acquire]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape110-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout10-32-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_neg-acquire]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape111-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape112-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout10-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape886-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape113-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout10-32-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape114-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape115-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-0-src_layout10-64-32]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape116-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape117-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout0-32-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape118-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout0-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape911-1-False-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape119-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape120-0-True-4]" time="0.064" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout0-32-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-float16-False-1024]" time="0.199" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape912-1-False-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout0-64-32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape913-1-False-16]" time="0.145" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout1-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout1-32-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout1-64-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-bfloat16-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape121-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-bfloat16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout2-32-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape122-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-bfloat16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape123-0-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-max-with-indices-bfloat16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout2-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int8-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape124-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape887-1-False-16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout2-32-64]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int8-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape125-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int8-512]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape126-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape888-1-False-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout2-64-32]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int16-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape127-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int16-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape128-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int16-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout3-32-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape129-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_neg-release]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int16-512]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape130-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout4-128-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout3-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[4-32-1-argmin-tie-break-left-int32-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape131-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout7-16-16]" time="0.320" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout3-32-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape132-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout4-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape133-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout3-64-32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape914-1-False-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_neg-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape134-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout4-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape135-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout4-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape915-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout4-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape136-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout4-32-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape137-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout5-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-float32-False-1024]" time="0.213" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape138-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout4-64-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape139-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout5-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape140-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout5-32-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape141-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout5-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape142-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout6-128-16]" time="0.135" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout5-32-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape143-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape144-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout5-64-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape145-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout6-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape146-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape147-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout6-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape148-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout6-32-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape149-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout6-64-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape150-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape151-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout7-32-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape152-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout7-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape153-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout7-32-64]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape916-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape154-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape155-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout7-64-32]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape156-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape157-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout8-32-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape158-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout6-128-128]" time="0.776" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape159-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout8-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_neg-release]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape160-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout8-32-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_neg-release]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape161-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_neg-release]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape162-0-False-4]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout8-64-32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_neg-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout9-32-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout9-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape163-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout9-32-64]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape164-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout8-128-16]" time="0.402" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape165-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout9-64-32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape166-0-False-4]" time="0.048" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout10-32-16]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape917-1-False-16]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout10-32-32]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape918-1-False-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape167-0-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape168-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout10-32-64]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape919-1-False-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape169-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape170-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape920-1-False-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape171-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[True-1-src_layout10-64-32]" time="0.001"><error message="failed on setup with &quot;worker 'gw4' crashed while running 'test/unit/language/test_core.py::test_scan_layouts[True-1-src_layout10-64-32]'&quot;">worker 'gw4' crashed while running 'test/unit/language/test_core.py::test_scan_layouts[True-1-src_layout10-64-32]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape172-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape921-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape173-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape174-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape175-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape176-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape177-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape178-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape179-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-int8-False-1024]" time="0.172" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape180-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape181-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape182-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_neg-release]" time="0.133" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape183-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape184-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape185-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape186-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape187-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape188-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape189-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape190-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape922-1-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape191-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape192-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape193-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape194-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape195-1-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape196-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_neg-release]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape197-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_neg-release]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape198-1-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-int16-False-1024]" time="0.163" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape199-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape200-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape201-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape202-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape203-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape204-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape205-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape206-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout8-128-128]" time="2.579" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape207-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape923-1-False-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape208-1-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape209-1-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape924-0-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape210-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape211-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape925-0-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape212-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape213-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape926-0-True-16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape214-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape215-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape927-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape216-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape217-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape218-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-int32-False-1024]" time="0.263" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape219-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape220-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape221-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape222-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape223-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape224-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_neg-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape225-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape226-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape227-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_neg-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape228-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape229-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape230-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape231-1-False-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout2-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape928-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout2-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout3-128-128]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout3-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout6-64-64]" time="0.376" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout4-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout4-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout4-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-int64-False-1024]" time="0.398" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape929-0-True-16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout4-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_neg-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout4-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape930-0-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_neg-acq_rel]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_neg-acq_rel]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape931-0-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout5-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_neg-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape932-0-True-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout5-128-128]" time="0.016" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float16-float8e4nv-rtne-24320]" time="3.026" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape933-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout5-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout5-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout5-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout6-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout6-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout6-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout6-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout7-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout7-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape934-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout7-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout7-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_neg-acq_rel]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_neg-acq_rel]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_neg-acq_rel]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout8-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_neg-acq_rel]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_neg-acq_rel]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout8-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout6-32-128]" time="0.576" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout0-128-16]" time="0.081" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape935-0-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape936-0-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape937-0-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape938-0-True-16]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-uint8-False-1024]" time="0.160" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout0-128-128]" time="1.128" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape939-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_neg-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_neg-relaxed]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_neg-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_neg-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_neg-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape940-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-uint16-False-1024]" time="0.165" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape941-0-True-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape942-0-True-16]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-uint32-False-1024]" time="0.209" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape943-0-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape944-0-True-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape945-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_neg-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout6-32-32]" time="0.135" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape946-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-uint64-False-1024]" time="0.221" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_neg-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_neg-relaxed]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_neg-relaxed]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout6-16-16]" time="0.063" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_neg-relaxed]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_neg-relaxed]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape947-0-True-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape948-0-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape949-0-True-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape950-0-True-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape951-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout8-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-float16-False-1024]" time="0.215" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout8-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout8-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout0-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout0-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_pos-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout0-16-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_pos-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_pos-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int64-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout1-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape952-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float64-all_pos-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout1-64-64]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout1-32-128]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout1-32-32]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout1-16-16]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout2-128-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-float32-False-1024]" time="0.227" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout2-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout2-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout2-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout3-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout3-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape953-0-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout4-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout4-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape954-0-True-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape955-0-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout0-64-64]" time="0.465" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape956-0-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape957-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout5-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint32-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int32-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float32-all_pos-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-uint64-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout5-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-int64-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout5-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[max-float64-all_pos-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout5-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout6-128-16]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout6-128-128]" time="0.698" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape958-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint32-all_pos-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int32-all_pos-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float32-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-uint64-all_pos-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-int8-False-1024]" time="0.166" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-int64-all_pos-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[min-float64-all_pos-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape959-0-True-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape960-0-True-16]" time="0.056" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape961-0-True-16]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape962-0-True-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape963-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-int16-False-1024]" time="0.164" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout0-32-128]" time="0.827" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float16-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint32-all_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-int32-all_pos-acquire]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-float32-all_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_atomic_rmw[add-uint64-all_pos-acquire]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape458-0-True-4]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape459-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape964-0-True-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-int32-False-1024]" time="0.160" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout8-64-64]" time="1.136" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape460-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape965-0-True-16]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape966-0-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape967-0-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape968-0-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape969-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-int64-False-1024]" time="0.407" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape461-0-True-4]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout6-64-64]" time="0.331" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape462-0-False-4]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape463-0-False-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape464-0-False-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape465-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape970-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape466-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape971-0-False-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape972-0-False-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape973-0-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape974-0-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape975-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout6-32-128]" time="0.474" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape467-0-False-4]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-uint8-False-1024]" time="0.159" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape468-0-False-4]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape469-0-False-4]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape470-0-False-4]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape471-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape976-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout0-32-32]" time="0.079" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-uint16-False-1024]" time="0.163" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout0-16-16]" time="0.069" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape472-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape977-0-False-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape978-0-False-16]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape979-0-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout1-128-16]" time="0.071" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape980-0-False-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape981-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-uint32-False-1024]" time="0.163" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout1-128-128]" time="1.504" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape473-0-False-4]" time="0.012" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[bfloat16-float8e5b16-rtne-18272]" time="0.001"><skipped type="pytest.xfail" message="float8e5b16 downcast with RTNE rounding tests only supported on AMDGPU MI300" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape474-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[bfloat16-float8e4b8-rtne-17264]" time="0.001"><skipped type="pytest.xfail" message="float8e4b8 downcast with RTNE rounding tests only supported on AMDGPU MI300" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape475-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float16-float8e5b16-rtne-31488]" time="0.001"><skipped type="pytest.xfail" message="float8e5b16 downcast with RTNE rounding tests only supported on AMDGPU MI300" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape476-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_conversions" name="test_typeconvert_downcast[float16-float8e4b8-rtne-23424]" time="0.001"><skipped type="pytest.xfail" message="float8e4b8 downcast with RTNE rounding tests only supported on AMDGPU MI300" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape477-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_empty_kernel[int8]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[int16]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[int32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[int64]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[uint8]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[uint16]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[uint32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape982-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_empty_kernel[uint64]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[float16]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[float32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_empty_kernel[float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout6-32-32]" time="0.171" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-uint64-False-1024]" time="0.270" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape478-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape983-0-False-16]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout8-32-128]" time="2.301" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape984-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout6-16-16]" time="0.059" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape985-0-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape986-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape987-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_empty_kernel[bfloat16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dtype_codegen" time="0.000" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int8-+]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout7-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape479-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int16-+]" time="0.077" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape480-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-float16-False-1024]" time="0.167" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape481-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout8-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape482-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape483-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout0-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape988-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int32-+]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout0-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout0-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout0-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout1-128-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-int64-+]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout1-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-float32-False-1024]" time="0.181" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape484-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout2-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout2-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout2-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint8-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout2-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape989-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint16-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape990-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape991-0-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint32-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape992-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-uint64-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout3-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape993-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout3-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout3-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float32-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout4-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout4-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape485-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape486-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout4-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape487-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout4-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape488-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape489-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout4-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout5-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout5-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape994-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout5-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout5-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout4-32-128]" time="1.544" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout6-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout6-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout7-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape490-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout7-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape995-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-int8-False-1024]" time="0.172" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape996-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape997-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int8-bfloat16-+]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape998-0-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape999-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int8-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout8-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout8-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-0-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int32-+]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout0-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape491-0-False-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout0-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape492-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout0-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape493-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape494-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-int64-+]" time="0.069" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape495-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout1-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-int16-False-1024]" time="0.173" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout1-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape1000-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint8-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout2-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout2-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint16-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout2-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint32-+]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-uint64-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout3-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float32-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape496-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout3-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-int32-False-1024]" time="0.172" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape1001-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape1002-0-False-16]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout4-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout4-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout4-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape1003-0-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape1004-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape1005-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout5-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout5-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape497-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout5-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape498-0-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape499-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape500-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout6-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape501-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout1-64-64]" time="0.549" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout6-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-int64-False-1024]" time="0.173" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout7-128-128]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape1006-0-False-16]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int16-bfloat16-+]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int8-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout6-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int16-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape502-0-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int32-+]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-int64-+]" time="0.068" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-uint8-False-1024]" time="0.170" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape1007-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape1008--1-False-4]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint8-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_histogram[2048-2]" time="0.178" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint16-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout6-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint32-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape503-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape504-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-uint64-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape505-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape506-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape507-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float16-+]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape508-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape509-1-True-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float32-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape510-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape511-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-uint16-False-1024]" time="0.180" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape512-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-float64-+]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape513-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape514-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape515-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape516-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape517-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape518-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape519-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape520-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape521-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape522-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout6-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape523-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_histogram[1024-8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape524-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_histogram[1024-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape525-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_histogram[256-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_histogram[32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape526-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_histogram[8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape527-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_histogram[8-2]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape528-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-32-sum]" time="0.070" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape529-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape530-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape531-1-True-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape532-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape533-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape534-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape535-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-uint32-False-1024]" time="0.170" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape536-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape537-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-32-max]" time="0.055" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape538-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout1-32-128]" time="0.999" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape539-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape540-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape541-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape542-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape543-1-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout6-32-32]" time="0.076" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape544-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape545-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-32-min]" time="0.054" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape546-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape547-1-False-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape548-1-False-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape549-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape550-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int32-bfloat16-+]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape551-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape552-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape553-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-64-sum]" time="0.060" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int8-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape554-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout6-16-16]" time="0.066" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape555-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape556-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int16-+]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape557-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape558-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape559-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-uint64-False-1024]" time="0.185" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape560-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op[1-int64-int32-+]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape561-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-64-max]" time="0.062" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape562-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape563-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float16-&lt;-real-real]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout7-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape564-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape565-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape566-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape567-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape568-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape569-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape570-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-64-min]" time="0.061" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape571-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape572-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape573-1-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape574-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape575-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape576-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape577-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape578-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-128-sum]" time="0.070" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape579-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape580-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape581-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape582-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape583-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout7-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape584-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-float16-False-1024]" time="0.173" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape585-1-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape586-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape587-1-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout5-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-128-max]" time="0.071" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape588-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape589-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape590-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape591-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape592-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape593-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout6-128-16]" time="0.096" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape594-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape595-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape596-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape597-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape598-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-512-128-min]" time="0.069" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape599-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape600-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape601-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape602-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape603-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape604-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape605-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout7-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape606-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape607-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout6-128-128]" time="0.599" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-32-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape608-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-float32-False-1024]" time="0.189" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int8-&lt;-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-32-max]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape609-0-True-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-32-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape610-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-64-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape611-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-64-max]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape612-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-64-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout8-32-32]" time="0.580" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape613-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-128-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape614-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-128-max]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape615-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int32-&lt;-real-real]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-1024-128-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape616-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-32-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape617-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-32-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape618-0-True-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-32-min]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-64-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape619-0-True-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int64-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-64-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape620-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-64-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape621-0-True-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-128-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint8-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape622-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-128-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape623-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[2-2048-128-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape624-0-True-16]" time="0.041" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint16-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-32-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout7-32-128]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-32-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-32-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-64-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-64-max]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-64-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape625-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-128-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape626-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape627-0-True-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-128-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float16-&lt;-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-512-128-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape628-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-32-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape629-0-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-32-max]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape630-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-32-min]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape631-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float32-&lt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-64-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape632-0-False-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-64-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape633-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-64-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape634-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape635-0-False-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-128-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape636-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-128-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape637-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-1024-128-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape638-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-32-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape639-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-32-max]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape640-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-32-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape641-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-64-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-64-max]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape642-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape643-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-64-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape644-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-128-sum]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout8-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape645-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-128-max]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape646-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_optimize_thread_locality[4-2048-128-min]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape647-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout0-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape648-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape649-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout0-32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape650-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape651-0-False-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout0-64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout1-32-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape652-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape653-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout1-32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape654-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape655-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout1-64-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape656-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout2-32-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape657-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape658-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout2-32-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape659-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-int8-False-1024]" time="0.175" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout2-64-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape660-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape661-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout3-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape662-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape663-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout3-32-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape664-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout3-64-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape665-0-False-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-int32-shape666-0-False-16]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout4-32-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout8-128-128]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout4-32-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout1-32-32]" time="0.166" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-int32-shape667-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-int32-shape668-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout4-64-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-int32-shape669-0-False-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout5-32-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-int32-shape670-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int8-&lt;-real-real]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-int32-shape671-0-False-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout5-32-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape672-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan_layouts[False-0-src_layout5-64-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape673-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape674-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape675-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape676-1-True-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int32-&lt;-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout5-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout3-16-16]" time="0.073" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-int16-False-1024]" time="0.180" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout6-64-64]" time="0.242" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout5-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout8-16-16]" time="0.353" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout6-128-16]" time="0.133" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout8-64-64]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint8-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout1-16-16]" time="0.069" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout4-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout4-128-128]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout4-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout4-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout2-128-16]" time="0.215" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout5-128-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout5-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout6-128-128]" time="0.901" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout5-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-int32-False-1024]" time="0.217" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout8-32-128]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout6-128-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout6-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout6-32-128]" time="0.443" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout7-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout7-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout8-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout8-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout2-128-128]" time="0.945" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout8-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout0-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout0-128-16]" time="0.304" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-int64-False-1024]" time="0.237" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout0-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout0-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout0-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int8-&lt;-real-real]" time="0.161" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout1-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout1-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout0-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout2-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout2-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout2-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int16-&lt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout3-128-128]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int32-&lt;-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout3-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-uint8-False-1024]" time="0.170" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint8-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout4-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout4-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout0-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout0-128-128]" time="1.139" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout6-32-32]" time="0.083" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout4-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint32-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout4-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint64-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout5-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout5-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout5-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout6-16-16]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout6-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-uint16-False-1024]" time="0.174" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout6-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout0-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout6-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout7-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout7-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout7-128-128]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout7-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout7-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout8-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout7-32-32]" time="0.085" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout8-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout8-16-16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout0-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout0-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-uint32-False-1024]" time="0.218" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout7-16-16]" time="0.055" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout1-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout1-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout1-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout1-16-16]" time="0.062" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int8-&lt;-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout8-128-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout6-64-64]" time="0.414" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout8-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int16-&lt;-real-real]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout8-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout0-128-128]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout2-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout0-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout0-32-128]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint8-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout0-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout1-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout3-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout1-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout1-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-uint64-False-1024]" time="0.240" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout4-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout1-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout4-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout2-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout2-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout2-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout2-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout2-64-64]" time="0.454" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout2-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout6-128-16]" time="0.116" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout1-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout3-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout3-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout4-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout6-128-128]" time="0.693" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout4-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout4-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-float16-False-1024]" time="0.159" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout4-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout5-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout5-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout1-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout6-32-128]" time="0.588" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout5-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout5-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int8-&lt;-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout6-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout6-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout6-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout6-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int32-&lt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout6-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-float32-False-1024]" time="0.227" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout7-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint8-&lt;-real-real]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout7-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout2-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint32-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout8-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout8-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float32-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout2-32-128]" time="0.778" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout0-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout0-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout0-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout2-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout1-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout1-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout0-64-64]" time="0.715" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout1-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout2-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout2-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout2-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout2-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout3-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout3-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int8-&lt;-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout4-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout4-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-int8-False-1024]" time="0.172" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout4-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout6-32-32]" time="0.154" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout4-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int32-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout5-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int64-&lt;-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout5-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout2-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout6-64-64]" time="0.347" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout5-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint8-&lt;-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout5-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout6-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout6-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint32-&lt;-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout6-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint64-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float16-&lt;-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-int16-False-1024]" time="0.175" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&lt;-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout6-16-16]" time="0.068" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout7-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout3-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout8-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout8-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce2d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout0-128-128]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout0-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-int32-False-1024]" time="0.169" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout0-32-128]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout0-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout0-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout3-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout1-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout6-32-128]" time="0.474" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout1-32-128]" time="0.142" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int8-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout2-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout2-32-32]" time="0.081" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout0-32-128]" time="0.972" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-int64-False-1024]" time="0.230" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout3-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout3-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout2-16-16]" time="0.071" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout2-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout2-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout2-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout2-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout5-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout3-128-16]" time="0.239" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout2-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout6-128-16]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout3-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout3-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout3-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout3-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout3-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout6-128-128]" time="0.730" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int16-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout4-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-uint8-False-1024]" time="0.169" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout4-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout4-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout4-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout5-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout5-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout5-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout5-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout4-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout3-128-128]" time="1.312" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout6-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout6-32-32]" time="0.120" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout6-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout6-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout6-32-128]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-uint16-False-1024]" time="0.185" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout6-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout7-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout6-16-16]" time="0.057" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout4-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int32-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout8-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout8-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout8-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-0-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout0-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-uint32-False-1024]" time="0.177" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout0-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout0-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout0-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout4-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout1-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout1-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout2-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout2-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout2-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout2-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-uint64-False-1024]" time="0.244" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout2-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout2-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout2-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout3-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout4-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout3-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout3-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout3-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout3-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout4-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout4-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout4-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout4-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout5-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout5-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout5-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout6-64-64]" time="0.372" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout0-32-32]" time="0.335" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout5-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout6-128-16]" time="0.072" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout5-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout5-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout6-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout6-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-float16-False-1024]" time="0.170" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout6-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout6-128-128]" time="0.431" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint8-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout7-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout8-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout5-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout8-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout8-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-float32-False-1024]" time="0.173" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-expand_reduce2d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout0-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout5-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout0-16-16]" time="0.293" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout6-32-128]" time="0.487" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint16-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout3-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout5-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout6-64-64]" time="0.206" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout6-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout6-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout6-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-int8-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout1-128-16]" time="0.319" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint32-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout6-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout8-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout3-64-64]" time="0.532" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout6-32-128]" time="0.321" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-0-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout6-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-int16-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout6-32-32]" time="0.143" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout2-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout1-128-128]" time="1.519" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout6-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout4-128-128]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout6-16-16]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-int32-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout6-32-32]" time="0.079" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout7-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout7-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout7-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout6-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout6-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout6-16-16]" time="0.066" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout8-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-1-src_layout8-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout0-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout7-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout8-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float16-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout7-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout7-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce1d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout1-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout3-32-128]" time="0.859" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout1-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout0-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout6-32-32]" time="0.070" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-int64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout2-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout0-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout6-16-16]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout3-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout3-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout3-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout4-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout7-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout2-128-128]" time="0.127" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout3-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout5-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout5-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout8-16-16]" time="0.083" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout5-16-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-uint8-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout6-128-16]" time="0.130" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float32-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout7-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout0-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout3-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout6-128-128]" time="0.815" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout2-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout5-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout2-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-uint16-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout7-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout6-128-16]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float64-&lt;-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout4-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout6-128-128]" time="0.345" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout7-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-uint32-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout6-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout7-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout8-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int8-&gt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int32-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout1-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout3-32-32]" time="0.081" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint8-&gt;=-real-real]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-uint64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout6-64-64]" time="0.182" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout8-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout2-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint16-&gt;=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout2-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout3-16-16]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint32-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout4-128-16]" time="0.346" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float32-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout6-32-128]" time="0.245" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout8-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-float16-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout5-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout5-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout6-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout6-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout6-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout6-64-64]" time="0.385" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout1-64-64]" time="0.786" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout8-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-0-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-float32-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int8-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout6-32-32]" time="0.070" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int16-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int32-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout4-128-128]" time="1.001" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint8-&gt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout6-16-16]" time="0.064" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout0-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint16-&gt;=-real-real]" time="0.064" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint32-&gt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout3-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout3-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout3-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float64-float64-False-1024]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce2d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout0-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout4-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout7-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout6-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout8-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout2-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout2-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout2-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout0-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout0-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout0-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout3-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-bfloat16-False-1024]" time="0.185" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout3-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout1-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce1d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout2-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout1-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout2-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout0-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int32-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_cast[1-bfloat16-float32-False-1024]" time="0.235" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint8-&gt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout5-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint32-&gt;=-real-real]" time="0.066" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout1-32-128]" time="1.143" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout1-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint64-&gt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout5-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout6-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout6-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout6-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-int32-True-1024]" time="0.165" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout7-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout6-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout7-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout1-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout8-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout8-32-32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout4-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout0-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout5-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout1-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-bool-False-1024]" time="0.192" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout6-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout1-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout2-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout6-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout2-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout6-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout7-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int8-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout7-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int16-&gt;=-real-real]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout6-128-16]" time="0.145" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-expand_reduce2d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint8-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout5-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout1-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout0-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint16-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-bfloat16-False-1024]" time="0.210" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout5-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout0-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout0-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout0-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint64-&gt;=-real-real]" time="0.069" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout6-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout1-128-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout6-128-128]" time="1.135" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout6-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout1-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout5-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout1-32-128]" time="0.957" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout6-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float16-&gt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout2-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-reduce2d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout7-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint8-int8-True-1024]" time="0.173" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout1-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-reduce2d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout0-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout2-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout2-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout2-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout1-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint16-int16-True-1024]" time="0.166" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout4-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout4-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout4-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout2-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout5-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int32-&gt;=-real-real]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout4-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout6-128-16]" time="0.094" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint8-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint32-int32-True-1024]" time="0.179" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout5-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint16-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout5-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint32-&gt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout6-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout6-128-128]" time="0.565" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout6-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout1-32-32]" time="0.426" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout2-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float32-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-uint64-int64-True-1024]" time="0.175" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-0-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout0-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout3-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout0-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout0-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout1-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout1-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout1-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout1-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout1-16-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int8-uint8-True-1024]" time="0.172" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout2-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout3-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int32-&gt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout4-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout4-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int64-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout4-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout4-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint8-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout1-32-32]" time="0.152" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout1-16-16]" time="0.303" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int16-uint16-True-1024]" time="0.166" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout5-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout3-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float16-&gt;=-real-real]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout6-128-16]" time="0.097" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout6-64-64]" time="0.232" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout1-16-16]" time="0.063" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout6-64-64]" time="0.465" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout6-128-128]" time="0.562" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int32-uint32-True-1024]" time="0.182" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout3-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout2-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout2-128-128]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout2-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout2-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout3-128-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-True-reduce1d-0-src_layout3-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout2-128-16]" time="0.399" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout2-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout6-32-128]" time="0.363" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout2-256]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout0-64]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_cast[1-int64-uint64-True-1024]" time="0.179" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout4-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout0-128]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout2-64]" time="0.048" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint8-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout0-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout0-128]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout4-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e4m3fn-float16-False-1024]" time="0.348" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout1-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout0-64]" time="0.047" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout6-32-128]" time="0.820" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout0-128]" time="0.050" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout4-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout0-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout2-128-128]" time="1.416" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout6-32-32]" time="0.078" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout6-64-64]" time="0.211" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-0-dst_layout2-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout0-64]" time="0.047" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout0-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout7-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout7-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout1-64]" time="0.059" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout4-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int8-&gt;=-real-real]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout7-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int16-&gt;=-real-real]" time="0.165" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout2-64]" time="0.046" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float32-False-expand_reduce2d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e4m3fn-float16-False-32]" time="0.160" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout1-256-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout1-256-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout1-128-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout6-32-128]" time="0.315" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout2-256-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout2-256-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout0-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout2-128-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout3-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout0-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout3-256-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout3-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout5-128-16]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout3-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout0-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout0-256-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout0-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout1-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout0-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout0-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int32-&gt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout1-256-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout1-256-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e4m3fn-float32-False-1024]" time="0.279" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout1-128-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint8-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout2-256-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout2-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[0-1-dst_layout2-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint16-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout2-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout0-64]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout0-128]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout3-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint32-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout3-256-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout3-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout1-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-max-src_layout3-128-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float16-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout0-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout1-256]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout5-128-128]" time="0.016"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float32-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout0-256-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout0-256-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout0-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout0-128-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout1-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout0-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout1-256-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout1-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout1-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout1-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout2-256-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout2-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout6-32-32]" time="0.079" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout2-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout1-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout3-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout3-256-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout0-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout3-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-sum-src_layout3-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout0-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout0-256-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout5-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout0-256-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout0-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout6-16-16]" time="0.071" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-0-dst_layout2-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e4m3fn-float32-False-32]" time="0.157" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout1-256-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout0-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout1-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout1-128-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout2-256-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout2-256-256]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout0-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout2-128-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout0-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout3-256-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout7-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout3-256-256]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout6-32-32]" time="0.165" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[1-max-src_layout3-128-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_generic_reduction" time="0.249" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout5-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout1-src_layout2-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout0-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout8-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout0-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e4m3fn-bfloat16-False-1024]" time="0.274" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout0-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-float16-False-expand_reduce2d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout0-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout1-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int32-&gt;=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout0-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout1-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout0-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout2-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout0-256]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout2-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_store_op[src_layout1-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert1d[1-1-dst_layout2-src_layout2-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm10-shape1-int32]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout0-256-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm10-shape1-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout0-256-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm11-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout0-128-256]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm11-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm11-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_chain_reduce[0-sum-src_layout1-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm11-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout6-16-16]" time="0.070" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-cols-ieee-float32-float32-1-None]" time="0.371" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm12-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm12-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint32-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm12-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm12-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm13-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm13-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout6-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm13-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm13-shape1-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm14-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm14-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm14-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm14-shape1-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm15-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm15-shape0-int8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_permute[1-float8e4b15-shape0-perm0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm15-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm15-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_permute[1-float8e4b15-shape1-perm1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm16-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_permute[1-float16-shape2-perm2]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm16-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout8-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm16-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_permute[1-float16-shape3-perm3]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm16-shape1-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm17-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm17-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_permute[1-float32-shape4-perm4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm17-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_permute[1-float32-shape5-perm5]" time="0.001"><skipped type="pytest.xfail" message="XPU: Not enough shared memory for float32 with shape 128x128" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm17-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e4m3fn-bfloat16-False-32]" time="0.150" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm18-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm18-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout0-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm18-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm18-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm19-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout0-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm19-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout6-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout0-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm19-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm19-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm20-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm20-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout1-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm20-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm20-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm21-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout1-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm21-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm21-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm21-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout1-16-16]" time="0.064" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm22-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm22-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm22-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm22-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm23-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm23-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm0-shape0-int32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm23-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e5m2-float16-False-1024]" time="0.231" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm23-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm0-shape0-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-none-tf32-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm0-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm0-shape1-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-none-tf32x3-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm1-shape0-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm1-shape0-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-none-ieee-float16-float16-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout6-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm1-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_2d[perm1-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-none-ieee-float16-float32-1-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm0-shape0-int32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-softmax-tf32-float32-float32-1-None]" time="0.778" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm0-shape0-int8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm0-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-none-ieee-float32-float32-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm0-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-trans-tf32-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int16-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm1-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm1-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-trans-tf32x3-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm1-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm1-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-trans-ieee-float16-float16-1-None]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm2-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm2-shape0-int8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-trans-ieee-float16-float32-1-None]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int64-&gt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm2-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-trans-ieee-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm2-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm3-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint8-&gt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-matrix-tf32-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm3-shape0-int8]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm3-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-matrix-tf32x3-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm3-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm4-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm4-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-matrix-ieee-float16-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm4-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm4-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint32-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm5-shape0-int32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout2-64-64]" time="0.776" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout6-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm5-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-rows-tf32-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-uint64-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm5-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout5-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-rows-tf32x3-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm5-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm6-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-rows-ieee-float16-float16-1-None]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float16-&gt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm6-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e5m2-float16-False-32]" time="0.153" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-rows-ieee-float16-float32-1-None]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm6-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout6-128-16]" time="0.128" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float32-&gt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm6-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-rows-ieee-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm7-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm7-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-cols-tf32-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm7-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-cols-tf32x3-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm7-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm8-shape0-int32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-cols-ieee-float16-float16-1-None]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm8-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm8-shape1-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-cols-ieee-float16-float32-1-None]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm8-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm9-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-add-cols-ieee-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm9-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm9-shape1-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-softmax-tf32-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm9-shape1-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm10-shape0-int32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-softmax-tf32x3-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_trans_4d[perm10-shape0-int8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-softmax-ieee-float16-float16-1-None]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-float32-float32-1-None1]" time="0.656" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout6-32-32]" time="0.073" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-softmax-ieee-float16-float32-1-None]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout6-128-128]" time="0.702" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-softmax-ieee-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-chain-dot-tf32-float32-float32-1-None]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e5m2-float32-False-1024]" time="0.223" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-chain-dot-tf32x3-float32-float32-1-None]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-chain-dot-ieee-float16-float16-1-None]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout6-16-16]" time="0.063" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-chain-dot-ieee-float16-float32-1-None]" time="0.016"><failure message="AssertionError: &#10;Not equal to tolerance rtol=0.01, atol=0.001&#10;&#10;Mismatched elements: 3968 / 4096 (96.9%)&#10;Max absolute difference: 0.996&#10;Max relative difference: 511.&#10; x: array([[-1.459  ,  0.3    ,  1.124  , ..., -1.45   , -0.8325 ,  0.994  ],&#10;       [ 0.4158 , -0.1995 ,  0.731  , ..., -0.4128 , -0.452  ,  0.129  ],&#10;       [-0.3174 , -0.3103 , -0.3906 , ..., -0.756  ,  0.7    ,  0.1741 ],...&#10; y: array([[-1.2705 ,  0.2299 ,  1.266  , ..., -1.56   , -0.9473 ,  0.6484 ],&#10;       [ 0.4443 ,  0.05536,  0.539  , ..., -0.0459 , -0.3777 ,  0.6387 ],&#10;       [-0.3547 , -0.2103 , -0.6206 , ..., -0.739  ,  0.7373 ,  0.4412 ],...">M = 64, N = 64, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'chain-dot', input_precision = 'ieee', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
        pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)
    
        # torch result
        if in_dtype == 'int8':
            z_ref = np.matmul(x.astype(np.float32), y.astype(np.float32())).astype(np.int32)
        elif 'float8' in in_dtype:
            x = convert_fp8_to_fp32(x, device, in_dtype)
            y = convert_fp8_to_fp32(y, device, in_dtype)
            z_ref = to_numpy(torch.matmul(x, y))
        else:
            z_ref = np.matmul(x, y)
    
        if epilogue == 'add-matrix':
            z_ref += z
        if epilogue == 'add-rows':
            z_ref += z[:, 0][:, None]
        if epilogue == 'add-cols':
            z_ref += z[0, :][None, :]
        if epilogue == 'softmax':
            num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))
            denom = np.sum(num, axis=-1, keepdims=True)
            z_ref = num / denom
        if epilogue == 'chain-dot':
            if 'float8' in in_dtype:
                # Reduce z_ref's precision to fp8 to match the kernel behavior
                if in_dtype == 'float8e4nv':
                    z_fp8 = torch.tensor(z_ref, dtype=torch.float8_e4m3fn)
                elif in_dtype == 'float8e5':
                    z_fp8 = torch.tensor(z_ref, dtype=torch.float8_e5m2)
                else:
                    assert "Unsupported float8 dtype"
                z_ref = to_numpy(z_fp8.to(torch.float32))
                w = to_numpy(convert_fp8_to_fp32(w, device, in_dtype))
            z_ref = np.matmul(z_ref, w)
        # compare
        if in_dtype == 'float32':
            # XXX: Somehow there's a larger difference when we use float32
            np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)
        elif out_dtype == tl.float16 or in_dtype == 'bfloat16':
            np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-2)
        else:
            # added atol, to loose precision for float16xfloat16-&gt;float32 case
&gt;           np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)

language/test_core.py:3552: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (&lt;function assert_allclose.&lt;locals&gt;.compare at 0x7d02e8740900&gt;, array([[-1.459  ,  0.3    ,  1.124  , ..., -1.45   , -0.8325 ,  0.994  ],
       [ 0.4158 , -0.1995 ,  0.731  , ..., -0.4128 , -0.452  ,  0.129  ],
       [-0.3174 , -0.3103 , -0.3906 , ..., -0.756  ,  0.7    ,  0.1741 ],
       ...,
       [ 0.1665 , -0.3364 , -0.87   , ...,  0.4995 ,  0.3633 ,  0.3232 ],
       [ 0.0872 , -0.0829 , -0.4763 , ...,  0.616  , -0.2773 , -0.1969 ],
       [ 0.1018 , -0.04434, -1.222  , ..., -0.1381 ,  0.765  ,  0.553  ]],
      dtype=float16), array([[-1.2705 ,  0.2299 ,  1.266  , ..., -1.56   , -0.9473 ,  0.6484 ],
       [ 0.4443 ,  0.05536,  0.539  , ..., -0.0459 , -0.3777 ,  0.6387 ],
       [-0.3547 , -0.2103 , -0.6206 , ..., -0.739  ,  0.7373 ,  0.4412 ],
       ...,
       [-0.1556 , -0.1843 , -0.5654 , ...,  0.2708 ,  0.552  ,  0.3254 ],
       [-0.1459 ,  0.1164 , -0.2512 , ...,  0.9473 , -0.08276, -0.02692],
       [ 0.0691 , -0.0901 , -1.09   , ..., -0.0835 ,  1.066  ,  0.7656 ]],
      dtype=float16))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=0.01, atol=0.001', 'verbose': True}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
&gt;           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=0.01, atol=0.001
E           
E           Mismatched elements: 3968 / 4096 (96.9%)
E           Max absolute difference: 0.996
E           Max relative difference: 511.
E            x: array([[-1.459  ,  0.3    ,  1.124  , ..., -1.45   , -0.8325 ,  0.994  ],
E                  [ 0.4158 , -0.1995 ,  0.731  , ..., -0.4128 , -0.452  ,  0.129  ],
E                  [-0.3174 , -0.3103 , -0.3906 , ..., -0.756  ,  0.7    ,  0.1741 ],...
E            y: array([[-1.2705 ,  0.2299 ,  1.266  , ..., -1.56   , -0.9473 ,  0.6484 ],
E                  [ 0.4443 ,  0.05536,  0.539  , ..., -0.0459 , -0.3777 ,  0.6387 ],
E                  [-0.3547 , -0.2103 , -0.6206 , ..., -0.739  ,  0.7373 ,  0.4412 ],...

/usr/lib/python3.12/contextlib.py:81: AssertionError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout7-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int8-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-chain-dot-ieee-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e5m2-float32-False-32]" time="0.141" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-none-tf32-float32-float32-1-None]" time="0.393" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout7-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e5m2-bfloat16-False-1024]" time="0.245" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout7-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int16-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-softmax-tf32x3-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-softmax-ieee-float16-float16-1-None]" time="0.785" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout7-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout2-32-128]" time="1.028" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float8_e5m2-bfloat16-False-32]" time="0.148" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-int8-int8-1-None0]" time="18.357" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-none-tf32x3-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-none-ieee-float16-float16-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-none-ieee-float16-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-none-ieee-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-trans-tf32-float32-float32-1-None]" time="0.394" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int32-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout6-64-64]" time="0.331" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout7-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout8-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-float8_e4m3fn-False-1024]" time="0.334" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout8-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-int64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout8-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout6-32-128]" time="0.464" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-trans-tf32x3-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-trans-ieee-float16-float16-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-trans-ieee-float16-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-trans-ieee-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-float8_e4m3fn-False-32]" time="0.144" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-matrix-tf32-float32-float32-1-None]" time="0.346" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout8-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint8-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-softmax-ieee-float16-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-float8_e4m3fn-False-1024]" time="0.290" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-softmax-ieee-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-chain-dot-tf32-float32-float32-1-None]" time="0.821" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout0-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-matrix-tf32x3-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout0-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-matrix-ieee-float16-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint16-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout6-32-32]" time="0.116" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-rows-tf32-float32-float32-1-None]" time="0.352" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-float8_e4m3fn-False-32]" time="0.146" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout6-16-16]" time="0.056" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout0-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout2-32-32]" time="0.350" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-bfloat16-float8_e4m3fn-False-1024]" time="0.280" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout7-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout7-16-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout8-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout8-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout8-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint32-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce1d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout0-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout0-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout0-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout1-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout1-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-rows-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout1-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-rows-ieee-float16-float16-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-rows-ieee-float16-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-rows-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout2-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-cols-tf32-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout2-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-cols-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-cols-ieee-float16-float16-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout2-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-add-cols-ieee-float16-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-int8-int8-1-None0]" time="87.499"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417b920&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout1-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout3-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout3-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cast[1-bfloat16-float8_e4m3fn-False-32]" time="0.152" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout2-16-16]" time="0.286" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout4-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-reduce2d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-uint64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout6-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout6-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout6-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout6-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout7-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout1-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout7-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-chain-dot-tf32x3-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout7-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-chain-dot-ieee-float16-float16-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout7-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-chain-dot-ieee-float16-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-float8_e5m2-False-1024]" time="0.205" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-32-4-False-False-chain-dot-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout8-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-none-tf32-float32-float32-1-None]" time="0.061" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout8-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout8-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout8-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float32-False-expand_reduce2d-1-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout0-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-none-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-none-ieee-float16-float16-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-none-ieee-float16-float32-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-none-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-trans-tf32-float32-float32-1-None]" time="0.057" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout1-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout3-128-16]" time="0.371" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-trans-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-trans-ieee-float16-float16-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float16-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-trans-ieee-float16-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-trans-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-matrix-tf32-float32-float32-1-None]" time="0.059" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float16-float8_e5m2-False-32]" time="0.142" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout0-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout1-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-matrix-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-matrix-ieee-float16-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-rows-tf32-float32-float32-1-None]" time="0.060" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-float8_e5m2-False-1024]" time="0.216" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout0-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-rows-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-rows-ieee-float16-float16-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-rows-ieee-float16-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-rows-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-cols-tf32-float32-float32-1-None]" time="0.059" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout2-128-16]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float32-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-cols-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-cols-ieee-float16-float16-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-cols-ieee-float16-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-add-cols-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-softmax-tf32-float32-float32-1-None]" time="0.229" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout0-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout3-128-128]" time="1.890" /><testcase classname="test.unit.language.test_core" name="test_cast[1-float32-float8_e5m2-False-32]" time="0.144" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout2-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout1-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float64-float64-&gt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_cast[1-bfloat16-float8_e5m2-False-1024]" time="0.271" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-softmax-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout2-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-softmax-ieee-float16-float16-1-None]" time="0.237" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout1-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout2-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-softmax-ieee-float16-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout1-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-softmax-ieee-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int8-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-chain-dot-tf32-float32-float32-1-None]" time="0.135" /><testcase classname="test.unit.language.test_core" name="test_cast[1-bfloat16-float8_e5m2-False-32]" time="0.151" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout3-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-int64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint8-&lt;=-real-real]" time="0.066" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint16-&lt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-chain-dot-tf32x3-float32-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-chain-dot-ieee-float16-float16-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cat[int8-4]" time="0.150" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-chain-dot-ieee-float16-float32-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint32-&lt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_dot[1-16-16-16-4-False-False-chain-dot-ieee-float32-float32-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-int8-int8-1-None0]" time="44.767" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-uint64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout3-128-128]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float32-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int8-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout1-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cat[int8-8]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_cat[int16-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_cat[int16-8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cat[int32-4]" time="0.045" /><testcase classname="test.unit.language.test_core" name="test_cat[int32-8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cat[int64-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_cat[int64-8]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_cat[float16-4]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_cat[float16-8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_cat[float32-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cat[float32-8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_cat[float64-4]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout2-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout3-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int8-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-int64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint8-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout2-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint16-&lt;=-real-real]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout3-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_cat[float64-8]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint32-&lt;=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-uint64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float16-&lt;=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float32-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout2-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int16-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout4-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-bool]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-int8]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-int16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout2-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-int64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-uint8]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-float16]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-float32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout4-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int8-&lt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout3-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int32-&lt;=-real-real]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-int64-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout4-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint8-&lt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-value-bfloat16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-bool]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-int8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-int16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-int32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint32-&lt;=-real-real]" time="0.080" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-int64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-uint8]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-float16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-float32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout3-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-uint64-&lt;=-real-real]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout4-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int32-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout3-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout3-64-64]" time="0.900" /><testcase classname="test.unit.language.test_core" name="test_store_constant[1-mask-bfloat16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_load_store_same_ptr" time="0.663" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout5-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout3-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout5-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int8-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int16-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-int64-&lt;=-real-real]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout3-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint8-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint16-&lt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout4-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-uint64-&lt;=-real-real]" time="0.066" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout5-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float16-&lt;=-real-real]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout4-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-int64-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout5-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout4-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout6-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_umulhi[int32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_join" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_join_scalars" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int8-&lt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_join_with_mma" time="0.071" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout4-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int32-&lt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_interleave[False]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-int64-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout6-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint8-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_interleave[True]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_interleave_scalars" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_split" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint16-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_split_to_scalar" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_convert_float16_to_float32[in_dtype0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert_float16_to_float32[in_dtype1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-uint64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_max_returns_zero" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int8-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int8-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float16-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int8-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int8-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int16-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float32-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout4-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int16-64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int16-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout5-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int16-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout3-32-128]" time="1.213" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint8-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int32-32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int32-64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int32-128]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int32-512]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int64-32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int64-64]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout6-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce1d[64-16-1-min-int64-128]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape232-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape233-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape234-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape235-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape236-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape237-1-False-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape238-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape239-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape240-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape241-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape242-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape243-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape244-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape245-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout5-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape246-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape247-1-False-4]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape248-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape249-1-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape250-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout6-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape251-1-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape252-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape253-0-True-4]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape254-0-True-4]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape255-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape256-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape257-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape258-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape259-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape260-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape261-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape262-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape263-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int8-&lt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape264-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape265-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape266-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout5-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape267-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape268-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int16-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape269-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape270-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape271-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int32-&lt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout6-32-32]" time="0.069" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape272-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape273-0-True-4]" time="0.140" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-int64-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint8-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout6-16-16]" time="0.062" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-uint64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout5-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout7-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape274-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint16-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape275-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape276-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape277-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape278-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape279-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape280-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape281-0-True-4]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape282-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape283-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape284-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape285-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape286-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape287-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape288-0-True-4]" time="0.050" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout6-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout7-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape289-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape290-0-True-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape291-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape292-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape293-0-True-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape294-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape295-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape296-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape297-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape298-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape299-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape300-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape301-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape302-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape303-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout6-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int8-&lt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape304-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout7-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape305-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape306-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape307-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape308-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int16-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape309-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape310-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int32-&lt;=-real-real]" time="0.033" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape311-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape312-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape313-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape314-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape315-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-int64-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape316-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape317-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint8-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape318-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape319-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint16-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape320-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape321-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape322-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout6-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape323-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout7-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape324-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-uint64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape325-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape326-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape327-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape328-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape329-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-float32-shape330-0-False-4]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint32-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-float32-shape331-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-float32-shape332-0-False-4]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-float32-shape333-0-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-float32-shape334-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-float32-shape335-0-False-4]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape336-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape337-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape338-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape339-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout6-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout8-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape340-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout6-32-32]" time="0.081" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout3-32-32]" time="0.367" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout8-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int8-&lt;=-real-real]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int16-&lt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout6-16-16]" time="0.067" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-int64-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint8-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape341-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout7-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape342-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint16-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape343-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape344-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout8-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape345-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-uint64-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float16-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-uint64-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout7-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout8-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape346-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout3-16-16]" time="0.239" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout7-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape347-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-expand_reduce2d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape348-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape349-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape350-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout0-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int8-&lt;=-real-real]" time="0.034" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape351-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout0-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout1-128-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int16-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout1-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout1-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int32-&lt;=-real-real]" time="0.151" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout1-32-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout1-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout7-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout4-128-16]" time="0.619" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout2-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout3-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape352-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout3-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout3-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout3-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout4-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-int64-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[max-int32-False-reduce1d-0-src_layout4-64-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-int8-int8-1-None1]" time="0.043" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint8-&lt;=-real-real]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout8-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint16-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-float16-float16-1-None0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint32-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-float16-float16-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-float16-float32-1-None0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-uint64-&lt;=-real-real]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-float16-float32-1-None1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape353-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float16-&lt;=-real-real]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape354-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-float32-float32-1-None0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape355-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float32-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape356-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-float32-float32-1-None1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float16-float64-&lt;=-real-real]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape357-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-int8-int8-1-None0]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-int8-int8-1-None1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-float16-float16-1-None0]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout8-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-float16-float16-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-float16-float32-1-None0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-float16-float32-1-None1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-float32-float32-1-None0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-True-none-tf32-float32-float32-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-int8-int8-1-None0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape358-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-int8-int8-1-None1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-float16-float16-1-None0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-float16-float16-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout8-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-float16-float32-1-None0]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-float16-float32-1-None1]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-float32-float32-1-None0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-False-False-none-tf32-float32-float32-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-int8-int8-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_compare_op[1-float32-int8-&lt;=-real-real]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-32-64-False-False-True-e4m3-bf16-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw0' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-64-False-False-True-e4m3-bf16-4-16-1]'&quot;">worker 'gw0' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-64-False-False-True-e4m3-bf16-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-float16-float16-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape359-1-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-float16-float16-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape360-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-float16-float32-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape361-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-float16-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape362-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-float32-float32-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape363-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-True-none-tf32-float32-float32-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout8-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-int8-int8-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout4-128-128]" time="1.249" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-float16-float16-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-float16-float16-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-float16-float32-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-float16-float32-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-float32-float32-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-True-False-none-tf32-float32-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-int8-int8-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-int8-int8-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-float16-float16-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-float16-float16-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape364-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout8-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-float16-float32-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-0-src_layout8-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout0-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-float16-float32-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-float32-float32-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-True-none-tf32-float32-float32-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-int8-int8-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-float16-float16-1-None0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-float16-float16-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-float16-float32-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-float16-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-float32-float32-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-32-4-False-False-none-tf32-float32-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape365-1-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-int8-int8-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape366-1-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-int8-int8-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout0-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape367-1-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-float16-float16-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape368-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-float16-float16-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape369-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-float16-float32-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-float16-float32-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-float32-float32-1-None0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-True-none-tf32-float32-float32-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-int8-int8-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-int8-int8-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-float16-float16-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-float16-float16-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-float16-float32-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-float16-float32-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-float32-float32-1-None0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout0-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-True-False-none-tf32-float32-float32-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-int8-int8-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape370-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-float16-float16-1-None0]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-float16-float16-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-float16-float32-1-None0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-float16-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-float32-float32-1-None0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-True-none-tf32-float32-float32-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-int8-int8-1-None0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-int8-int8-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-float16-float16-1-None0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-float16-float16-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-float16-float32-1-None0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout0-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-float16-float32-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-float32-float32-1-None0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-32-128-16-False-False-none-tf32-float32-float32-1-None1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape371-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-int8-int8-1-None0]" time="146.663"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62fde390&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape372-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape373-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape374-1-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape375-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout1-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape376-1-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout1-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape377-1-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape378-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape379-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape380-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape381-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout1-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape382-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout1-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout4-64-64]" time="0.546" /><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape383-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape384-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape385-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape386-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape387-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout1-16-16]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout2-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape388-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout2-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape389-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape390-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape391-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape392-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape393-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout2-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape394-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout4-32-128]" time="1.078" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout2-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape395-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape396-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape397-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape398-1-False-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout3-128-16]" time="0.003"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape399-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout3-128-128]" time="0.003"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape400-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout3-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape401-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape402-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape403-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape404-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape405-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout3-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape406-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout4-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape407-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape408-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape409-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape410-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape411-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout4-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape412-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout4-32-32]" time="0.349" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout4-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape413-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape414-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape415-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape416-1-False-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape417-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout4-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape418-1-False-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout4-16-16]" time="0.313" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout5-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape419-1-False-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape420-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape421-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape422-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape423-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout5-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout5-128-16]" time="0.321" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape424-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout5-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape425-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape426-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout5-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape427-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape428-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape429-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout5-128-128]" time="1.406" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout6-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape430-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout6-128-128]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape431-0-True-4]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape432-0-True-4]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape433-0-True-4]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape434-0-True-4]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape435-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout6-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape436-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout6-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape437-0-True-4]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape438-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape439-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape440-0-True-4]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape441-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout6-32-32]" time="0.061" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout6-16-16]" time="0.057" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape442-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout7-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-64-False-True-True-e4m3-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw9' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-True-True-e4m3-e4m3-4-16-1]'&quot;">worker 'gw9' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-True-True-e4m3-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout7-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape443-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape444-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape445-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape446-0-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape447-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout7-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape448-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout7-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape449-0-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape450-0-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape451-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[get_first_element-bfloat16-shape452-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scan2d[linear_recurrence-bfloat16-shape453-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="Skipping linear_recurrence scan on bfloat16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout8-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout5-64-64]" time="0.725" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cummax-bfloat16-shape454-0-True-4]" time="0.001"><skipped type="pytest.xfail" message="bfloat16 compare not supported before sm90" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout8-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[roll-bfloat16-shape455-0-True-4]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_scan2d[cumsum-bfloat16-shape456-0-True-4]" time="0.075" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout8-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scan2d[cumprod-bfloat16-shape457-0-True-4]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-64-64-False-False-False-e5m2-e5m2-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw5' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-64-False-False-False-e5m2-e5m2-4-16-1]'&quot;">worker 'gw5' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-64-False-False-False-e5m2-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout8-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce1d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout0-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout5-32-128]" time="0.936" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout0-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout0-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout0-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout1-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout1-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout1-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout1-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout5-32-32]" time="0.317" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout1-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout2-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout2-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout5-16-16]" time="0.273" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout2-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout2-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout6-128-16]" time="0.437" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout3-128-16]" time="0.003"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout3-128-128]" time="0.002"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout6-128-128]" time="1.532" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout3-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout3-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout3-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout4-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout4-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout4-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout4-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout5-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout5-128-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout5-64-64]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout5-32-128]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout6-64-64]" time="0.856" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-float16-False-reduce2d-0-src_layout6-128-16]" time="0.001"><skipped type="pytest.xfail" message="Skipping sum reduction on float16 due to accuracy issues" /></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-64-128-False-False-False-e2m1-e5m2-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw3' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-False-False-False-e2m1-e5m2-4-16-1]'&quot;">worker 'gw3' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-False-False-False-e2m1-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout6-32-128]" time="1.075" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-32-64-True-False-True-e2m1-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-32-64-True-False-True-e4m3-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw10' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-64-True-False-True-e4m3-e4m3-4-16-1]'&quot;">worker 'gw10' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-64-True-False-True-e4m3-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-float16-float16-1-None0]" time="7.345" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout6-32-32]" time="0.432" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-64-64-True-False-True-e5m2-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw11' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-64-True-False-True-e5m2-e5m2-4-16-1]'&quot;">worker 'gw11' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-64-True-False-True-e5m2-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout6-16-16]" time="0.342" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout7-128-16]" time="0.359" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout7-128-128]" time="2.406" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-128-64-True-True-True-e2m1-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw12' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-128-64-True-True-True-e2m1-e5m2-4-16-1]'&quot;">worker 'gw12' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-128-64-True-True-True-e2m1-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout7-64-64]" time="1.151" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout7-32-128]" time="1.362" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-32-64-True-True-True-e2m1-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw13' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-32-64-True-True-True-e2m1-e5m2-4-16-1]'&quot;">worker 'gw13' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-32-64-True-True-True-e2m1-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout7-32-32]" time="0.399" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout7-16-16]" time="0.241" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout8-128-16]" time="0.271" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-64-64-True-True-False-e5m2-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-64-64-True-True-True-e2m1-e4m3-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw14' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-64-True-True-True-e2m1-e4m3-4-16-1]'&quot;">worker 'gw14' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-64-True-True-True-e2m1-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout8-128-128]" time="0.734" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-float16-float16-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-float16-float32-1-None0]" time="7.329" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout8-64-64]" time="0.394" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout8-32-128]" time="0.527" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout8-32-32]" time="0.282" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-1-src_layout8-16-16]" time="0.243" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout0-128-16]" time="0.330" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout0-128-128]" time="2.004" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-64-128-False-True-True-e5m2-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw15' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-128-False-True-True-e5m2-e5m2-4-16-1]'&quot;">worker 'gw15' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-128-False-True-True-e5m2-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout0-64-64]" time="1.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout0-32-128]" time="1.586" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-128-128-False-True-True-e5m2-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw16' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-128-128-False-True-True-e5m2-e5m2-4-16-1]'&quot;">worker 'gw16' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-128-128-False-True-True-e5m2-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-2-None]" time="1.882" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout0-32-32]" time="0.405" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-float16-float32-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-float32-float32-1-None0]" time="5.134" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout0-16-16]" time="0.394" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout1-128-16]" time="0.375" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout1-128-128]" time="2.173" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-2-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-3-None]" time="0.036" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-3-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-4-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-0-None]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-0-1]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-1-None]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-1-1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-2-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-2-1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-3-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-3-1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-4-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-128-4-1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-0-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-1-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-1-1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-2-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-2-1]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-3-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-3-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-3-1]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-4-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int8-512-4-1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-0-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-0-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-0-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-1-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-2-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-3-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-3-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-4-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-128-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-0-None]" time="0.030" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-0-0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-1-None]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-1-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-2-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-3-None]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-3-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-4-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int16-512-4-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-0-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-1-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-2-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-2-1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-3-None]" time="0.064" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-3-0]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-3-1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-4-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-4-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-128-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-0-None]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-1-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-2-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-3-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-3-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-3-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-4-None]" time="0.043" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-4-0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int32-512-4-1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-0-None]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-0-0]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-1-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-2-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-2-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-3-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-3-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-4-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-128-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-0-None]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-1-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-1-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-2-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-3-None]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-3-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-4-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-int64-512-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-3-0]" time="1.848" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-0-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-0-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-1-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout1-64-64]" time="1.014" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-1-1]" time="0.120" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-2-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-3-None]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-3-0]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-3-1]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-4-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-128-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-0-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-0-1]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-1-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-2-None]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-2-0]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-2-1]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-3-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-&amp;]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-^]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-|]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-+]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False--]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-*]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-/]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-%]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-&gt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-&lt;&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-&gt;&gt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-&amp;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-^]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-False-|]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-+]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True--]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-*]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-/]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-%]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-&gt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-&lt;&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-&gt;&gt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-&amp;]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-^]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[False-True-|]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_constexpr_shape" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr_scalar_shape" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reshape[formats0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reshape[formats1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reshape[formats2]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reshape[formats3]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_reshape_err" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_trans_reshape" time="0.056" /><testcase classname="test.unit.language.test_core" name="test_call[1-inline]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_call[1-noinline]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_if[if]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_if[if_and_dynamic]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if[if_exp_static]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_if[if_exp_dynamic]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if[if_exp_dynamic_constexpr]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if[if_exp_dynamic_void]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if[if_and_static]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_num_warps_pow2" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_unary_math[sqrt]" time="0.133" /><testcase classname="test.unit.language.test_core" name="test_unary_math[rsqrt]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_unary_math[exp]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_unary_math[exp2]" time="0.047" /><testcase classname="test.unit.language.test_core" name="test_unary_math[log]" time="0.059" /><testcase classname="test.unit.language.test_core" name="test_unary_math[log2]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_unary_math[sin]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_unary_math[cos]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_inline_asm[1]" time="0.001"><skipped type="pytest.xfail" message="test_inline_asm is only supported in CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout2-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout2-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout2-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout2-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout3-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_inline_asm_packed[1]" time="0.001"><skipped type="pytest.xfail" message="test_inline_asm is only supported in CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout3-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout3-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout4-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout4-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout4-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout5-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout5-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout5-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_inline_asm_with_pointers[1]" time="0.001"><skipped type="pytest.xfail" message="test_inline_asm is only supported in CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout6-128-16]" time="0.180" /><testcase classname="test.unit.language.test_core" name="test_inline_asm_multiple_outputs" time="0.001"><skipped type="pytest.xfail" message="test_inline_asm is only supported in CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout6-128-128]" time="1.106" /><testcase classname="test.unit.language.test_core" name="test_inline_asm_packed_multiple_outputs" time="0.001"><skipped type="pytest.xfail" message="test_inline_asm is only supported in CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_for_iv[1073741824-1073741844-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-3-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_for_iv[34359738368-34359738388-2]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_for_iv[34359738368-34359738388-3]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-4-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_for_iv[15--16--1]" time="0.027" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-4-0]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-uint8-512-4-1]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-0-None]" time="0.355" /><testcase classname="test.unit.language.test_core" name="test_for_iv[15--16--2]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_for_iv[15--16--3]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_for_iv[-18--22--1]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_for_iv[22-18--1]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_if_else" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_if_return[dynamic]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_if_return[static]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if_call[attribute]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_if_call[attribute_jit]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if_call[jit]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if_call[jit_if]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if_call[jit_expr]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_if_call[jit_static_cond]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_if_call[jit_noinline]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_if_call[jit_extern]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[True-True-True]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[True-True-False]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout1-float16-128-128]" time="0.102" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-True-none-tf32-float32-float32-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-int8-int8-1-None0]" time="22.100" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout1-float16-1-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout2-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-0-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-0-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-1-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-1-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-1-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[True-False-True]" time="1.725" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-2-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-2-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout2-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-3-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-3-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-3-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-4-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-4-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-128-4-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-0-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-0-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-0-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-1-None]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-1-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-1-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-2-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout2-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-2-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-2-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-3-None]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-3-0]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-3-1]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-4-None]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-4-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float16-512-4-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-0-None]" time="0.045" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout6-64-64]" time="0.554" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-0-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout2-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-0-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-1-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-1-0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-1-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-2-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-2-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-2-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-3-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-3-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-3-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-4-None]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-4-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout0-float16-64-1]" time="0.057" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-128-4-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-0-None]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-0-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-0-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-1-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout0-float16-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout0-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-1-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout0-float16-1-64]" time="0.648" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-1-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-2-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-2-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-2-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-3-None]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-3-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-3-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-4-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-4-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float32-512-4-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-0-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout6-32-128]" time="0.782" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-0-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-0-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout1-float16-64-1]" time="0.054" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout1-float16-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout1-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout1-float16-1-64]" time="0.059" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout2-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-1-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout2-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-1-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout6-32-32]" time="0.199" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout2-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[True-False-False]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[False-True-True]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[False-True-False]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[False-False-True]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_nested_if_else_return[False-False-False]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_while" time="0.072" /><testcase classname="test.unit.language.test_core" name="test_nested_while" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_constexpr_if_return" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_poison_return" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_num_threads" time="0.081" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout6-16-16]" time="0.091" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout2-src_layout2-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-1-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_globaltimer" time="0.001"><skipped type="pytest.xfail" message="Only for cuda or HIP" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout7-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout7-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout7-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout7-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout7-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout7-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout0-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout8-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout8-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout0-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout8-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout0-float16-1-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout1-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-0-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout1-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout0-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout1-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout0-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout1-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout0-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout2-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout0-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout0-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout0-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout1-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout1-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout1-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout2-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout2-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_smid" time="0.001"><skipped type="pytest.xfail" message="Only for cuda or HIP" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout2-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout2-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout2-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-2-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout2-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout3-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout2-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout3-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout3-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout3-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout3-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout3-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout4-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout4-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout4-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout4-32-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout4-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout4-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout5-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout5-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout5-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout5-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout2-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout5-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout6-128-16]" time="0.092" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout0-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-2-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout6-128-128]" time="0.951" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout3-src_layout2-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout0-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout0-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-2-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout0-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout0-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout0-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout1-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout1-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout1-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout1-float16-1-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout2-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout0-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout2-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-3-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout1-float16-64-1]" time="0.149" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout2-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-3-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout1-float16-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout1-float16-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout1-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout2-float16-64-1]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout2-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-None-src_layout2-float16-1-64]" time="0.126" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout4-src_layout2-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout0-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-3-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout0-shared_layout0-8-16-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout0-shared_layout1-8-16-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout0-shared_layout2-8-16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout6-64-64]" time="0.471" /><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout0-shared_layout3-8-16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout1-shared_layout0-8-16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout1-shared_layout1-8-16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout1-shared_layout2-8-16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_local_load_store[dist_layout1-shared_layout3-8-16-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_local_load_store_mma[shared_layout0-mma_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_local_load_store_mma[shared_layout1-mma_layout0-128-128]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_local_load_store_mma[shared_layout2-mma_layout0-128-128]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert_mma2mma[mma_pair0-float16-16-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_convert_mma2mma[mma_pair0-float16-64-1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_convert_mma2mma[mma_pair0-float16-1-64]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout0-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert_mma2mma[mma_pair0-float16-64-64]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_convert_mma2mma[mma_pair0-float16-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_convert_mma2mma[mma_pair0-float16-256-256]" time="0.047" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-4-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_load_scalar_with_mask" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_ptx_cast[float16]" time="0.124" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout0-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_ptx_cast[int16]" time="0.047" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[0-float8e5-64-64-64-128-256-256]" time="0.172" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-4-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout0-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout6-32-128]" time="0.646" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[0-float8e4nv-64-64-64-128-256-256]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[0-float8e4b15-64-64-64-128-256-256]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[32-float8e5-64-64-64-128-256-256]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout1-float16-64-1]" time="0.058" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[32-float8e4nv-64-64-64-128-256-256]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[32-float8e4b15-64-64-64-128-256-256]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-128-4-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[64-float8e5-64-64-64-128-256-256]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout1-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[64-float8e4nv-64-64-64-128-256-256]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout1-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[64-float8e4b15-64-64-64-128-256-256]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout1-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout2-float16-64-1]" time="0.054" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[128-float8e5-64-64-64-128-256-256]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[128-float8e4nv-64-64-64-128-256-256]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot_max_num_imprecise_acc[128-float8e4b15-64-64-64-128-256-256]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_enable_fp_fusion[False-False]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_enable_fp_fusion[False-True]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout2-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_enable_fp_fusion[True-False]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_enable_fp_fusion[True-True]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_override_arch[False-sm70]" time="0.001"><skipped type="pytest.xfail" message="arch only for CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout1-src_layout2-float16-1-64]" time="0.435" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-0-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_override_arch[False-sm80]" time="0.001"><skipped type="pytest.xfail" message="arch only for CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_override_arch[False-sm90]" time="0.001"><skipped type="pytest.xfail" message="arch only for CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-0-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout6-32-32]" time="0.180" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout0-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_override_arch[True-sm70]" time="0.001"><skipped type="pytest.xfail" message="arch only for CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout6-16-16]" time="0.081" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-0-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout0-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_override_arch[True-sm80]" time="0.001"><skipped type="pytest.xfail" message="arch only for CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout7-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout7-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout7-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout7-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout7-16-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout8-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout8-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout8-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout0-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce2d-1-src_layout8-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout0-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_override_arch[True-sm90]" time="0.001"><skipped type="pytest.xfail" message="arch only for CUDA" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout0-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-1-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout0-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout0-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout0-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout1-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout1-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout1-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout2-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout2-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout2-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout0-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout2-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout2-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout2-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[minimum-NONE-float16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout3-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[minimum-NONE-float32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[minimum-ALL-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout3-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[minimum-ALL-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout3-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[maximum-NONE-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[maximum-NONE-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout3-32-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[maximum-ALL-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout3-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[maximum-ALL-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[clamp-NONE-float16]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-expand_reduce2d-0-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[clamp-NONE-float32]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[2-2]" time="0.145" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[clamp-ALL-float16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_propagate_nan[clamp-ALL-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-1-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_clamp[float16]" time="0.153" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout1-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout1-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout1-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout1-float16-1-64]" time="0.450" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[2-3]" time="0.006" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[3-0]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[3-1]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[3-2]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_clamp[float32]" time="0.006" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[3-3]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[4-0]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_clamp_symmetric[float16]" time="0.006" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[4-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_clamp_symmetric[float32]" time="0.006" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[4-2]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_static_range" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[4-3]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_tl_range" time="0.008" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[5-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_maxnreg" time="0.001"><skipped type="pytest.xfail" message="maxnreg only works on CUDA" /></testcase><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[5-1]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[5-2]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[5-3]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randint[10-0-int32-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-1-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[10-0-int32-False]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randint[10-0-int64-True]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-0-int64-False]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-42-int32-True]" time="0.008" /><testcase classname="test.unit.language.test_random" name="test_randint[10-42-int32-False]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randint[10-42-int64-True]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-42-int64-False]" time="0.008" /><testcase classname="test.unit.language.test_random" name="test_randint[10-124-int32-True]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randint[10-124-int32-False]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randint[10-124-int64-True]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-124-int64-False]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-54-int32-True]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randint[10-54-int32-False]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randint[10-54-int64-True]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-54-int64-False]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-4294967295-int32-True]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randint[10-4294967295-int32-False]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_temp_var_in_loop" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[10-4294967295-int64-True]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_num_programs" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randint[10-4294967295-int64-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_math_extern[float32]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randint[10-67830198458-int32-True]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_unroll_attr" time="0.012" /><testcase classname="test.unit.language.test_random" name="test_randint[10-67830198458-int32-False]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randint[10-67830198458-int64-True]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_side_effectful_reduction" time="0.001"><skipped type="pytest.xfail" message="" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[10-67830198458-int64-False]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-0-int32-True]" time="0.022" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-0-int32-False]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-2-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-0-int64-True]" time="0.025" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-0-int64-False]" time="0.024" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-42-int32-True]" time="0.022" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-42-int32-False]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout2-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout2-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-42-int64-True]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_side_effectful_reduction_2d[0]" time="0.001"><skipped type="pytest.xfail" message="" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout2-src_layout2-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout0-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-42-int64-False]" time="0.024" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-124-int32-True]" time="0.022" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-124-int32-False]" time="0.022" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-124-int64-True]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-2-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-124-int64-False]" time="0.024" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-54-int32-True]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_side_effectful_reduction_2d[1]" time="0.001"><skipped type="pytest.xfail" message="" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-54-int32-False]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout0-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-54-int64-True]" time="0.024" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-54-int64-False]" time="0.024" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-4294967295-int32-True]" time="0.023" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-4294967295-int32-False]" time="0.022" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-4294967295-int64-True]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_dtype" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_side_effectful_scan" time="0.001"><skipped type="pytest.xfail" message="" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-4294967295-int64-False]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout0-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-67830198458-int32-True]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-2-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[4,53-67830198458-int32-False]" time="0.022" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-67830198458-int64-True]" time="0.024" /><testcase classname="test.unit.language.test_random" name="test_randint[4,53-67830198458-int64-False]" time="0.024" /><testcase classname="test.unit.language.test_random" name="test_randint[400-0-int32-True]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_chained_reductions[in_shape0-perm0-red_dims0]" time="0.001"><skipped type="pytest.xfail" message="XPU: Not enough shared memory" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-0-int32-False]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout0-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-0-int64-True]" time="0.039" /><testcase classname="test.unit.language.test_random" name="test_randint[400-0-int64-False]" time="0.039" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-3-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-42-int32-True]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_chained_reductions[in_shape1-perm1-red_dims1]" time="0.056"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 73728, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">in_shape = (8, 2, 32, 4, 16), perm = [4, 0, 1, 3, 2], red_dims = [0, 2, 0]
device = 'xpu'

    @pytest.mark.parametrize("in_shape, perm, red_dims", [
        ((4, 32, 32, 4, 2), [2, 1, 0, 3, 4], [3, 1, 0]),
        ((8, 2, 32, 4, 16), [4, 0, 1, 3, 2], [0, 2, 0]),
    ])
    def test_chained_reductions(in_shape, perm, red_dims, device):
        if is_xpu() and in_shape == (4, 32, 32, 4, 2):
            # check maximum shared memory
            if triton.runtime.driver.active.utils.get_device_properties(
                    triton.runtime.driver.active.get_current_device())["max_shared_mem"] &lt;= 163840:
                pytest.xfail("XPU: Not enough shared memory")
    
        @triton.jit
        def kernel(In, Out,  #
                   dim_0: tl.constexpr, dim_1: tl.constexpr, dim_2: tl.constexpr, dim_3: tl.constexpr, dim_4: tl.constexpr,
                   perm_0: tl.constexpr, perm_1: tl.constexpr, perm_2: tl.constexpr, perm_3: tl.constexpr,
                   perm_4: tl.constexpr, red_dim_0: tl.constexpr, red_dim_1: tl.constexpr, red_dim_2: tl.constexpr):
            idx = tl.arange(0, dim_0 * dim_1 * dim_2 * dim_3 * dim_4)
            idx = idx.reshape(dim_0, dim_1, dim_2, dim_3, dim_4)
            vals = tl.load(In + idx)
            vals = tl.permute(vals, [perm_0, perm_1, perm_2, perm_3, perm_4])
            r = tl.sum(tl.sum(tl.sum(vals, red_dim_0), red_dim_1), red_dim_2)
            st_idx = tl.arange(0, r.shape[0] * r.shape[1]).reshape(r.shape)
            tl.store(Out + st_idx, r)
    
        input = torch.randint(0, 1000, in_shape, device=device, dtype=torch.int32)
        temp = torch.permute(input, perm).contiguous()
        ref = torch.sum(torch.sum(torch.sum(temp, dim=red_dims[0]), dim=red_dims[1]), dim=red_dims[2])
        result = torch.empty_like(ref)
&gt;       kernel[(1, )](input, result, input.shape[0], input.shape[1], input.shape[2], input.shape[3], input.shape[4],
                      perm[0], perm[1], perm[2], perm[3], perm[4], red_dims[0], red_dims[1], red_dims[2])

language/test_core.py:6631: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7ebc50367440&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 73728, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-42-int32-False]" time="0.037" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout1-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout1-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout1-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-int64]" time="0.645" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout1-float16-1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout2-float16-64-1]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout2-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout3-src_layout2-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout0-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-3-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_gather[src_shape0-indices_shape0-0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout0-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_gather[src_shape1-indices_shape1-0]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_gather[src_shape2-indices_shape2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_gather_warp_shuffle[src_shape0-indices_shape0-0-linear&lt;{register = [[0, 2], [2, 0]], lane = [[0, 8], [8, 0], [1, 0], [4, 0], [16, 0]], warp = [[0, 1], [0, 4]], block = []}&gt;-linear&lt;{register = [[2, 0], [0, 2]], lane = [[0, 8], [16, 0], [1, 0], [8, 0], [4, 0]], warp = [[0, 1], [0, 4]], block = []}&gt;]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_gather_warp_shuffle[src_shape1-indices_shape1-0-linear&lt;{register = [[0, 2], [32, 0], [2, 0], [0, 16], [0, 32], [64, 0]], lane = [[0, 8], [8, 0], [1, 0], [4, 0], [16, 0]], warp = [[0, 1], [0, 4]], block = []}&gt;-linear&lt;{register = [[0, 2], [32, 0], [0, 32], [2, 0], [0, 16], [64, 0], [128, 0]], lane = [[0, 8], [8, 0], [1, 0], [4, 0], [16, 0]], warp = [[0, 1], [0, 4]], block = []}&gt;]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_zero_strided_tensors" time="0.006" /><testcase classname="test.unit.language.test_decorator" name="test_decorator_with_def" time="0.002" /><testcase classname="test.unit.language.test_decorator" name="test_triton_heuristic" time="0.004" /><testcase classname="test.unit.language.test_libdevice" name="test_bessel[j0-bessel_j0-float32]" time="0.009" /><testcase classname="test.unit.language.test_libdevice" name="test_bessel[j0-bessel_j0-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_libdevice" name="test_bessel[j1-bessel_j1-float32]" time="0.010" /><testcase classname="test.unit.language.test_libdevice" name="test_bessel[j1-bessel_j1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_libdevice" name="test_bessel[y0-bessel_y0-float32]" time="0.009" /><testcase classname="test.unit.language.test_libdevice" name="test_bessel[y0-bessel_y0-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_libdevice" name="test_bessel[y1-bessel_y1-float32]" time="0.010" /><testcase classname="test.unit.language.test_libdevice" name="test_bessel[y1-bessel_y1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_libdevice" name="test_bessel[cyl_bessel_i0-i0-float32]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-3-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_libdevice" name="test_bessel[cyl_bessel_i0-i0-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_libdevice" name="test_bessel[cyl_bessel_i1-i1-float32]" time="0.006" /><testcase classname="test.unit.language.test_libdevice" name="test_bessel[cyl_bessel_i1-i1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_libdevice" name="test_libdevice_rename" time="0.004" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_matmul[True]" time="0.000"><error message="failed on setup with &quot;worker 'gw17' crashed while running 'test/unit/language/test_pipeliner.py::test_pipeline_matmul[True]'&quot;">worker 'gw17' crashed while running 'test/unit/language/test_pipeliner.py::test_pipeline_matmul[True]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout0-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout0-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-4-None]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-uint8]" time="0.632" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout1-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout1-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout1-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout1-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout2-float16-64-1]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout2-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout0-interm_layout4-src_layout2-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout0-float16-64-1]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout0-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-4-0]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout0-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout1-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout1-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-float64-512-4-1]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout1-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-0-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-0-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-1-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout1-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-1-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-2-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-2-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-2-1]" time="0.005" /><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-uint16]" time="0.637" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-3-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-3-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-4-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-128-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-0-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-1-None]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-1-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-1-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-2-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-2-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-2-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-3-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-3-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-3-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-4-None]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-4-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bfloat16-512-4-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load_scalar[0-True-1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout2-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout2-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load_scalar[0-False-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-None-src_layout2-float16-1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load_scalar[1-True-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load_scalar[1-False-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout0-float16-64-1]" time="0.049" /><testcase classname="test.unit.language.test_core" name="test_masked_load_shared_memory[dtype0]" time="0.062" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout0-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_masked_load_shared_memory[dtype1]" time="0.049" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout0-float16-1-64]" time="0.647" /><testcase classname="test.unit.language.test_core" name="test_masked_load_shared_memory[dtype2]" time="0.118" /><testcase classname="test.unit.language.test_core" name="test_load_cache_modifier[]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_load_cache_modifier[.ca]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_load_cache_modifier[.cg]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_load_cache_modifier[.cv]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_vectorization[1-16]" time="0.025" /><testcase classname="test.unit.language.test_core" name="test_vectorization[1-10]" time="0.029" /><testcase classname="test.unit.language.test_core" name="test_vectorization[1-11]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_vectorization[1-1024]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_vectorization_hints[False]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_vectorization_hints[True]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_assume" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_store_cache_modifier[]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_cache_modifier[.wb]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_cache_modifier[.cg]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_cache_modifier[.cs]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_cache_modifier[.wt]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_store_eviction_policy[]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_store_eviction_policy[evict_last]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_store_eviction_policy[evict_first]" time="0.001" /><testcase classname="test.unit.language.test_core" name="test_default" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_noop" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_pointer_arguments[xpu]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_pointer_arguments[cpu]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_pointer_arguments[cpu_pinned]" time="0.001"><skipped type="pytest.xfail" message="RuntimeError: Pinned memory requires CUDA." /></testcase><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-uint32]" time="0.641" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[-1-i32]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[0-i32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[-2147483648-i32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[2147483647-i32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[2147483648-i64]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[4294967295-i64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[4294967296-i64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[9223372036854775807-i64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[-9223372036854775808-i64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[9223372036854775808-u64]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_value_specialization[18446744073709551615-u64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization_overflow[18446744073709551615-False]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_value_specialization_overflow[18446744073709551616-True]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_value_specialization_overflow[-9223372036854775808-False]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_value_specialization_overflow[-9223372036854775809-True]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-+]" time="0.023" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False--]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout1-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-*]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-/]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-%]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-&gt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-&lt;&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-&gt;&gt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-&amp;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-^]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-False-|]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-+]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True--]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-*]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-/]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-%]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-&gt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-&lt;&lt;]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_bin_op_constexpr[True-True-&gt;&gt;]" time="0.005" /><testcase classname="test.unit.language.test_tuple" name="test_index[4]" time="0.006" /><testcase classname="test.unit.language.test_tuple" name="test_assign" time="0.005" /><testcase classname="test.unit.language.test_tuple" name="test_serialize" time="0.007" /><testcase classname="test.unit.language.test_tuple" name="test_namedtuple" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout1-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout0-128-16]" time="0.409" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout1-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-uint64]" time="0.724" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-int8-int8-1-None1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-float16-float16-1-None0]" time="10.112" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout1-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout0-128-128]" time="2.768" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout2-float16-64-1]" time="0.065" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout2-float16-64-64]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout2-float16-128-128]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout1-src_layout2-float16-1-64]" time="0.206" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout0-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout0-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout0-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout1-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-float16]" time="0.699" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout1-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout1-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout1-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout2-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout2-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout2-src_layout2-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout0-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout0-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout0-float16-1-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout1-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-float32]" time="0.658" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout1-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout1-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout1-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout2-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout2-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout3-src_layout2-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout0-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout0-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout0-float16-1-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout1-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout1-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_standard" name="test_maximum_minium[minimum-bfloat16]" time="0.716" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout1-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout1-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout2-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout2-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout2-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout1-interm_layout4-src_layout2-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout0-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout0-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout0-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout0-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout1-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout1-float16-64-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout1-float16-128-128]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout1-float16-1-64]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout2-float16-64-1]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout2-float16-64-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout0-64-64]" time="1.290" /><testcase classname="test.unit.language.test_standard" name="test_sort[int32-False-1-512]" time="0.242" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-32-128-True-False-False-e4m3-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw20' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-128-True-False-False-e4m3-e5m2-4-16-1]'&quot;">worker 'gw20' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-128-True-False-False-e4m3-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout2-float16-128-128]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-None-src_layout2-float16-1-64]" time="0.001"><skipped type="pytest.xfail" message="Do not convert same layout" /></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[int32-False-8-64]" time="0.635" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout0-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout0-float16-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout0-float16-128-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout0-float16-1-64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout1-float16-64-1]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_convert2d[dst_layout2-interm_layout1-src_layout1-float16-64-64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout4-16-16]" time="0.030" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout5-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout5-128-128]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout5-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout5-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout5-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout5-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout6-128-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout6-128-128]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout6-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout6-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout6-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout7-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout7-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout7-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout8-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout8-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout8-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout8-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout0-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout0-128-128]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout0-64-64]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout0-32-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout0-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout0-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout1-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout1-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout1-64-64]" time="0.011" /><testcase classname="test.unit.language.test_standard" name="test_sort[int32-False-256-16]" time="4.173" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout1-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout2-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout2-128-128]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout2-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout2-32-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout2-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout2-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout3-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout3-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout3-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout3-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout4-128-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout4-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout4-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout4-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout4-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout4-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout5-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout5-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout5-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout5-32-128]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout5-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout0-32-128]" time="2.447" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout5-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout6-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout6-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout6-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout6-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout6-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout7-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-64-128-True-False-True-e2m1-e4m3-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw19' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-True-False-True-e2m1-e4m3-4-16-1]'&quot;">worker 'gw19' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-True-False-True-e2m1-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout0-32-32]" time="0.579" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout0-16-16]" time="0.392" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout1-128-16]" time="0.441" /><testcase classname="test.unit.language.test_standard" name="test_sort[int32-False-512-8]" time="5.863" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout1-128-128]" time="3.214" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-64-False-False-False-e5m2-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw21' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-False-False-e5m2-e4m3-4-16-1]'&quot;">worker 'gw21' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-False-False-e5m2-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-float16-float16-1-None1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-float16-float32-1-None0]" time="18.369" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-32-64-True-True-True-e2m1-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw22' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-64-True-True-True-e2m1-e4m3-4-16-1]'&quot;">worker 'gw22' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-64-True-True-True-e2m1-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout1-64-64]" time="1.280" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout1-32-128]" time="1.593" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-int8-int8-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-float16-float16-1-None0]" time="1.590" /><testcase classname="test.unit.language.test_standard" name="test_sort[int32-True-1-512]" time="0.181" /><testcase classname="test.unit.language.test_standard" name="test_sort[int32-True-8-64]" time="0.658" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout1-32-32]" time="0.441" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-float16-float16-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-float16-float32-1-None0]" time="7.423" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout1-16-16]" time="0.349" /><testcase classname="test.unit.language.test_standard" name="test_sort[int32-True-256-16]" time="4.137" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout2-128-16]" time="0.601" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout2-128-128]" time="2.630" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-64-False-True-False-e2m1-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-64-False-True-False-e4m3-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw23' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-True-False-e4m3-e4m3-4-16-1]'&quot;">worker 'gw23' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-True-False-e4m3-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-32-128-False-True-False-e5m2-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw24' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-128-False-True-False-e5m2-e4m3-4-16-1]'&quot;">worker 'gw24' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-128-False-True-False-e5m2-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout2-64-64]" time="1.267" /><testcase classname="test.unit.language.test_standard" name="test_sort[int32-True-512-8]" time="5.824" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout2-32-128]" time="2.597" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-float16-float32-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-float32-float32-1-None0]" time="4.819" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout2-32-32]" time="0.593" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout2-16-16]" time="0.386" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout3-128-16]" time="0.681" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-64-64-False-True-True-e2m1-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw25' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-64-False-True-True-e2m1-e5m2-4-16-1]'&quot;">worker 'gw25' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-64-False-True-True-e2m1-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout3-128-128]" time="3.138" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-64-128-False-False-True-e4m3-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw26' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-128-False-False-True-e4m3-e4m3-4-16-1]'&quot;">worker 'gw26' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-128-False-False-True-e4m3-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float16-False-1-512]" time="0.271" /><testcase classname="test.unit.language.test_standard" name="test_sort[float16-False-8-64]" time="0.631" /><testcase classname="test.unit.language.test_standard" name="test_sort[float16-False-256-16]" time="4.931" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-True-False-none-tf32-float32-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-int8-int8-1-None0]" time="8.364" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout3-64-64]" time="1.582" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout3-32-128]" time="2.557" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-float16-float32-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-float32-float32-1-None0]" time="26.077" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-128-64-False-False-True-e5m2-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw27' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-128-64-False-False-True-e5m2-e5m2-4-16-1]'&quot;">worker 'gw27' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-128-64-False-False-True-e5m2-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float16-False-512-8]" time="6.814" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout3-32-32]" time="0.531" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-32-64-True-False-False-e2m1-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw28' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-32-64-True-False-False-e2m1-e5m2-4-16-1]'&quot;">worker 'gw28' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-32-64-True-False-False-e2m1-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout3-16-16]" time="0.316" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout4-128-16]" time="0.805" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout4-128-128]" time="2.869" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-float16-float16-1-None0]" time="5.243" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout4-64-64]" time="1.118" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout4-32-128]" time="2.480" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-32-128-True-False-False-e4m3-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw29' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-32-128-True-False-False-e4m3-e5m2-4-16-1]'&quot;">worker 'gw29' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-32-128-True-False-False-e4m3-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float16-True-1-512]" time="0.226" /><testcase classname="test.unit.language.test_standard" name="test_sort[float16-True-8-64]" time="0.660" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-64-64-False-True-False-e5m2-bf16-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw30' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-64-False-True-False-e5m2-bf16-4-16-1]'&quot;">worker 'gw30' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-64-False-True-False-e5m2-bf16-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float16-True-256-16]" time="5.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout4-32-32]" time="0.553" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout4-16-16]" time="0.429" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout5-128-16]" time="0.418" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-float16-float16-1-None1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-float16-float32-1-None0]" time="4.772" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout5-128-128]" time="3.093" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-int8-int8-1-None1]" time="85.940"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e41e0ad0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float16-True-512-8]" time="6.775" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout5-64-64]" time="1.416" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-64-128-False-True-True-e2m1-bf16-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw31' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-128-False-True-True-e2m1-bf16-4-16-1]'&quot;">worker 'gw31' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-64-128-False-True-True-e2m1-bf16-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-128-64-False-False-True-e4m3-bf16-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw32' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-128-64-False-False-True-e4m3-bf16-4-16-1]'&quot;">worker 'gw32' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[128-128-64-False-False-True-e4m3-bf16-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout5-32-128]" time="2.634" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-float16-float32-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-float32-float32-1-None0]" time="3.850" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout5-32-32]" time="0.597" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout5-16-16]" time="0.439" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout6-128-16]" time="0.405" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-True-none-tf32-float32-float32-1-None1]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-int8-int8-1-None0]" time="8.645" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout6-128-128]" time="1.767" /><testcase classname="test.unit.language.test_standard" name="test_sort[float32-False-1-512]" time="0.206" /><testcase classname="test.unit.language.test_standard" name="test_sort[float32-False-8-64]" time="0.687" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[128-128-128-False-False-True-e5m2-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout6-64-64]" time="0.838" /><testcase classname="test.unit.language.test_standard" name="test_sort[float32-False-256-16]" time="4.973" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout6-32-128]" time="1.620" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_const[direct-False-False]" time="1.792" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-8-16-float16-float16]" time="2.432" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout6-32-32]" time="0.460" /><testcase classname="test.unit.language.test_core" name="test_const[call-True-True]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_const[call-True-False]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_const[call-False-True]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_const[call-False-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_const[ternary-True-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_const[ternary-True-False]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_const[ternary-False-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_const[ternary-False-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_const[if-True-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_const[if-True-False]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_const[if-False-True]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_const[if-False-False]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot_without_load[float32]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_dot_without_load[float16]" time="0.147" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout6-16-16]" time="0.359" /><testcase classname="test.unit.language.test_core" name="test_arange[1-0]" time="0.030" /><testcase classname="test.unit.language.test_core" name="test_arange[1-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_arange[1-7]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_arange[1-16]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-0-None]" time="0.092" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-0-0]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-0-1]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-1-None]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-1-0]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-1-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-2-None]" time="0.041" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout7-128-16]" time="0.720" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-2-0]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-2-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-3-None]" time="0.029" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-3-0]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-3-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-4-None]" time="0.028" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-4-0]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-128-4-1]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-0-None]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-0-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-0-1]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-1-None]" time="0.024" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-1-0]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_masked_load[1-bool-512-1-1]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_matmul[False]" time="7.742" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-1-32-32-32-8-16-float32-float32]" time="0.528" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-True-none-tf32-float32-float32-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-int8-int8-1-None0]" time="55.330" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout7-128-128]" time="2.974" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-8-16-float16-float16]" time="0.439" /><testcase classname="test.unit.language.test_standard" name="test_sort[float32-False-512-8]" time="6.908" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-2-32-32-32-8-16-float32-float32]" time="0.489" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-float16-float16-1-None0]" time="5.105" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-8-16-float16-float16]" time="0.466" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout7-64-64]" time="1.415" /><testcase classname="test.unit.language.test_core" name="test_dot3d[1-4-32-32-32-8-16-float32-float32]" time="0.505" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout7-32-128]" time="2.443" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-8-16-float16-float16]" time="1.487" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-1-32-32-32-8-16-float32-float32]" time="0.844" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-False-reduce1d-0-src_layout7-32-32]" time="0.581" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_vecadd" time="5.185" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout8-64-64]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout8-32-128]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout8-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce1d-1-src_layout8-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout0-128-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout0-128-128]" time="0.032" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout0-64-64]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout0-32-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout0-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout7-32-128]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout7-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout7-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout8-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout8-128-128]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout8-64-64]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout8-32-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout8-32-32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-0-src_layout8-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout0-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout0-128-128]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout0-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout0-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout0-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout0-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout1-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout1-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout1-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout1-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout1-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout1-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout2-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-expand_reduce2d-1-src_layout2-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-32-64-False-False-True-e4m3-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-32-64-False-False-True-e5m2-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw18' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-64-False-False-True-e5m2-e4m3-4-16-1]'&quot;">worker 'gw18' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-64-False-False-True-e5m2-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float32-True-1-512]" time="0.184" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-float16-float16-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-float16-float32-1-None0]" time="5.978" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float32-True-8-64]" time="0.686" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-8-16-float16-float16]" time="0.958" /><testcase classname="test.unit.language.test_standard" name="test_sort[float32-True-256-16]" time="5.020" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-2-32-32-32-8-16-float32-float32]" time="0.545" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-8-16-float16-float16]" time="0.721" /><testcase classname="test.unit.language.test_core" name="test_dot3d[2-4-32-32-32-8-16-float32-float32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[1-0]" time="0.172" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-8-16-float16-float16]" time="0.013"><failure message="AssertionError: &#10;Not equal to tolerance rtol=0.01, atol=0.01&#10;&#10;Mismatched elements: 1 / 8192 (0.0122%)&#10;Max absolute difference: 0.04688&#10;Max relative difference: 5.3&#10; x: array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,&#10;          -0.7812 ],&#10;        [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,...&#10; y: array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,&#10;          -0.7803 ],&#10;        [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,...">B = 8, num_warps = 1, M = 32, N = 32, K = 32, BLOCK_M = 8, BLOCK_N = 16
in_dtype_str = 'float16', out_dtype_str = 'float16', device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str",
                             [(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str)
                              for B in [1, 2, 4, 8]
                              for num_warps in [1, 2, 4, 8, 16]
                              for BLOCK_M, BLOCK_N in [(32, 32)]
                              for M, N, K in [(64, 64, 64), (32, 32, 32)]
                              for in_dtype_str, out_dtype_str in [('int8', 'int8'), ('float16', 'float16'),
                                                                  ('float16', 'float32'), ('float32', 'float32')]] +
                             # Large block sizes
                             [(4, 4, 128, 128, 64, 64, 64, 'float16', 'float16')] +
                             # Small block sizes
                             [(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str)
                              for B in [1, 2, 8]
                              for num_warps in [1, 2, 4]
                              for BLOCK_M, BLOCK_N in [(1, 32), (32, 2), (8, 8), (8, 16)]
                              for M, N, K in [(32, 32, 32)]
                              for in_dtype_str, out_dtype_str in [('float16', 'float16'), ('float32', 'float32')]])
    def test_dot3d(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str, device):
        if is_hip():
            # hip does not support tf32 precision, so use ieee for all tests
            input_precision = "ieee"
            arch = triton.runtime.driver.active.get_current_target().arch
            if "gfx11" in arch or "gfx12" in arch:
                if in_dtype_str == "float32":
                    pytest.skip(f"{in_dtype_str} is not supported in WMMA dot, FMA does not support dot3d")
                if out_dtype_str == "float16":
                    pytest.skip(f"{out_dtype_str} has low precision in WMMA dot")
        else:
            input_precision = "tf32" if (is_cuda() or is_xpu()) and in_dtype_str == 'float32' else "ieee"
            if is_xpu():
                if (BLOCK_M &lt; 8 or BLOCK_N &lt; 16):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_interpreter() and (BLOCK_M &lt; 16 or BLOCK_N &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
    
        if B == 8 and M == 64 and in_dtype_str == "float32" and out_dtype_str == "float32":
            if not is_interpreter() and triton.runtime.driver.active.utils.get_device_properties(
                    triton.runtime.driver.active.get_current_device())["max_shared_mem"] &lt; 131072:
                pytest.skip(
                    "Skipping tests with B = 8, M = 64, in_type = float32, out_type = float32 due to insufficient shared memory (less than 128 KB per SM) on this GPU."
                )
    
        @triton.jit
        def kernel(
            q_ptr,
            k_ptr,
            o_ptr,
            stride_qb,
            stride_qm,
            stride_qk,
            stride_kb,
            stride_kk,
            stride_kn,
            stride_ob,
            stride_om,
            stride_on,
            BLOCK_B: tl.constexpr,
            BLOCK_M: tl.constexpr,
            BLOCK_N: tl.constexpr,
            BLOCK_K: tl.constexpr,
            INPUT_PRECISION: tl.constexpr,
            out_dtype: tl.constexpr = tl.float32,
        ):
            startm = tl.program_id(0) * BLOCK_M
            startn = tl.program_id(1) * BLOCK_N
            offs_b = tl.arange(0, BLOCK_B)
            offs_m = startm + tl.arange(0, BLOCK_M)
            offs_n = startn + tl.arange(0, BLOCK_N)
            offs_k = tl.arange(0, BLOCK_K)
            q_ptrs = q_ptr + offs_b[:, None, None] * stride_qb + offs_m[None, :, None] * stride_qm + offs_k[
                None, None, :] * stride_qk
            k_ptrs = k_ptr + offs_b[:, None, None] * stride_kb + offs_k[None, :, None] * stride_kk + offs_n[
                None, None, :] * stride_kn
            q = tl.load(q_ptrs)
            k = tl.load(k_ptrs)
            qk = tl.dot(q, k, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            o_ptrs = o_ptr + offs_b[:, None, None] * stride_ob + offs_m[None, :, None] * stride_om + offs_n[
                None, None, :] * stride_on
            tl.store(o_ptrs, qk)
    
        if out_dtype_str == 'int8':
            out_dtype = tl.int8
        elif out_dtype_str == 'float16':
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        rs = RandomState(17)
        x = numpy_random((B, M, K), dtype_str=in_dtype_str, rs=rs)
        y = numpy_random((B, K, N), dtype_str=in_dtype_str, rs=rs)
        if in_dtype_str == 'int8':
            out = numpy_random((B, M, N), dtype_str='int32', rs=rs)
        else:
            if is_hip() and (BLOCK_M &lt; 16 or BLOCK_N &lt; 16) and out_dtype_str == 'float16':
                # float16 accumulator in FMA dot loose precision too fast
                x *= 0.1
                y *= 0.1
            out = numpy_random((B, M, N), dtype_str=out_dtype_str, rs=rs)
    
        x_tri = to_triton(x, device=device)
        y_tri = to_triton(y, device=device)
        out_tri = to_triton(out, device=device)
    
        BLOCK_B = B
        BLOCK_K = K
    
        grid = (
            triton.cdiv(M, BLOCK_M),
            triton.cdiv(N, BLOCK_N),
        )
        kernel[grid](
            x_tri,
            y_tri,
            out_tri,
            x_tri.stride(0),
            x_tri.stride(1),
            x_tri.stride(2),
            y_tri.stride(0),
            y_tri.stride(1),
            y_tri.stride(2),
            out_tri.stride(0),
            out_tri.stride(1),
            out_tri.stride(2),
            BLOCK_B=BLOCK_B,
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
            BLOCK_K=BLOCK_K,
            INPUT_PRECISION=input_precision,
            out_dtype=out_dtype,
            num_warps=num_warps,
        )
    
        if in_dtype_str == 'int8':
            out_ref = np.matmul(x.astype(np.float32), y.astype(np.float32)).astype(np.int32)
        else:
            out_ref = np.matmul(x, y)
&gt;       np.testing.assert_allclose(out_ref, to_numpy(out_tri), rtol=0.01, atol=1e-2)

language/test_core.py:3988: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (&lt;function assert_allclose.&lt;locals&gt;.compare at 0x72ba5a7c39c0&gt;, array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,
          -0.7812 ],
        [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,
          -1.988  ],
        [  5.625  ,  -7.008  ,   0.3352 , ...,  -9.94   ,  -4.54   ,
          -3.932  ],
        ...,
        [  3.283  ,  -4.203  ,   6.477  , ...,  -5.062  ,   8.56   ,
           1.532  ],
        [ -3.682  ,  -0.4055 ,  -9.195  , ...,   3.326  ,  -5.33   ,
           4.664  ],
        [  0.2546 ,  -0.05045,   3.504  , ...,   0.989  ,   3.068  ,
         -10.04   ]],

       [[  0.893  ,   9.664  ,   3.02   , ...,   1.995  ,  -3.467  ,
           4.867  ],
        [  1.944  ,  -1.934  ,   2.781  , ...,  -4.133  ,   1.016  ,
           2.2    ],
        [ -7.508  ,  -5.14   ,   0.98   , ..., -10.375  ,   0.0855 ,
          -6.4    ],
        ...,
        [  6.57   ,   6.62   ,  -9.47   , ...,  -5.375  ,  -0.38   ,
          -0.5933 ],
        [ -2.744  ,   3.22   , -11.01   , ...,  -9.25   ,   8.85   ,
           0.6387 ],
        [ -9.31   , -13.49   ,   1.81   , ...,   3.744  ,   5.258  ,
           3.582  ]],

       [[ 15.086  ,  -4.258  ,  -1.312  , ...,  -5.254  , -10.11   ,
           1.088  ],
        [  1.731  ,  -7.285  ,  -3.436  , ...,   1.447  ,  -0.61   ,
          15.164  ],
        [  2.785  ,   2.49   ,  10.13   , ...,  -3.564  ,  -4.14   ,
          -1.098  ],
        ...,
        [  9.914  ,  -0.965  ,  -1.426  , ...,   0.9814 , -10.     ,
           2.895  ],
        [  2.209  , -10.28   ,  -5.867  , ..., -21.84   ,   1.362  ,
           8.555  ],
        [  6.906  ,  -2.012  ,   1.046  , ...,   1.676  ,   7.06   ,
           8.08   ]],

       ...,

       [[ -3.225  ,  -4.36   ,  -7.312  , ...,   2.512  ,   3.041  ,
          -8.29   ],
        [ -7.21   ,  -2.969  ,  -0.5684 , ...,   2.182  ,  -6.477  ,
          -2.574  ],
        [ -3.236  ,   0.04898,   2.79   , ...,   1.393  ,   3.607  ,
          11.5    ],
        ...,
        [ -3.938  ,  12.8    ,  -3.084  , ...,  -4.25   ,   0.1508 ,
          -6.47   ],
        [  2.732  ,  -3.33   ,  -9.5    , ...,   0.4858 ,   9.73   ,
           4.457  ],
        [ -0.158  ,   1.752  ,  -1.329  , ...,   0.6816 ,  -9.05   ,
          -8.7    ]],

       [[ -3.566  ,   5.094  ,  -6.645  , ...,   5.066  ,   7.605  ,
           5.242  ],
        [  5.676  ,   7.887  ,  -0.459  , ...,  -0.7827 ,   3.318  ,
           2.72   ],
        [ -4.86   ,  -5.766  ,   7.95   , ...,   4.793  ,  -3.594  ,
          13.31   ],
        ...,
        [  1.794  ,   3.436  ,   1.274  , ...,  10.74   ,   0.0841 ,
           2.248  ],
        [  2.535  ,   0.6626 ,  -4.727  , ...,   5.195  ,  -4.477  ,
           0.9097 ],
        [  3.139  ,   2.152  ,   1.08   , ..., -10.87   ,  -4.68   ,
           3.613  ]],

       [[ 11.     ,  14.37   ,  11.38   , ...,   7.61   ,  -1.203  ,
           6.008  ],
        [ -4.04   , -12.05   ,  -4.1    , ..., -11.38   ,   2.613  ,
          -0.661  ],
        [-12.51   ,   1.341  ,   3.652  , ...,  -3.684  ,   0.421  ,
          -8.555  ],
        ...,
        [  2.924  ,  -5.1    ,  -2.48   , ...,   1.538  ,  -0.6533 ,
          -0.4302 ],
        [ -0.1746 ,  -2.014  ,   7.96   , ...,   7.16   ,   5.83   ,
           5.684  ],
        [ -1.436  ,  11.32   ,   3.916  , ...,  -0.2876 ,  -2.12   ,
          -1.1455 ]]], dtype=float16), array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,
          -0.7803 ],
        [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,
          -1.987  ],
        [  5.62   ,  -7.008  ,   0.3328 , ...,  -9.945  ,  -4.54   ,
          -3.934  ],
        ...,
        [  3.283  ,  -4.203  ,   6.48   , ...,  -5.07   ,   8.555  ,
           1.53   ],
        [ -3.684  ,  -0.4045 ,  -9.2    , ...,   3.324  ,  -5.324  ,
           4.66   ],
        [  0.253  ,  -0.04852,   3.502  , ...,   0.988  ,   3.068  ,
         -10.04   ]],

       [[  0.893  ,   9.67   ,   3.02   , ...,   1.994  ,  -3.463  ,
           4.863  ],
        [  1.945  ,  -1.933  ,   2.777  , ...,  -4.137  ,   1.014  ,
           2.201  ],
        [ -7.508  ,  -5.137  ,   0.9775 , ..., -10.37   ,   0.0782 ,
          -6.402  ],
        ...,
        [  6.566  ,   6.613  ,  -9.46   , ...,  -5.37   ,  -0.3806 ,
          -0.593  ],
        [ -2.748  ,   3.223  , -11.     , ...,  -9.25   ,   8.85   ,
           0.636  ],
        [ -9.31   , -13.484  ,   1.81   , ...,   3.742  ,   5.254  ,
           3.58   ]],

       [[ 15.086  ,  -4.25   ,  -1.31   , ...,  -5.258  , -10.11   ,
           1.086  ],
        [  1.734  ,  -7.293  ,  -3.438  , ...,   1.445  ,  -0.6064 ,
          15.164  ],
        [  2.787  ,   2.488  ,  10.15   , ...,  -3.566  ,  -4.145  ,
          -1.099  ],
        ...,
        [  9.914  ,  -0.968  ,  -1.424  , ...,   0.978  , -10.     ,
           2.896  ],
        [  2.21   , -10.27   ,  -5.87   , ..., -21.8    ,   1.364  ,
           8.555  ],
        [  6.906  ,  -2.01   ,   1.047  , ...,   1.675  ,   7.062  ,
           8.06   ]],

       ...,

       [[ -3.23   ,  -4.363  ,  -7.31   , ...,   2.51   ,   3.041  ,
          -8.3    ],
        [ -7.207  ,  -2.965  ,  -0.5684 , ...,   2.182  ,  -6.473  ,
          -2.574  ],
        [ -3.236  ,   0.04822,   2.799  , ...,   1.394  ,   3.607  ,
          11.53   ],
        ...,
        [ -3.941  ,  12.79   ,  -3.082  , ...,  -4.25   ,   0.1515 ,
          -6.47   ],
        [  2.732  ,  -3.334  ,  -9.516  , ...,   0.4878 ,   9.734  ,
           4.453  ],
        [ -0.159  ,   1.751  ,  -1.329  , ...,   0.6797 ,  -9.04   ,
          -8.7    ]],

       [[ -3.568  ,   5.098  ,  -6.65   , ...,   5.07   ,   7.61   ,
           5.246  ],
        [  5.676  ,   7.89   ,  -0.4565 , ...,  -0.784  ,   3.316  ,
           2.72   ],
        [ -4.855  ,  -5.773  ,   7.945  , ...,   4.797  ,  -3.598  ,
          13.31   ],
        ...,
        [  1.791  ,   3.438  ,   1.273  , ...,  10.75   ,   0.08276,
           2.256  ],
        [  2.541  ,   0.667  ,  -4.727  , ...,   5.195  ,  -4.477  ,
           0.9155 ],
        [  3.137  ,   2.154  ,   1.079  , ..., -10.89   ,  -4.684  ,
           3.613  ]],

       [[ 10.99   ,  14.37   ,  11.38   , ...,   7.605  ,  -1.198  ,
           6.004  ],
        [ -4.04   , -12.05   ,  -4.105  , ..., -11.38   ,   2.617  ,
          -0.658  ],
        [-12.516  ,   1.343  ,   3.652  , ...,  -3.68   ,   0.4246 ,
          -8.56   ],
        ...,
        [  2.924  ,  -5.098  ,  -2.479  , ...,   1.536  ,  -0.6533 ,
          -0.4316 ],
        [ -0.1714 ,  -2.018  ,   7.96   , ...,   7.152  ,   5.832  ,
           5.68   ],
        [ -1.433  ,  11.32   ,   3.918  , ...,  -0.2898 ,  -2.117  ,
          -1.145  ]]], dtype=float16))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'verbose': True}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
&gt;           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=0.01, atol=0.01
E           
E           Mismatched elements: 1 / 8192 (0.0122%)
E           Max absolute difference: 0.04688
E           Max relative difference: 5.3
E            x: array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,
E                     -0.7812 ],
E                   [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,...
E            y: array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,
E                     -0.7803 ],
E                   [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,...

/usr/lib/python3.12/contextlib.py:81: AssertionError</failure></testcase><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[1-1]" time="0.216" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-1-32-32-32-8-16-float32-float32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[1-2]" time="0.007" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[1-3]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[2-0]" time="0.005" /><testcase classname="test.unit.language.test_pipeliner" name="test_pipeline_epilogue[2-1]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randint[400-42-int64-True]" time="0.042" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-42-int64-False]" time="0.040" /><testcase classname="test.unit.language.test_random" name="test_randint[400-124-int32-True]" time="0.039" /><testcase classname="test.unit.language.test_random" name="test_randint[400-124-int32-False]" time="0.038" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-64-128-False-True-True-e5m2-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw35' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-False-True-True-e5m2-e4m3-4-16-1]'&quot;">worker 'gw35' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-False-True-True-e5m2-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-124-int64-True]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-124-int64-False]" time="0.040" /><testcase classname="test.unit.language.test_random" name="test_randint[400-54-int32-True]" time="0.041" /><testcase classname="test.unit.language.test_random" name="test_randint[400-54-int32-False]" time="0.046" /><testcase classname="test.unit.language.test_random" name="test_randint[400-54-int64-True]" time="0.040" /><testcase classname="test.unit.language.test_random" name="test_randint[400-54-int64-False]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-4294967295-int32-True]" time="0.053" /><testcase classname="test.unit.language.test_random" name="test_randint[400-4294967295-int32-False]" time="0.049" /><testcase classname="test.unit.language.test_random" name="test_randint[400-4294967295-int64-True]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-4294967295-int64-False]" time="7.197" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-8-16-float16-float16]" time="0.013"><failure message="AssertionError: &#10;Not equal to tolerance rtol=0.01, atol=0.01&#10;&#10;Mismatched elements: 1 / 8192 (0.0122%)&#10;Max absolute difference: 0.04688&#10;Max relative difference: 5.3&#10; x: array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,&#10;          -0.7812 ],&#10;        [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,...&#10; y: array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,&#10;          -0.7803 ],&#10;        [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,...">B = 8, num_warps = 2, M = 32, N = 32, K = 32, BLOCK_M = 8, BLOCK_N = 16
in_dtype_str = 'float16', out_dtype_str = 'float16', device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str",
                             [(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str)
                              for B in [1, 2, 4, 8]
                              for num_warps in [1, 2, 4, 8, 16]
                              for BLOCK_M, BLOCK_N in [(32, 32)]
                              for M, N, K in [(64, 64, 64), (32, 32, 32)]
                              for in_dtype_str, out_dtype_str in [('int8', 'int8'), ('float16', 'float16'),
                                                                  ('float16', 'float32'), ('float32', 'float32')]] +
                             # Large block sizes
                             [(4, 4, 128, 128, 64, 64, 64, 'float16', 'float16')] +
                             # Small block sizes
                             [(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str)
                              for B in [1, 2, 8]
                              for num_warps in [1, 2, 4]
                              for BLOCK_M, BLOCK_N in [(1, 32), (32, 2), (8, 8), (8, 16)]
                              for M, N, K in [(32, 32, 32)]
                              for in_dtype_str, out_dtype_str in [('float16', 'float16'), ('float32', 'float32')]])
    def test_dot3d(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str, device):
        if is_hip():
            # hip does not support tf32 precision, so use ieee for all tests
            input_precision = "ieee"
            arch = triton.runtime.driver.active.get_current_target().arch
            if "gfx11" in arch or "gfx12" in arch:
                if in_dtype_str == "float32":
                    pytest.skip(f"{in_dtype_str} is not supported in WMMA dot, FMA does not support dot3d")
                if out_dtype_str == "float16":
                    pytest.skip(f"{out_dtype_str} has low precision in WMMA dot")
        else:
            input_precision = "tf32" if (is_cuda() or is_xpu()) and in_dtype_str == 'float32' else "ieee"
            if is_xpu():
                if (BLOCK_M &lt; 8 or BLOCK_N &lt; 16):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_interpreter() and (BLOCK_M &lt; 16 or BLOCK_N &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
    
        if B == 8 and M == 64 and in_dtype_str == "float32" and out_dtype_str == "float32":
            if not is_interpreter() and triton.runtime.driver.active.utils.get_device_properties(
                    triton.runtime.driver.active.get_current_device())["max_shared_mem"] &lt; 131072:
                pytest.skip(
                    "Skipping tests with B = 8, M = 64, in_type = float32, out_type = float32 due to insufficient shared memory (less than 128 KB per SM) on this GPU."
                )
    
        @triton.jit
        def kernel(
            q_ptr,
            k_ptr,
            o_ptr,
            stride_qb,
            stride_qm,
            stride_qk,
            stride_kb,
            stride_kk,
            stride_kn,
            stride_ob,
            stride_om,
            stride_on,
            BLOCK_B: tl.constexpr,
            BLOCK_M: tl.constexpr,
            BLOCK_N: tl.constexpr,
            BLOCK_K: tl.constexpr,
            INPUT_PRECISION: tl.constexpr,
            out_dtype: tl.constexpr = tl.float32,
        ):
            startm = tl.program_id(0) * BLOCK_M
            startn = tl.program_id(1) * BLOCK_N
            offs_b = tl.arange(0, BLOCK_B)
            offs_m = startm + tl.arange(0, BLOCK_M)
            offs_n = startn + tl.arange(0, BLOCK_N)
            offs_k = tl.arange(0, BLOCK_K)
            q_ptrs = q_ptr + offs_b[:, None, None] * stride_qb + offs_m[None, :, None] * stride_qm + offs_k[
                None, None, :] * stride_qk
            k_ptrs = k_ptr + offs_b[:, None, None] * stride_kb + offs_k[None, :, None] * stride_kk + offs_n[
                None, None, :] * stride_kn
            q = tl.load(q_ptrs)
            k = tl.load(k_ptrs)
            qk = tl.dot(q, k, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            o_ptrs = o_ptr + offs_b[:, None, None] * stride_ob + offs_m[None, :, None] * stride_om + offs_n[
                None, None, :] * stride_on
            tl.store(o_ptrs, qk)
    
        if out_dtype_str == 'int8':
            out_dtype = tl.int8
        elif out_dtype_str == 'float16':
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        rs = RandomState(17)
        x = numpy_random((B, M, K), dtype_str=in_dtype_str, rs=rs)
        y = numpy_random((B, K, N), dtype_str=in_dtype_str, rs=rs)
        if in_dtype_str == 'int8':
            out = numpy_random((B, M, N), dtype_str='int32', rs=rs)
        else:
            if is_hip() and (BLOCK_M &lt; 16 or BLOCK_N &lt; 16) and out_dtype_str == 'float16':
                # float16 accumulator in FMA dot loose precision too fast
                x *= 0.1
                y *= 0.1
            out = numpy_random((B, M, N), dtype_str=out_dtype_str, rs=rs)
    
        x_tri = to_triton(x, device=device)
        y_tri = to_triton(y, device=device)
        out_tri = to_triton(out, device=device)
    
        BLOCK_B = B
        BLOCK_K = K
    
        grid = (
            triton.cdiv(M, BLOCK_M),
            triton.cdiv(N, BLOCK_N),
        )
        kernel[grid](
            x_tri,
            y_tri,
            out_tri,
            x_tri.stride(0),
            x_tri.stride(1),
            x_tri.stride(2),
            y_tri.stride(0),
            y_tri.stride(1),
            y_tri.stride(2),
            out_tri.stride(0),
            out_tri.stride(1),
            out_tri.stride(2),
            BLOCK_B=BLOCK_B,
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
            BLOCK_K=BLOCK_K,
            INPUT_PRECISION=input_precision,
            out_dtype=out_dtype,
            num_warps=num_warps,
        )
    
        if in_dtype_str == 'int8':
            out_ref = np.matmul(x.astype(np.float32), y.astype(np.float32)).astype(np.int32)
        else:
            out_ref = np.matmul(x, y)
&gt;       np.testing.assert_allclose(out_ref, to_numpy(out_tri), rtol=0.01, atol=1e-2)

language/test_core.py:3988: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (&lt;function assert_allclose.&lt;locals&gt;.compare at 0x72ba5a799440&gt;, array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,
          -0.7812 ],
        [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,
          -1.988  ],
        [  5.625  ,  -7.008  ,   0.3352 , ...,  -9.94   ,  -4.54   ,
          -3.932  ],
        ...,
        [  3.283  ,  -4.203  ,   6.477  , ...,  -5.062  ,   8.56   ,
           1.532  ],
        [ -3.682  ,  -0.4055 ,  -9.195  , ...,   3.326  ,  -5.33   ,
           4.664  ],
        [  0.2546 ,  -0.05045,   3.504  , ...,   0.989  ,   3.068  ,
         -10.04   ]],

       [[  0.893  ,   9.664  ,   3.02   , ...,   1.995  ,  -3.467  ,
           4.867  ],
        [  1.944  ,  -1.934  ,   2.781  , ...,  -4.133  ,   1.016  ,
           2.2    ],
        [ -7.508  ,  -5.14   ,   0.98   , ..., -10.375  ,   0.0855 ,
          -6.4    ],
        ...,
        [  6.57   ,   6.62   ,  -9.47   , ...,  -5.375  ,  -0.38   ,
          -0.5933 ],
        [ -2.744  ,   3.22   , -11.01   , ...,  -9.25   ,   8.85   ,
           0.6387 ],
        [ -9.31   , -13.49   ,   1.81   , ...,   3.744  ,   5.258  ,
           3.582  ]],

       [[ 15.086  ,  -4.258  ,  -1.312  , ...,  -5.254  , -10.11   ,
           1.088  ],
        [  1.731  ,  -7.285  ,  -3.436  , ...,   1.447  ,  -0.61   ,
          15.164  ],
        [  2.785  ,   2.49   ,  10.13   , ...,  -3.564  ,  -4.14   ,
          -1.098  ],
        ...,
        [  9.914  ,  -0.965  ,  -1.426  , ...,   0.9814 , -10.     ,
           2.895  ],
        [  2.209  , -10.28   ,  -5.867  , ..., -21.84   ,   1.362  ,
           8.555  ],
        [  6.906  ,  -2.012  ,   1.046  , ...,   1.676  ,   7.06   ,
           8.08   ]],

       ...,

       [[ -3.225  ,  -4.36   ,  -7.312  , ...,   2.512  ,   3.041  ,
          -8.29   ],
        [ -7.21   ,  -2.969  ,  -0.5684 , ...,   2.182  ,  -6.477  ,
          -2.574  ],
        [ -3.236  ,   0.04898,   2.79   , ...,   1.393  ,   3.607  ,
          11.5    ],
        ...,
        [ -3.938  ,  12.8    ,  -3.084  , ...,  -4.25   ,   0.1508 ,
          -6.47   ],
        [  2.732  ,  -3.33   ,  -9.5    , ...,   0.4858 ,   9.73   ,
           4.457  ],
        [ -0.158  ,   1.752  ,  -1.329  , ...,   0.6816 ,  -9.05   ,
          -8.7    ]],

       [[ -3.566  ,   5.094  ,  -6.645  , ...,   5.066  ,   7.605  ,
           5.242  ],
        [  5.676  ,   7.887  ,  -0.459  , ...,  -0.7827 ,   3.318  ,
           2.72   ],
        [ -4.86   ,  -5.766  ,   7.95   , ...,   4.793  ,  -3.594  ,
          13.31   ],
        ...,
        [  1.794  ,   3.436  ,   1.274  , ...,  10.74   ,   0.0841 ,
           2.248  ],
        [  2.535  ,   0.6626 ,  -4.727  , ...,   5.195  ,  -4.477  ,
           0.9097 ],
        [  3.139  ,   2.152  ,   1.08   , ..., -10.87   ,  -4.68   ,
           3.613  ]],

       [[ 11.     ,  14.37   ,  11.38   , ...,   7.61   ,  -1.203  ,
           6.008  ],
        [ -4.04   , -12.05   ,  -4.1    , ..., -11.38   ,   2.613  ,
          -0.661  ],
        [-12.51   ,   1.341  ,   3.652  , ...,  -3.684  ,   0.421  ,
          -8.555  ],
        ...,
        [  2.924  ,  -5.1    ,  -2.48   , ...,   1.538  ,  -0.6533 ,
          -0.4302 ],
        [ -0.1746 ,  -2.014  ,   7.96   , ...,   7.16   ,   5.83   ,
           5.684  ],
        [ -1.436  ,  11.32   ,   3.916  , ...,  -0.2876 ,  -2.12   ,
          -1.1455 ]]], dtype=float16), array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,
          -0.7803 ],
        [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,
          -1.987  ],
        [  5.62   ,  -7.008  ,   0.3328 , ...,  -9.945  ,  -4.54   ,
          -3.934  ],
        ...,
        [  3.283  ,  -4.203  ,   6.48   , ...,  -5.07   ,   8.555  ,
           1.53   ],
        [ -3.684  ,  -0.4045 ,  -9.2    , ...,   3.324  ,  -5.324  ,
           4.66   ],
        [  0.253  ,  -0.04852,   3.502  , ...,   0.988  ,   3.068  ,
         -10.04   ]],

       [[  0.893  ,   9.67   ,   3.02   , ...,   1.994  ,  -3.463  ,
           4.863  ],
        [  1.945  ,  -1.933  ,   2.777  , ...,  -4.137  ,   1.014  ,
           2.201  ],
        [ -7.508  ,  -5.137  ,   0.9775 , ..., -10.37   ,   0.0782 ,
          -6.402  ],
        ...,
        [  6.566  ,   6.613  ,  -9.46   , ...,  -5.37   ,  -0.3806 ,
          -0.593  ],
        [ -2.748  ,   3.223  , -11.     , ...,  -9.25   ,   8.85   ,
           0.636  ],
        [ -9.31   , -13.484  ,   1.81   , ...,   3.742  ,   5.254  ,
           3.58   ]],

       [[ 15.086  ,  -4.25   ,  -1.31   , ...,  -5.258  , -10.11   ,
           1.086  ],
        [  1.734  ,  -7.293  ,  -3.438  , ...,   1.445  ,  -0.6064 ,
          15.164  ],
        [  2.787  ,   2.488  ,  10.15   , ...,  -3.566  ,  -4.145  ,
          -1.099  ],
        ...,
        [  9.914  ,  -0.968  ,  -1.424  , ...,   0.978  , -10.     ,
           2.896  ],
        [  2.21   , -10.27   ,  -5.87   , ..., -21.8    ,   1.364  ,
           8.555  ],
        [  6.906  ,  -2.01   ,   1.047  , ...,   1.675  ,   7.062  ,
           8.06   ]],

       ...,

       [[ -3.23   ,  -4.363  ,  -7.31   , ...,   2.51   ,   3.041  ,
          -8.3    ],
        [ -7.207  ,  -2.965  ,  -0.5684 , ...,   2.182  ,  -6.473  ,
          -2.574  ],
        [ -3.236  ,   0.04822,   2.799  , ...,   1.394  ,   3.607  ,
          11.53   ],
        ...,
        [ -3.941  ,  12.79   ,  -3.082  , ...,  -4.25   ,   0.1515 ,
          -6.47   ],
        [  2.732  ,  -3.334  ,  -9.516  , ...,   0.4878 ,   9.734  ,
           4.453  ],
        [ -0.159  ,   1.751  ,  -1.329  , ...,   0.6797 ,  -9.04   ,
          -8.7    ]],

       [[ -3.568  ,   5.098  ,  -6.65   , ...,   5.07   ,   7.61   ,
           5.246  ],
        [  5.676  ,   7.89   ,  -0.4565 , ...,  -0.784  ,   3.316  ,
           2.72   ],
        [ -4.855  ,  -5.773  ,   7.945  , ...,   4.797  ,  -3.598  ,
          13.31   ],
        ...,
        [  1.791  ,   3.438  ,   1.273  , ...,  10.75   ,   0.08276,
           2.256  ],
        [  2.541  ,   0.667  ,  -4.727  , ...,   5.195  ,  -4.477  ,
           0.9155 ],
        [  3.137  ,   2.154  ,   1.079  , ..., -10.89   ,  -4.684  ,
           3.613  ]],

       [[ 10.99   ,  14.37   ,  11.38   , ...,   7.605  ,  -1.198  ,
           6.004  ],
        [ -4.04   , -12.05   ,  -4.105  , ..., -11.38   ,   2.617  ,
          -0.658  ],
        [-12.516  ,   1.343  ,   3.652  , ...,  -3.68   ,   0.4246 ,
          -8.56   ],
        ...,
        [  2.924  ,  -5.098  ,  -2.479  , ...,   1.536  ,  -0.6533 ,
          -0.4316 ],
        [ -0.1714 ,  -2.018  ,   7.96   , ...,   7.152  ,   5.832  ,
           5.68   ],
        [ -1.433  ,  11.32   ,   3.918  , ...,  -0.2898 ,  -2.117  ,
          -1.145  ]]], dtype=float16))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'verbose': True}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
&gt;           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=0.01, atol=0.01
E           
E           Mismatched elements: 1 / 8192 (0.0122%)
E           Max absolute difference: 0.04688
E           Max relative difference: 5.3
E            x: array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,
E                     -0.7812 ],
E                   [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,...
E            y: array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,
E                     -0.7803 ],
E                   [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,...

/usr/lib/python3.12/contextlib.py:81: AssertionError</failure></testcase><testcase classname="test.unit.language.test_standard" name="test_sort[float32-True-512-8]" time="7.309" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-2-32-32-32-8-16-float32-float32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-1-32-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-float16-float32-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-float32-float32-1-None0]" time="3.275" /><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-1-32-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-32-2-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-32-2-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-8-8-float16-float16]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-8-8-float32-float32]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-8-16-float16-float16]" time="0.013"><failure message="AssertionError: &#10;Not equal to tolerance rtol=0.01, atol=0.01&#10;&#10;Mismatched elements: 1 / 8192 (0.0122%)&#10;Max absolute difference: 0.04688&#10;Max relative difference: 5.3&#10; x: array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,&#10;          -0.7812 ],&#10;        [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,...&#10; y: array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,&#10;          -0.7803 ],&#10;        [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,...">B = 8, num_warps = 4, M = 32, N = 32, K = 32, BLOCK_M = 8, BLOCK_N = 16
in_dtype_str = 'float16', out_dtype_str = 'float16', device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str",
                             [(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str)
                              for B in [1, 2, 4, 8]
                              for num_warps in [1, 2, 4, 8, 16]
                              for BLOCK_M, BLOCK_N in [(32, 32)]
                              for M, N, K in [(64, 64, 64), (32, 32, 32)]
                              for in_dtype_str, out_dtype_str in [('int8', 'int8'), ('float16', 'float16'),
                                                                  ('float16', 'float32'), ('float32', 'float32')]] +
                             # Large block sizes
                             [(4, 4, 128, 128, 64, 64, 64, 'float16', 'float16')] +
                             # Small block sizes
                             [(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str)
                              for B in [1, 2, 8]
                              for num_warps in [1, 2, 4]
                              for BLOCK_M, BLOCK_N in [(1, 32), (32, 2), (8, 8), (8, 16)]
                              for M, N, K in [(32, 32, 32)]
                              for in_dtype_str, out_dtype_str in [('float16', 'float16'), ('float32', 'float32')]])
    def test_dot3d(B, num_warps, M, N, K, BLOCK_M, BLOCK_N, in_dtype_str, out_dtype_str, device):
        if is_hip():
            # hip does not support tf32 precision, so use ieee for all tests
            input_precision = "ieee"
            arch = triton.runtime.driver.active.get_current_target().arch
            if "gfx11" in arch or "gfx12" in arch:
                if in_dtype_str == "float32":
                    pytest.skip(f"{in_dtype_str} is not supported in WMMA dot, FMA does not support dot3d")
                if out_dtype_str == "float16":
                    pytest.skip(f"{out_dtype_str} has low precision in WMMA dot")
        else:
            input_precision = "tf32" if (is_cuda() or is_xpu()) and in_dtype_str == 'float32' else "ieee"
            if is_xpu():
                if (BLOCK_M &lt; 8 or BLOCK_N &lt; 16):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_interpreter() and (BLOCK_M &lt; 16 or BLOCK_N &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
    
        if B == 8 and M == 64 and in_dtype_str == "float32" and out_dtype_str == "float32":
            if not is_interpreter() and triton.runtime.driver.active.utils.get_device_properties(
                    triton.runtime.driver.active.get_current_device())["max_shared_mem"] &lt; 131072:
                pytest.skip(
                    "Skipping tests with B = 8, M = 64, in_type = float32, out_type = float32 due to insufficient shared memory (less than 128 KB per SM) on this GPU."
                )
    
        @triton.jit
        def kernel(
            q_ptr,
            k_ptr,
            o_ptr,
            stride_qb,
            stride_qm,
            stride_qk,
            stride_kb,
            stride_kk,
            stride_kn,
            stride_ob,
            stride_om,
            stride_on,
            BLOCK_B: tl.constexpr,
            BLOCK_M: tl.constexpr,
            BLOCK_N: tl.constexpr,
            BLOCK_K: tl.constexpr,
            INPUT_PRECISION: tl.constexpr,
            out_dtype: tl.constexpr = tl.float32,
        ):
            startm = tl.program_id(0) * BLOCK_M
            startn = tl.program_id(1) * BLOCK_N
            offs_b = tl.arange(0, BLOCK_B)
            offs_m = startm + tl.arange(0, BLOCK_M)
            offs_n = startn + tl.arange(0, BLOCK_N)
            offs_k = tl.arange(0, BLOCK_K)
            q_ptrs = q_ptr + offs_b[:, None, None] * stride_qb + offs_m[None, :, None] * stride_qm + offs_k[
                None, None, :] * stride_qk
            k_ptrs = k_ptr + offs_b[:, None, None] * stride_kb + offs_k[None, :, None] * stride_kk + offs_n[
                None, None, :] * stride_kn
            q = tl.load(q_ptrs)
            k = tl.load(k_ptrs)
            qk = tl.dot(q, k, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            o_ptrs = o_ptr + offs_b[:, None, None] * stride_ob + offs_m[None, :, None] * stride_om + offs_n[
                None, None, :] * stride_on
            tl.store(o_ptrs, qk)
    
        if out_dtype_str == 'int8':
            out_dtype = tl.int8
        elif out_dtype_str == 'float16':
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        rs = RandomState(17)
        x = numpy_random((B, M, K), dtype_str=in_dtype_str, rs=rs)
        y = numpy_random((B, K, N), dtype_str=in_dtype_str, rs=rs)
        if in_dtype_str == 'int8':
            out = numpy_random((B, M, N), dtype_str='int32', rs=rs)
        else:
            if is_hip() and (BLOCK_M &lt; 16 or BLOCK_N &lt; 16) and out_dtype_str == 'float16':
                # float16 accumulator in FMA dot loose precision too fast
                x *= 0.1
                y *= 0.1
            out = numpy_random((B, M, N), dtype_str=out_dtype_str, rs=rs)
    
        x_tri = to_triton(x, device=device)
        y_tri = to_triton(y, device=device)
        out_tri = to_triton(out, device=device)
    
        BLOCK_B = B
        BLOCK_K = K
    
        grid = (
            triton.cdiv(M, BLOCK_M),
            triton.cdiv(N, BLOCK_N),
        )
        kernel[grid](
            x_tri,
            y_tri,
            out_tri,
            x_tri.stride(0),
            x_tri.stride(1),
            x_tri.stride(2),
            y_tri.stride(0),
            y_tri.stride(1),
            y_tri.stride(2),
            out_tri.stride(0),
            out_tri.stride(1),
            out_tri.stride(2),
            BLOCK_B=BLOCK_B,
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
            BLOCK_K=BLOCK_K,
            INPUT_PRECISION=input_precision,
            out_dtype=out_dtype,
            num_warps=num_warps,
        )
    
        if in_dtype_str == 'int8':
            out_ref = np.matmul(x.astype(np.float32), y.astype(np.float32)).astype(np.int32)
        else:
            out_ref = np.matmul(x, y)
&gt;       np.testing.assert_allclose(out_ref, to_numpy(out_tri), rtol=0.01, atol=1e-2)

language/test_core.py:3988: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (&lt;function assert_allclose.&lt;locals&gt;.compare at 0x72ba52fbe980&gt;, array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,
          -0.7812 ],
        [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,
          -1.988  ],
        [  5.625  ,  -7.008  ,   0.3352 , ...,  -9.94   ,  -4.54   ,
          -3.932  ],
        ...,
        [  3.283  ,  -4.203  ,   6.477  , ...,  -5.062  ,   8.56   ,
           1.532  ],
        [ -3.682  ,  -0.4055 ,  -9.195  , ...,   3.326  ,  -5.33   ,
           4.664  ],
        [  0.2546 ,  -0.05045,   3.504  , ...,   0.989  ,   3.068  ,
         -10.04   ]],

       [[  0.893  ,   9.664  ,   3.02   , ...,   1.995  ,  -3.467  ,
           4.867  ],
        [  1.944  ,  -1.934  ,   2.781  , ...,  -4.133  ,   1.016  ,
           2.2    ],
        [ -7.508  ,  -5.14   ,   0.98   , ..., -10.375  ,   0.0855 ,
          -6.4    ],
        ...,
        [  6.57   ,   6.62   ,  -9.47   , ...,  -5.375  ,  -0.38   ,
          -0.5933 ],
        [ -2.744  ,   3.22   , -11.01   , ...,  -9.25   ,   8.85   ,
           0.6387 ],
        [ -9.31   , -13.49   ,   1.81   , ...,   3.744  ,   5.258  ,
           3.582  ]],

       [[ 15.086  ,  -4.258  ,  -1.312  , ...,  -5.254  , -10.11   ,
           1.088  ],
        [  1.731  ,  -7.285  ,  -3.436  , ...,   1.447  ,  -0.61   ,
          15.164  ],
        [  2.785  ,   2.49   ,  10.13   , ...,  -3.564  ,  -4.14   ,
          -1.098  ],
        ...,
        [  9.914  ,  -0.965  ,  -1.426  , ...,   0.9814 , -10.     ,
           2.895  ],
        [  2.209  , -10.28   ,  -5.867  , ..., -21.84   ,   1.362  ,
           8.555  ],
        [  6.906  ,  -2.012  ,   1.046  , ...,   1.676  ,   7.06   ,
           8.08   ]],

       ...,

       [[ -3.225  ,  -4.36   ,  -7.312  , ...,   2.512  ,   3.041  ,
          -8.29   ],
        [ -7.21   ,  -2.969  ,  -0.5684 , ...,   2.182  ,  -6.477  ,
          -2.574  ],
        [ -3.236  ,   0.04898,   2.79   , ...,   1.393  ,   3.607  ,
          11.5    ],
        ...,
        [ -3.938  ,  12.8    ,  -3.084  , ...,  -4.25   ,   0.1508 ,
          -6.47   ],
        [  2.732  ,  -3.33   ,  -9.5    , ...,   0.4858 ,   9.73   ,
           4.457  ],
        [ -0.158  ,   1.752  ,  -1.329  , ...,   0.6816 ,  -9.05   ,
          -8.7    ]],

       [[ -3.566  ,   5.094  ,  -6.645  , ...,   5.066  ,   7.605  ,
           5.242  ],
        [  5.676  ,   7.887  ,  -0.459  , ...,  -0.7827 ,   3.318  ,
           2.72   ],
        [ -4.86   ,  -5.766  ,   7.95   , ...,   4.793  ,  -3.594  ,
          13.31   ],
        ...,
        [  1.794  ,   3.436  ,   1.274  , ...,  10.74   ,   0.0841 ,
           2.248  ],
        [  2.535  ,   0.6626 ,  -4.727  , ...,   5.195  ,  -4.477  ,
           0.9097 ],
        [  3.139  ,   2.152  ,   1.08   , ..., -10.87   ,  -4.68   ,
           3.613  ]],

       [[ 11.     ,  14.37   ,  11.38   , ...,   7.61   ,  -1.203  ,
           6.008  ],
        [ -4.04   , -12.05   ,  -4.1    , ..., -11.38   ,   2.613  ,
          -0.661  ],
        [-12.51   ,   1.341  ,   3.652  , ...,  -3.684  ,   0.421  ,
          -8.555  ],
        ...,
        [  2.924  ,  -5.1    ,  -2.48   , ...,   1.538  ,  -0.6533 ,
          -0.4302 ],
        [ -0.1746 ,  -2.014  ,   7.96   , ...,   7.16   ,   5.83   ,
           5.684  ],
        [ -1.436  ,  11.32   ,   3.916  , ...,  -0.2876 ,  -2.12   ,
          -1.1455 ]]], dtype=float16), array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,
          -0.7803 ],
        [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,
          -1.987  ],
        [  5.62   ,  -7.008  ,   0.3328 , ...,  -9.945  ,  -4.54   ,
          -3.934  ],
        ...,
        [  3.283  ,  -4.203  ,   6.48   , ...,  -5.07   ,   8.555  ,
           1.53   ],
        [ -3.684  ,  -0.4045 ,  -9.2    , ...,   3.324  ,  -5.324  ,
           4.66   ],
        [  0.253  ,  -0.04852,   3.502  , ...,   0.988  ,   3.068  ,
         -10.04   ]],

       [[  0.893  ,   9.67   ,   3.02   , ...,   1.994  ,  -3.463  ,
           4.863  ],
        [  1.945  ,  -1.933  ,   2.777  , ...,  -4.137  ,   1.014  ,
           2.201  ],
        [ -7.508  ,  -5.137  ,   0.9775 , ..., -10.37   ,   0.0782 ,
          -6.402  ],
        ...,
        [  6.566  ,   6.613  ,  -9.46   , ...,  -5.37   ,  -0.3806 ,
          -0.593  ],
        [ -2.748  ,   3.223  , -11.     , ...,  -9.25   ,   8.85   ,
           0.636  ],
        [ -9.31   , -13.484  ,   1.81   , ...,   3.742  ,   5.254  ,
           3.58   ]],

       [[ 15.086  ,  -4.25   ,  -1.31   , ...,  -5.258  , -10.11   ,
           1.086  ],
        [  1.734  ,  -7.293  ,  -3.438  , ...,   1.445  ,  -0.6064 ,
          15.164  ],
        [  2.787  ,   2.488  ,  10.15   , ...,  -3.566  ,  -4.145  ,
          -1.099  ],
        ...,
        [  9.914  ,  -0.968  ,  -1.424  , ...,   0.978  , -10.     ,
           2.896  ],
        [  2.21   , -10.27   ,  -5.87   , ..., -21.8    ,   1.364  ,
           8.555  ],
        [  6.906  ,  -2.01   ,   1.047  , ...,   1.675  ,   7.062  ,
           8.06   ]],

       ...,

       [[ -3.23   ,  -4.363  ,  -7.31   , ...,   2.51   ,   3.041  ,
          -8.3    ],
        [ -7.207  ,  -2.965  ,  -0.5684 , ...,   2.182  ,  -6.473  ,
          -2.574  ],
        [ -3.236  ,   0.04822,   2.799  , ...,   1.394  ,   3.607  ,
          11.53   ],
        ...,
        [ -3.941  ,  12.79   ,  -3.082  , ...,  -4.25   ,   0.1515 ,
          -6.47   ],
        [  2.732  ,  -3.334  ,  -9.516  , ...,   0.4878 ,   9.734  ,
           4.453  ],
        [ -0.159  ,   1.751  ,  -1.329  , ...,   0.6797 ,  -9.04   ,
          -8.7    ]],

       [[ -3.568  ,   5.098  ,  -6.65   , ...,   5.07   ,   7.61   ,
           5.246  ],
        [  5.676  ,   7.89   ,  -0.4565 , ...,  -0.784  ,   3.316  ,
           2.72   ],
        [ -4.855  ,  -5.773  ,   7.945  , ...,   4.797  ,  -3.598  ,
          13.31   ],
        ...,
        [  1.791  ,   3.438  ,   1.273  , ...,  10.75   ,   0.08276,
           2.256  ],
        [  2.541  ,   0.667  ,  -4.727  , ...,   5.195  ,  -4.477  ,
           0.9155 ],
        [  3.137  ,   2.154  ,   1.079  , ..., -10.89   ,  -4.684  ,
           3.613  ]],

       [[ 10.99   ,  14.37   ,  11.38   , ...,   7.605  ,  -1.198  ,
           6.004  ],
        [ -4.04   , -12.05   ,  -4.105  , ..., -11.38   ,   2.617  ,
          -0.658  ],
        [-12.516  ,   1.343  ,   3.652  , ...,  -3.68   ,   0.4246 ,
          -8.56   ],
        ...,
        [  2.924  ,  -5.098  ,  -2.479  , ...,   1.536  ,  -0.6533 ,
          -0.4316 ],
        [ -0.1714 ,  -2.018  ,   7.96   , ...,   7.152  ,   5.832  ,
           5.68   ],
        [ -1.433  ,  11.32   ,   3.918  , ...,  -0.2898 ,  -2.117  ,
          -1.145  ]]], dtype=float16))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'verbose': True}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
&gt;           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=0.01, atol=0.01
E           
E           Mismatched elements: 1 / 8192 (0.0122%)
E           Max absolute difference: 0.04688
E           Max relative difference: 5.3
E            x: array([[[  3.875  ,  -2.373  ,  -4.844  , ...,  -6.58   ,   1.685  ,
E                     -0.7812 ],
E                   [ -2.953  ,   5.11   ,  -0.6025 , ...,  -3.53   ,  -3.87   ,...
E            y: array([[[  3.875  ,  -2.387  ,  -4.848  , ...,  -6.57   ,   1.686  ,
E                     -0.7803 ],
E                   [ -2.955  ,   5.105  ,  -0.6025 , ...,  -3.533  ,  -3.88   ,...

/usr/lib/python3.12/contextlib.py:81: AssertionError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot3d[8-4-32-32-32-8-16-float32-float32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot_mulbroadcasted[float32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-int8]" time="0.169" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-int16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-int32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-int64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-uint8]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-uint16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-uint32]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-uint64]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-float16]" time="0.114" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-float32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_full[shape0-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_full[shape0-bfloat16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-int8]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-int16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-int32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-int64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-uint8]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-uint16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-uint32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-uint64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-float16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-float32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape1-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_full[shape1-bfloat16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-int8]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-int16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-int32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-int64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-uint8]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-uint16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-uint32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-uint64]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-float16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-float32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_full[shape2-float64]" time="0.001"><skipped type="pytest.xfail" message="float64 not supported on current xpu hardware" /></testcase><testcase classname="test.unit.language.test_core" name="test_full[shape2-bfloat16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_constexpr[1e+50-f64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr[10000000000.0-f32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr[1.0-f32]" time="0.005" /><testcase classname="test.unit.language.test_core" name="test_constexpr[float(&quot;inf&quot;)-f32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr[float(&quot;-inf&quot;)-f32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr[float(&quot;nan&quot;)-f32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr[float(&quot;-nan&quot;)-f32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr[0.0-f32]" time="0.003" /><testcase classname="test.unit.language.test_core" name="test_constexpr[5-i32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_constexpr[1099511627776-i64]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_const[direct-True-True]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_const[direct-True-False]" time="0.006" /><testcase classname="test.unit.language.test_core" name="test_const[direct-False-True]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-64-False-False-True-e5m2-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-128-True-True-False-e2m1-e4m3-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw33' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-128-True-True-False-e2m1-e4m3-4-16-1]'&quot;">worker 'gw33' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-128-True-True-False-e2m1-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-4-False-False-none-tf32-float32-float32-1-None1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-int8-int8-1-None0]" time="45.326"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865ec080&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-32-64-True-True-False-e2m1-e5m2-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw36' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-64-True-True-False-e2m1-e5m2-4-16-1]'&quot;">worker 'gw36' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-64-True-True-False-e2m1-e5m2-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_random" name="test_randint[400-67830198458-int32-True]" time="0.039" /><testcase classname="test.unit.language.test_random" name="test_randint[400-67830198458-int32-False]" time="0.039" /><testcase classname="test.unit.language.test_random" name="test_randint[400-67830198458-int64-True]" time="0.040" /><testcase classname="test.unit.language.test_random" name="test_randint[400-67830198458-int64-False]" time="0.039" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-0-int32-True]" time="0.936" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-False-1-512]" time="0.006" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-False-8-64]" time="0.006" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-False-256-16]" time="0.008" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-False-512-8]" time="0.011" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-True-1-512]" time="0.006" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-True-8-64]" time="0.005" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-True-256-16]" time="0.009" /><testcase classname="test.unit.language.test_standard" name="test_sort[bfloat16-True-512-8]" time="0.011" /><testcase classname="test.unit.language.test_standard" name="test_flip[int32-1-512]" time="0.295" /><testcase classname="test.unit.language.test_standard" name="test_flip[int32-8-64]" time="1.615" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-64-True-False-True-e5m2-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-128-64-False-True-False-e2m1-e4m3-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw37' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-True-False-e2m1-e4m3-4-16-1]'&quot;">worker 'gw37' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-128-64-False-True-False-e2m1-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_random" name="test_rand[100000-0-int32-False]" time="1.255" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-0-int64-True]" time="0.902" /><testcase classname="test.unit.language.test_standard" name="test_flip[int32-256-16]" time="8.482" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-0-int64-False]" time="7.850" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-32-128-True-True-True-e4m3-e4m3-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw38' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-128-True-True-True-e4m3-e4m3-4-16-1]'&quot;">worker 'gw38' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-32-128-True-True-True-e4m3-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[64-64-64-True-False-False-e2m1-bf16-4-16-1]" time="0.001"><error message="failed on setup with &quot;worker 'gw39' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-64-True-False-False-e2m1-bf16-4-16-1]'&quot;">worker 'gw39' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[64-64-64-True-False-False-e2m1-bf16-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_standard" name="test_flip[int32-512-8]" time="9.793" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-42-int32-True]" time="0.988" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-42-int32-False]" time="0.954" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-42-int64-True]" time="0.855" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-42-int64-False]" time="0.862" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-124-int32-True]" time="0.883" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-124-int32-False]" time="0.895" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-124-int64-True]" time="0.862" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-124-int64-False]" time="0.875" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-54-int32-True]" time="0.894" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-54-int32-False]" time="0.900" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-54-int64-True]" time="0.898" /><testcase classname="test.unit.language.test_standard" name="test_flip[float16-1-512]" time="0.284" /><testcase classname="test.unit.language.test_random" name="test_rand[100000-54-int64-False]" time="0.864" /><testcase classname="test.unit.language.test_standard" name="test_flip[float16-8-64]" time="1.557" /><testcase classname="test.unit.language.test_random" name="test_seed_is_int" time="0.022" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-0-int32-True]" time="0.031" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-0-int32-False]" time="1.404" /><testcase classname="test.unit.language.test_standard" name="test_flip[float16-256-16]" time="7.997" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-0-int64-True]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-0-int64-False]" time="9.283" /><testcase classname="test.unit.language.test_standard" name="test_flip[float16-512-8]" time="9.620" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-int8-int8-1-None1]" time="140.500"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778bd8b2e870&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_random" name="test_randn[100000-42-int32-True]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-42-int32-False]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-42-int64-True]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-42-int64-False]" time="0.007" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-124-int32-True]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-124-int32-False]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-124-int64-True]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-124-int64-False]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-54-int32-True]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-54-int32-False]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-54-int64-True]" time="0.005" /><testcase classname="test.unit.language.test_random" name="test_randn[100000-54-int64-False]" time="0.006" /><testcase classname="test.unit.language.test_random" name="test_rand_limits[int32]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-64-128-True-True-False-e4m3-e4m3-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw34' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-True-True-False-e4m3-e4m3-4-16-1]'&quot;">worker 'gw34' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-64-128-True-True-False-e4m3-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-int8-int8-1-None1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-float16-float16-1-None0]" time="8.892" /><testcase classname="test.unit.language.test_standard" name="test_flip[float32-1-512]" time="0.272" /><testcase classname="test.unit.language.test_standard" name="test_flip[float32-8-64]" time="1.539" /><testcase classname="test.unit.language.test_standard" name="test_flip[float32-256-16]" time="8.150" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-int8-int8-1-None1]" time="44.123"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba86534da0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-float16-float16-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-float16-float32-1-None0]" time="17.743" /><testcase classname="test.unit.language.test_standard" name="test_flip[float32-512-8]" time="9.799" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-float16-float16-1-None0]" time="12.972"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea5610a0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_standard" name="test_flip[bfloat16-1-512]" time="0.006" /><testcase classname="test.unit.language.test_standard" name="test_flip[bfloat16-8-64]" time="0.006" /><testcase classname="test.unit.language.test_standard" name="test_flip[bfloat16-256-16]" time="0.005" /><testcase classname="test.unit.language.test_standard" name="test_flip[bfloat16-512-8]" time="0.006" /><testcase classname="test.unit.language.test_standard" name="test_flip_inf" time="0.615" /><testcase classname="test.unit.language.test_standard" name="test_swizzle2d[5-7-3]" time="5.092" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-float16-float32-1-None1]" time="0.022" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-float32-float32-1-None0]" time="21.957" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-float16-float16-1-None1]" time="13.033"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d036b1b80e0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_tuple" name="test_index[0]" time="4.955" /><testcase classname="test.unit.language.test_tuple" name="test_index[1]" time="5.037" /><testcase classname="test.unit.language.test_tuple" name="test_index[2]" time="0.004" /><testcase classname="test.unit.language.test_tuple" name="test_index[3]" time="0.004" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout0-16-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout1-128-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout1-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout1-64-64]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout1-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout1-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout1-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout2-128-16]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout2-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout2-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout2-32-128]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout2-32-32]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout2-16-16]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout3-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout3-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout3-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout3-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout3-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout4-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout4-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout4-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout4-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout4-16-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout5-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout5-128-128]" time="0.018" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout5-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout5-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout5-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout5-16-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout6-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout6-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout6-64-64]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout6-32-128]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout6-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout6-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout7-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout7-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout7-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout7-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout7-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout7-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout8-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout8-128-128]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout8-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout8-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout8-32-32]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-0-src_layout8-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout0-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout0-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout0-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout0-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout0-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout0-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout1-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout1-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout1-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout1-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout1-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout1-16-16]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout2-128-16]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout2-128-128]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout2-64-64]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout2-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout2-32-32]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout2-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout3-128-16]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout3-128-128]" time="0.017" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout3-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout3-32-128]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout3-32-32]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout3-16-16]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout4-128-16]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout4-128-128]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout4-64-64]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout4-32-128]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_reduce_layouts[sum-int32-True-reduce2d-1-src_layout4-32-32]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-float16-float32-1-None0]" time="23.550"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e8772a50&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-True-False-none-tf32-float32-float32-1-None1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-int8-int8-1-None0]" time="40.363" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-float16-float16-1-None0]" time="0.040" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-float16-float16-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-float16-float32-1-None0]" time="13.230"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba9192c860&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-float16-float32-1-None1]" time="13.089"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8d36e120&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-float16-float32-1-None1]" time="23.510"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e87be0f0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-float32-float32-1-None0]" time="13.456"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba919a8590&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-int8-int8-1-None1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-float16-float16-1-None0]" time="5.034" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-float32-float32-1-None0]" time="22.866"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02f098bf80&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-True-none-tf32-float32-float32-1-None1]" time="13.478"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8653af60&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-float16-float16-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-float16-float32-1-None0]" time="14.196" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-int8-int8-1-None0]" time="46.815"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8d3fd820&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-float16-float32-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-float32-float32-1-None0]" time="17.085" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-True-none-tf32-float32-float32-1-None1]" time="22.467"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d0364a8ee40&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-True-none-tf32-float32-float32-1-None1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-int8-int8-1-None0]" time="56.515" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-int8-int8-1-None0]" time="84.018"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e41aeb40&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-float16-float16-1-None0]" time="52.861"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b64348290&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-int8-int8-1-None1]" time="47.105"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba919b5160&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-int8-int8-1-None1]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-float16-float16-1-None0]" time="4.543" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-float16-float16-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-float16-float32-1-None0]" time="12.616" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-float16-float16-1-None1]" time="53.204"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62f22a80&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-float16-float32-1-None1]" time="0.035" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-float32-float32-1-None0]" time="18.098" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-float16-float16-1-None0]" time="40.208" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-256-32-8-False-False-none-tf32-float32-float32-1-None1]" time="0.015" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-int8-int8-1-None0]" time="3.232" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-int8-int8-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-float16-float16-1-None0]" time="1.270" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-int8-int8-1-None1]" time="83.202"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea55a360&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-float16-float16-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-float16-float32-1-None0]" time="0.990" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-float16-float32-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-float32-float32-1-None0]" time="1.140" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-True-none-tf32-float32-float32-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-int8-int8-1-None0]" time="3.656" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-int8-int8-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-float16-float16-1-None0]" time="0.303" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-float16-float16-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-float16-float32-1-None0]" time="1.276" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-float16-float32-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-float32-float32-1-None0]" time="1.051" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-True-False-none-tf32-float32-float32-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-int8-int8-1-None0]" time="2.058" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-int8-int8-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-float16-float16-1-None0]" time="1.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-float16-float16-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-float16-float32-1-None0]" time="1.294" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-float16-float32-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-float32-float32-1-None0]" time="1.152" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-True-none-tf32-float32-float32-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-int8-int8-1-None0]" time="1.792" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-int8-int8-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-float16-float16-1-None0]" time="1.050" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-float16-float16-1-None1]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-float16-float32-1-None0]" time="1.153" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-float16-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-16-32-4-False-False-none-tf32-float32-float32-1-None0]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-add-rows-ieee-float16-float16-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-add-rows-ieee-float32-float32-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-add-cols-ieee-float16-float16-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-add-cols-ieee-float32-float32-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-32-64-True-True-False-e2m1-e4m3-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw8' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-64-True-True-False-e2m1-e4m3-4-16-1]'&quot;">worker 'gw8' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-64-True-True-False-e2m1-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-float16-float16-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-float16-float32-1-None0]" time="13.345"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb07f61a60&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-float16-float32-1-None0]" time="45.814"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b64343290&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-float16-float32-1-None1]" time="13.319"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0cb95f70&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-float32-float32-1-None0]" time="13.693"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0cb2a180&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-True-False-none-tf32-float32-float32-1-None1]" time="13.850"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba86551580&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-float16-float32-1-None1]" time="46.217"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62f4b3b0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-int8-int8-1-None0]" time="29.782"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f156750&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-float16-float16-1-None0]" time="13.543"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e87be660&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-float16-float16-1-None1]" time="13.409"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e41786e0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-int8-int8-1-None1]" time="29.785"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79baedfe1a90&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-float16-float32-1-None0]" time="20.871"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea563800&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-float32-float32-1-None0]" time="44.261"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62f92330&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-float16-float32-1-None1]" time="21.321"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea50d340&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-float16-float16-1-None0]" time="15.014" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-float16-float16-1-None1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-float16-float32-1-None0]" time="7.462"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba919a93d0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-float32-float32-1-None0]" time="20.924"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e41ace00&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-float16-float32-1-None1]" time="7.661"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0cb945f0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-True-none-tf32-float32-float32-1-None1]" time="43.995"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62ff8710&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-float32-float32-1-None0]" time="7.344"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0090f620&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-True-none-tf32-float32-float32-1-None1]" time="7.395"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8d3fed20&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-False-False-none-tf32-float32-float32-1-None1]" time="20.721"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e2d5fe60&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-int8-int8-1-None0]" time="34.720"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f1c4fb0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-int8-int8-1-None0]" time="145.583"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e8799f10&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-int8-int8-1-None0]" time="143.361"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b6434b170&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-int8-int8-1-None1]" time="34.441"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba91991e50&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-float16-float16-1-None0]" time="19.974" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-float16-float16-1-None1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-float16-float32-1-None0]" time="7.851"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79baedfe2630&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-float16-float32-1-None1]" time="8.025"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0cb95040&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-float32-float32-1-None0]" time="8.241"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f1ed490&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-none-tf32-float32-float32-1-None1]" time="8.341"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb07f61940&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-int8-int8-1-None0]" time="44.435"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0cb954c0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-int8-int8-1-None1]" time="147.015"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea562ed0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-int8-int8-1-None1]" time="44.179"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865ecc80&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-int8-int8-1-None1]" time="142.204"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b6072d280&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-float16-float16-1-None0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-float16-float16-1-None1]" time="0.019" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-float16-float32-1-None0]" time="13.927"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f1f2fc0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-float16-float32-1-None1]" time="14.241"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8a199be0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-float32-float32-1-None0]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f1f02c0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-True-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba919c5a30&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-int8-int8-1-None0]" time="50.136"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba919c6300&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-int8-int8-1-None1]" time="50.251"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865f5e20&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-float16-float16-1-None0]" time="54.065"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e87bdc40&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-float16-float16-1-None0]" time="53.720"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62fb4170&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-float16-float16-1-None0]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-float16-float16-1-None1]" time="0.021" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-float16-float32-1-None0]" time="14.482"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865379e0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-float16-float32-1-None1]" time="14.524"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865a26f0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-float16-float16-1-None1]" time="54.611"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e87bd1f0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-float16-float16-1-None1]" time="53.293"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b5e98e300&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-float32-float32-1-None0]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f18ed20&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-True-False-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba919f8bc0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-int8-int8-1-None0]" time="22.147"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba86510170&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-int8-int8-1-None1]" time="22.366"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865393a0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-float16-float16-1-None0]" time="0.026" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-float16-float16-1-None1]" time="0.028" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-float16-float32-1-None0]" time="8.163"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f1ef020&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-float16-float32-1-None0]" time="48.387"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e41780b0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-float16-float32-1-None0]" time="46.252"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62f12060&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-float16-float32-1-None1]" time="8.082"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0093af60&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-float32-float32-1-None0]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865d6870&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-True-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865d5820&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-int8-int8-1-None0]" time="17.668"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb009396a0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-int8-int8-1-None1]" time="17.551"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79bb0cb94200&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-float16-float32-1-None1]" time="45.962"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778bd89b9190&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-float16-float32-1-None1]" time="48.310"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417e450&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-float16-float16-1-None0]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-float16-float16-1-None1]" time="0.020" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-float16-float32-1-None0]" time="8.805"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79baedfe2ea0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-float16-float32-1-None1]" time="8.729"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f157680&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-float32-float32-1-None0]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba865f5df0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-4-False-False-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 4, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x79ba8f1c5be0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-int8-int8-1-None0]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-int8-int8-1-None1]" time="0.014" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-float16-float16-1-None0]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-float16-float16-1-None1]" time="0.012" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-float16-float32-1-None0]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-float16-float32-1-None1]" time="0.013" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-float32-float32-1-None0]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-True-none-tf32-float32-float32-1-None1]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-32-128-64-2-True-False-none-tf32-int8-int8-1-None0]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-float32-float32-1-None0]" time="43.974"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778bdc8a9b20&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-float32-float32-1-None0]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e874b6e0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-True-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e87842f0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-int8-int8-1-None0]" time="145.904"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417acf0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-2-True-False-none-tf32-float32-float32-1-None1]" time="43.253"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x778b62fde840&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-32-128-False-False-True-e2m1-fp16-4-16-1]" time="0.001"><skipped type="pytest.skip" message="scaled_dot with fp16 input not supported on XPU yet">/home/tanjingder/4d8e9bb/intel-xpu-backend-for-triton/python/test/unit/language/test_core.py:3619: scaled_dot with fp16 input not supported on XPU yet</skipped></testcase><testcase classname="test.unit.language.test_core" name="test_scaled_dot[32-32-128-False-False-True-e4m3-e4m3-4-16-1]" time="0.000"><error message="failed on setup with &quot;worker 'gw6' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-128-False-False-True-e4m3-e4m3-4-16-1]'&quot;">worker 'gw6' crashed while running 'test/unit/language/test_core.py::test_scaled_dot[32-32-128-False-False-True-e4m3-e4m3-4-16-1]'</error></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-int8-int8-1-None1]" time="143.079"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e87bfbf0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-float16-float16-1-None0]" time="52.559"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e4179580&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-float16-float16-1-None1]" time="52.567"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e8771220&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-float16-float32-1-None0]" time="47.287"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417a210&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-float16-float32-1-None1]" time="46.977"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e8787da0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-float32-float32-1-None0]" time="0.010"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea50dbe0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-True-False-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = True, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea560770&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-int8-int8-1-None0]" time="69.387"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d0364a8f020&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-int8-int8-1-None1]" time="69.273"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d0364a8f020&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-float16-float16-1-None0]" time="11.737"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417f3b0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-float16-float16-1-None1]" time="11.784"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e87e3560&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-float16-float32-1-None0]" time="20.595"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e2d1c980&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-float16-float32-1-None1]" time="20.517"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea50c7d0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-float32-float32-1-None0]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417b050&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-True-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = True
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea50c7d0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-int8-int8-1-None0]" time="69.509"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417b9b0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-int8-int8-1-None1]" time="69.480"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'int8'
out_dtype = triton.language.int8, kpack = 1, mma_nonk_size = None, num_ctas = 1
device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e4178c80&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-float16-float16-1-None0]" time="12.500"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e8749c10&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-float16-float16-1-None1]" time="12.550"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float16, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417d700&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-float16-float32-1-None0]" time="21.484"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e417c7a0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-float16-float32-1-None1]" time="21.685"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float16'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e8785100&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-float32-float32-1-None0]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e5f618b0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-128-128-2-False-False-none-tf32-float32-float32-1-None1]" time="0.009"><failure message="triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.">M = 64, N = 128, K = 128, num_warps = 2, col_a = False, col_b = False
epilogue = 'none', input_precision = 'tf32', in_dtype = 'float32'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea50ff50&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
&gt;           raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
E           triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:400: OutOfResources</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-True-True-none-ieee-float32-float32-1-None]" time="0.011" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-True-False-none-ieee-float32-float32-1-None]" time="0.010" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-True-none-ieee-float32-float32-1-None]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-none-ieee-float32-float32-1-None1]" time="0.009" /><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-64-4-False-False-chain-dot-ieee-bfloat16-float32-1-None]" time="0.016" /><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-chain-dot-ieee-float8e5-float32-1-None]" time="1.289"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'chain-dot', input_precision = 'ieee', in_dtype = 'float8e5'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02e5f508c0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-128-128-64-4-False-False-chain-dot-ieee-float8e4nv-float32-1-None]" time="1.632"><failure message="RuntimeError: Triton Error [ZE]: 0x70000004">M = 128, N = 128, K = 64, num_warps = 4, col_a = False, col_b = False
epilogue = 'chain-dot', input_precision = 'ieee', in_dtype = 'float8e4nv'
out_dtype = triton.language.float32, kpack = 1, mma_nonk_size = None
num_ctas = 1, device = 'xpu'

    @pytest.mark.interpreter
    @pytest.mark.parametrize(
        "M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size",
        get_test_dot_base_cases() + \
        get_test_dot_mixed_sizes_cases() + \
        get_test_dot_transposed_op_base_cases() + \
        get_test_dot_h100_shortcut_cases() + \
        get_test_dot_mfma_edge_cases() + \
        get_test_dot_fp8_output_cases() + \
        get_test_dot_small_k_mfma_cases() + \
        get_test_dot_small_mn_fma_cases())
    @pytest.mark.parametrize("num_ctas", num_ctas_list)
    def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, input_precision, in_dtype, out_dtype, kpack, mma_nonk_size,
                 num_ctas, device):
        if is_interpreter():
            if in_dtype == 'bfloat16':
                pytest.xfail("bfloat16 is not supported in the interpreter")
        else:
            if is_xpu():
                if (M &lt; 8 or N &lt; 16 or (K &lt; 16 and in_dtype == 'float16') or (K &lt; 8 and in_dtype == 'float32')):
                    pytest.xfail("XPU: small dots are not supported")
            elif not is_hip() and (M &lt; 16 or N &lt; 16 or K &lt; 16):
                pytest.skip("small dots are supported only on HIP at the moment")
            if is_cuda():
                capability = torch.cuda.get_device_capability()
    
                if capability[0] &lt; 7:
                    pytest.skip("Only test tl.dot() on devices with sm &gt;= 70")
                if capability[0] &lt; 8:
                    if capability[1] == 0 and in_dtype == 'int8':
                        pytest.skip("Only test int8 on devices with sm &gt;= 75")
                    if input_precision != "ieee":
                        pytest.skip("Only test tf32 on devices with sm &gt;= 80")
                if capability[0] == 7:
                    if (M, N, K, num_warps) in [(128, 256, 32, 8), (64, 128, 128, 4), (64, 128, 128, 2)]:
                        pytest.skip("shared memory out of resource")
                    if out_dtype == 'float16':
                        # TODO: support out_dtype=float16 for tl.dot on V100
                        pytest.skip("Only test out_dtype=float16 on devices with sm &gt;=80")
                if capability[0] &lt; 9 and in_dtype == 'float8e4nv':
                    pytest.skip("float8e4nv not supported on sm &lt;= 80")
            if is_hip() and (in_dtype == 'float8e4nv' or in_dtype == 'float8e5'):
                pytest.skip("float8e4nv and float8e5 not supported on HIP")
            if is_hip() and (input_precision != "ieee"):
                pytest.skip(f"{input_precision} not supported on HIP")
            if is_hip() and (kpack == 2 and in_dtype == 'int8' and K &lt; 64):
                pytest.skip("kpack too large for K")
            if not is_hip() and kpack == 2:
                pytest.xfail("Skip duplicated tests on nv path")
    
        if is_cuda():
            torch.backends.cuda.matmul.allow_tf32 = input_precision == "tf32"
    
        if num_ctas &gt; 1 and in_dtype == 'int8':
            # FIXME: mma v2 with num_ctas &gt; 1 does not work
            pytest.xfail()
    
        # triton kernel
        @triton.jit
        def kernel(X, stride_xm, stride_xk, Y, stride_yk, stride_yn, W, stride_wn, stride_wl, Z, stride_zm, stride_zn,
                   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ADD_MATRIX: tl.constexpr,
                   ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr, INPUT_PRECISION: tl.constexpr, DO_SOFTMAX: tl.constexpr,
                   CHAIN_DOT: tl.constexpr, COL_A: tl.constexpr, COL_B: tl.constexpr, out_dtype: tl.constexpr = tl.float32):
            off_m = tl.arange(0, BLOCK_M)
            off_n = tl.arange(0, BLOCK_N)
            off_l = tl.arange(0, BLOCK_N)
            off_k = tl.arange(0, BLOCK_K)
            Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk
            Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn
            Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl
            Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn
            x = tl.load(Xs)
            y = tl.load(Ys)
            z = tl.dot(x, y, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            if ADD_MATRIX:
                z += tl.load(Zs)
            if ADD_ROWS:
                ZRs = Z + off_m * stride_zm
                z += tl.load(ZRs)[:, None]
            if ADD_COLS:
                ZCs = Z + off_n * stride_zn
                z += tl.load(ZCs)[None, :]
            if DO_SOFTMAX:
                max = tl.max(z, 1)
                z = z - max[:, None]
                num = tl.exp(z.to(tl.float32)).to(max.dtype)
                den = tl.sum(num, 1)
                z = num / den[:, None]
            if CHAIN_DOT:
                w = tl.load(Ws)
                z = tl.dot(z.to(w.dtype), w, input_precision=INPUT_PRECISION, out_dtype=out_dtype)
            tl.store(Zs, z)
    
        # input
        rs = RandomState(17)
        if col_a:
            x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T
        else:
            x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)
        if col_b:
            y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T
        else:
            y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)
        w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)
        if 'int' not in in_dtype and 'float8' not in in_dtype:
            x *= .1
            y *= .1
        if in_dtype == 'float32' and input_precision == "tf32":
            x = (x.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            y = (y.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
            w = (w.view('uint32') &amp; np.uint32(0xffffe000)).view('float32')
        x_tri = to_triton(x, device=device, dst_type=in_dtype)
        y_tri = to_triton(y, device=device, dst_type=in_dtype)
        w_tri = to_triton(w, device=device, dst_type=in_dtype)
        # triton result
        if out_dtype == 'int8':
            z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)
        else:
            z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1
    
        z_tri = to_triton(z, device=device)
        if epilogue == 'trans':
            z_tri = torch.as_strided(z_tri, (M, N), [1, M])
    
        if out_dtype == 'int8':
            out_dtype = tl.int8
        elif out_dtype == 'float16' and epilogue != 'softmax':
            # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will
            # fail with the following error: 'llvm.fmul' op requires the same type
            # for all operands and results
            out_dtype = tl.float16
        else:
            out_dtype = tl.float32
    
        kern_kwargs = {
            'COL_A': col_a, 'COL_B': col_b, 'BLOCK_M': M, 'BLOCK_K': K, 'BLOCK_N': N, 'ADD_MATRIX':
            epilogue == 'add-matrix', 'ADD_ROWS': epilogue == 'add-rows', 'ADD_COLS': epilogue == 'add-cols', 'DO_SOFTMAX':
            epilogue == 'softmax', 'CHAIN_DOT': epilogue == 'chain-dot', 'INPUT_PRECISION': input_precision, 'num_warps':
            num_warps, 'num_ctas': num_ctas, 'out_dtype': out_dtype
        }
    
        if is_hip():
            kern_kwargs['kpack'] = kpack
            if mma_nonk_size is not None:
                kern_kwargs['matrix_instr_nonkdim'] = mma_nonk_size
    
&gt;       pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1), y_tri, y_tri.stride(0), y_tri.stride(1), w_tri,
                             w_tri.stride(0), w_tri.stride(1), z_tri, z_tri.stride(0), z_tri.stride(1), **kern_kwargs)

language/test_core.py:3509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:336: in &lt;lambda&gt;
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
../../../.prebuild/lib/python3.12/site-packages/triton/runtime/jit.py:585: in run
    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,
../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:407: in __getattribute__
    self._init_handles()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;triton.compiler.compiler.CompiledKernel object at 0x7d02ea55acf0&gt;

    def _init_handles(self):
        if self.module is not None:
            return
        device = driver.active.get_current_device()
        # create launcher
        self.run = driver.active.launcher_cls(self.src, self.metadata)
        # not enough shared memory to run the kernel
        max_shared = driver.active.utils.get_device_properties(device)["max_shared_mem"]
        if self.metadata.shared &gt; max_shared:
            raise OutOfResources(self.metadata.shared, max_shared, "shared memory")
        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`
&gt;       self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(
            self.name, self.kernel, self.metadata.shared, self.metadata.build_flags, device)
E       RuntimeError: Triton Error [ZE]: 0x70000004

../../../.prebuild/lib/python3.12/site-packages/triton/compiler/compiler.py:402: RuntimeError</failure></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-none-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-none-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-trans-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-trans-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-add-rows-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-add-rows-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-add-cols-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-2-2-16-1-False-False-add-cols-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-none-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-none-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-trans-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-trans-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-add-rows-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-add-rows-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-add-cols-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-1-64-64-1-False-False-add-cols-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-none-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-none-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-trans-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-trans-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-add-rows-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-add-rows-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-add-cols-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-2-64-2-False-False-add-cols-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-none-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-none-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-trans-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-trans-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-add-matrix-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-add-matrix-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-add-rows-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-add-rows-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-add-cols-ieee-float16-float16-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-64-64-4-4-False-False-add-cols-ieee-float32-float32-1-None]" time="0.001"><skipped type="pytest.xfail" message="XPU: small dots are not supported" /></testcase><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-none-ieee-float16-float16-1-None]" time="0.008" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-none-ieee-float32-float32-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-trans-ieee-float16-float16-1-None]" time="0.007" /><testcase classname="test.unit.language.test_core" name="test_dot[1-8-16-16-1-False-False-trans-ieee-float32-float32-1-None]" time="0.011" /></testsuite></testsuites>